{
  "eval_results": {
    "bleu": 1.961837509265967e-08,
    "rouge1": 0.05215107349536119,
    "rouge2": 0.0,
    "rougeL": 0.04826902795598083,
    "length_ratio": 0.03379814517228014,
    "combined_f1": 3.9236734238013125e-08,
    "final_score": 1.3261288398625423e-09
  },
  "eval_dataset": [
    {
      "input_text": "Title: A Cosmological Solution to Mimetic Dark Matter\nDomains: Physics\n\nintroduction: a modification of general relativity was proposed in   where a metric inlineform0 is defined by a scalar field inlineform1 and an auxiliary metric inlineform2 displayform0the equations of motion that result are similar to einstein 's equations of motion with an extra mode term that mimics cold dark matter even in the absence of normal matter .mimetic dark matter is an interesting model because it 's a model that works not only on a cosmological scale , but also a model that works on a galactic scale after adding higher derivative terms that alter the speed of sound    .for further discussion about mimetic dark matter , degrees of freedom , and extensions check   .consider the actions in      , displayform0where inlineform3 is the lagrangian of general relativity , inlineform4 is a potential , and inlineform5 is the lagrangian of matter .by varying the action with respect to inlineform6 , inlineform7 , and inlineform8 , and taking the trace , equations ( 13 ) and ( 14 ) in   are obtained , displayform0where inlineform9 is the energy - momentum tensor .note that the normalization condition on the four - velocity inlineform10 is the normalization condition ( eqref3 ) .for a spatially flat frw universe with metric , displayform0taking inlineform11 and calculating the time - time component of equation ( 3 ) , which is the friedmann equation in the absence of ordinary matter ( inlineform12 )   displayform0where , displayform0and inlineform13 is determined by given potential .note that mimetic dark matter appears as an integration constant in the right hand side of ( eqref5 ) which gives a non - trivial solution even for inlineform14 .by multiplying equation ( 6 ) by inlineform15 and differentiating with respect to time , and substituting inlineform16 , displayform0this equation allows to find cosmological solutions for inlineform17 for any given potential .\n\nsolution for exponential potential: plugging in the exponential potential , displayform0where inlineform18 and inlineform19 are constants , we obtain displayform0by applying the transformation inlineform20 , differential equation ( eqref9 ) transforms to , displayform0the solution to this differential equation is well - known by bessel 's functions , displayform0where inlineform21 and inlineform22 are constants .the form of inlineform23 is , displayform0it can be deduced that for inlineform24 , inlineform25 which is similar to the scaling factor of a matter - dominated universe .on the other hand , for inlineform26 , displayform0for inlineform27 positive , inlineform28 grows exponentially as in an inflationary universe .however , for inlineform29 negative , inlineform30 leads to an oscillatory universe in the beginning of time .the energy density of mimetic matter can be obtained as displayform0and the pressure , displayform0and the equation of state is displayform0moreover , it can be deduced from ( eqref9 ) that the density , pressure , and equation of state evolve like dust in a matter - dominated universe for inlineform31 displayform0for inlineform32 , displayform0the equation of state for inlineform33 ( ) is at the phantom divide line similar to the equation of state of a positive cosmological constant that drives inflation but without a graceful exit .in order to trigger inflation in the beginning of time , inlineform34 must be satisfied .the acceleration equation is , displayform0hence , inlineform35 must be true .density is always positive ; therefore , we must have negative pressure satisfying displayform0this is valid for inlineform36 very small , positive inlineform37 , and all initial conditions inlineform38 and inlineform39 .a 60 e - folds inflation can be generated in this picture for any inlineform40 because it satisfies the inequality .let 's consider another potential  displayform0given that inlineform41 is true always for positive time and suitable inlineform42 .as inlineform43 and inlineform44it evolves as inlineform45 , and as inlineform46 it generates inflation satisfying the 60 e - folds condition with displayform0with displayform0the number of e - folds is calculated by displayform0note that at inlineform47 both potentials ( eqref8 ) and ( eqref21 ) behaves the same because at inlineform48 ( eqref21 ) can be approximated as ( eqref8 ) .in order to give an estimate for inlineform49 , calculate ( eqref24 ) for 60 e - folds for ( eqref22 ) , and noting that inlineform50 for this model because inflation starts from inlineform51 ; and hence displayform0\n\nperturbative solution of the scalar field in the newtonian gauge: scalar perturbations are considered in the newtonian gauge .vector perturbations are neglected because they decay in an expanding universe and because inflation rules out large primordial vector perturbations .in the newtonian gauge , the metric of the perturbed universe can be expressed as   displayform0and displayform0is the perturbation of the scalar field .perturbing the equations that result from action ( eqref2 ) , it can be deduced from   that there 's one expression for inlineform52 for all wavelengths , displayform0note that the first equality in ( ) is deduced from ( eqref3 ) .when spatial derivatives are neglected , expressions ( eqref28 ) and ( ) are exact general solutions for long wavelength cosmological perturbations   .if inlineform53 is calculated by using action ( eqref2 ) , we would get ( ) for all wavelengths , and it does n't distinguish between short and long wavelength perturbations   .we would n't be able to define quantum perturbations that are short wavelength perturbations .therefore , in order to account for different wavelengths ' perturbations , a term is added to the action inlineform54 , inlineform55 where inlineform56 is a constant and inlineform57 .the action becomes   , displayform0the inlineform58 and inlineform59einstein 's equations remain the same up to a normalization constant .on the other hand , the perturbed inlineform60 einstein 's equation   displayform0where displayform0note that after adding inlineform61 to inlineform62 , equation ( eqref7 ) becomes   displayform0by using ( eqref31 ) .we can define a new inlineform63 in order to absorb this constant .hence , let 's define displayform0therefore , inlineform64 becomes inlineform65 in potentials ( eqref8 ) and ( eqref21 ) , and all the equations that are mentioned above that depend on inlineform66 .considering a plane wave perturbation inlineform67 , equation ( eqref30 ) becomes , displayform0by taking the limit of inlineform68 in ( eqref12 ) is similar when taking the limit of the argument of bessel 's function to zero because of the decaying exponential function inside the argument of bessel 's functions .so for small inlineform69 , displayform0hence , the scaling factor ( eqref12 ) becomes as inlineform70 , displayform0this equation can be expressed again as , displayform0where inlineform71 is just a constant , and inlineform72 and inlineform73 .by substituting ( eqref37 ) in ( eqref30 ) , and solving the differential equation , we can get an idea about the evolution of inlineform74 at a very large time - scale and for different wavelengths .for short wavelength perturbation inlineform75 and inlineform76 are neglected because inlineform77 , displayform0however , for long wavelength perturbation , the term inlineform78 is neglected because inlineform79 ; and hence , the solution to equation ( eqref30 ) is displayform0equation ( eqref39 ) can also be obtained by a second method ; if we plug equation ( eqref37 ) in ( eqref28 ) , and choose inlineform80 we would get equation ( eqref39 ) again .note that the perturbation amplitude grows as a function of time only .action ( eqref29 ) to second order and integrating by parts yield displayform0the canonically normalized quantum fluctuation variable    is displayform0with vacuum fluctuation displayform0and hence , displayform0during inflation , displayform0matching long wavelength perturbations ( eqref28 ) with quantum perturbations ( eqref43 ) displayform0hence , the gravitational potential in comoving scales inlineform81 displayform0in order to obtain the gravitational potential for comoving scales for potential ( eqref21 ) from quantum perturbations , substitute ( eqref22 ) , ( eqref23 ) , and ( eqref25 ) in ( eqref46 ) with absolute value displayform0note that inlineform82 is just a constant in the action ( eqref29 ) .hence , by choosing inlineform83 , inlineform84 , and inlineform85 appropriately , one can fit the value of the gravitational potential to be equal to the measured value inlineform86 in cmb experiments and   .\n\nspectral index calculations: in order to calculate the spectral index for potentials ( eqref8 ) and ( eqref21 ) , we should calculate the slow - roll hubble parameters , displayform0for potential ( eqref8 ) displayform0it behaves like a cosmological constant and no gravitational waves .for potential ( eqref21 ) displayform0substituting ( eqref25 ) and ( eqref33 ) in ( eqref50 )we obtain , displayform0where inlineform87 is evaluated at the horizon crossing inlineform88 .in order to make ( eqref51 )less than one and match the data in cmb experiments and  , the second term must be negative .in potential ( eqref21 ) inflation starts from inlineform89 ; so inlineform90 and inlineform91 are negative .hence , inlineform92 .the tensor spectral index displayform0if inlineform93 , then there are no gravitational waves .\n\nconclusion: in this paper , an exponential potential was substituted in the differential equation of mdm that relates any potential to any scaling factor in cosmology .at the limit of time goes to infinity , the density , pressure , and equation of state behave like dust in a matter - dominated universe , and in the limit of time goes to zero , a condition on the density can trigger inflation for some initial conditions satisfying the 60 e - folds condition .another general potential is given that satisfies the 60 e - folds condition too .furthermore , solutions to scalar perturbations are obtained for the general potential .this can be accomplished by taking the limit of inlineform94 at infinity and substituting it in the inlineform95perturbed einstein 's equation of a scalar field in the newtonian gauge to get long wavelength perturbations .it is worth noting that after performing quantum perturbations , the obtained amplitude fluctuation from mdm can be tuned to be of the same order as the cmb .finally , the spectral index for the mentioned potentials is calculated and the parameters were constrained .hence , it was shown that mimetic inflation can have a red - tilt for the spectral index of adiabatic fluctuations .therefore , mdm can have a model for dark matter , inflation with 60 e - folds at early times , and cmb 's fluctuation .",
      "target_text": "in this paper , a cosmological solution to mimetic dark matter ( mdm ) for an exponential potential is provided .then , a solution for the $ 0-i$ perturbed einstein 's differential equation of mdm is obtained based on an exponential potential that satisfies inflation for some initial conditions .another general potential is suggested that incorporates inflation too .then , quantum perturbations are included .the constants in the model can be tuned to be in agreement with the amplitude fluctuation of the cosmic microwave background ( cmb ) radiation .finally , the spectral index is calculated for the suggested potentials .moreover , mdm is shown to be a viable model to produce dark matter , inflation , and cmb 's fluctuation .",
      "output_text": "thea: and to of"
    },
    {
      "input_text": "Title: An inverse medium problem using Stekloff eigenvalues and a Bayesian approach\nDomains: Mathematics\n\nintroduction: inverse scattering problems for inhomogeneous media have many applications such as medical imaging and nondestructive testing .in this paper , the inverse spectrum problem to reconstruct the stekloff eigenvalues from cauchy data is investigated first using a new integral equation for the reciprocity gap method .then these eigenvalues are used to estimate the index of refraction of the inhomogeneous medium .due to the lack of knowledge of the relation between stekloff eigenvalues and the index of refraction , we propose a bayesian approach .since the eigenvalues are complex for absorbing media and the multiplicities are not known , the recently developed spectral indicator method is employed to compute the stekloff eigenvalues   .the reconstruction of certain eigenvalues from the scattering data has been studied by many researchers .in the context of qualitative methods in inverse scattering , it has been shown that interior eigenvalues such as dirichlet eigenvalues and transmission eigenvalues can be determined from the scattering data   ( see also the special issue edited by lechleiter and sun   ) .a related method , which can be used to compute interior eigenvalues using the scattering data , is the inside - outside duality   .given reconstructed eigenvalues , a legitimate question is what information about the obstacle can be obtained .for inhomogeneous non - absorbing media , transmission eigenvalues have been used to reconstruct the shape of the obstacle   and obtain useful information of the index of refraction   .however , the use of transmission eigenvalues has two drawbacks : 1 ) multi - frequency data are necessary ; and 2 ) only real transmission eigenvalues can be determined from the scattering data so far .it has been shown that stekloff eigenvalues associated with the scattering problem can be determined from far field data of a single frequency   .unlike transmission eigenvalues , stekloff eigenvalues exist for absorbing media as well .hence the use of stekloff eigenvalues avoids the above two drawbacks and has the potential to work for a wider class of problems .in this paper , a new integral equation for the reciprocity gap( rg ) method   is introduced to determine stekloff eigenvalues from cauchy data .then a bayesian approach is proposed to estimate the index of refraction .the metropolis - hastings ( m - h ) algorithm is used to explore the posterior distribution .numerical examples show that the proposed methods are effective .we refer the readers to   ,   and references therein on the bayesian framework for inverse problems and   for the bayesian methods for some inverse scattering problems .the rest of the paper is organized as follows .in section 2 , the forward scattering problem and the associated stekloff eigenvalue problem are introduced .in section 3 , a new integral equation for the reciprocity gap method is proposed to reconstruct stekloff eigenvalues using cauchy data .in section 4 , a bayesian approach and the mcmc method are proposed to estimate the index of refraction .finally , numerical examples are provided in section 5 .\n\nscattering problem and stekloff eigenvalues: in this section , we introduce the direct scattering problem , the stekloff eigenvalue problem , and the inverse scattering problems using cauchy data .then a monotonicity of the largest negative stekloff eigenvalue is proved .let inlineform0 be a bounded domain in inlineform1 with boundary inlineform2 of class inlineform3 .let inlineform4 be the wavenumber and inlineform5 be the index of refractionsuch that inlineform6 .assume that inlineform7 for inlineform8 and inlineform9 ,where inlineform10 and inlineform11 denote the real and imaginary parts , respectively .the direct scattering problem is to find the total field inlineform12such that displayform0where inlineform13is the incident wave generated by a point source .here inlineform14 is the fundamental solution of the helmholtz equation .the associated stekloff eigenvalue problem is defined as follows   .find inlineform15 and a non - trivial function inlineform16such that displayform0where inlineform17 be a bounded domain in inlineform18 and inlineform19such thatinlineform20 .assume that the cauchy data inlineform21 and inlineform22 are known on inlineform23 for each the incident wave inlineform24 , where inlineform25 is a simple closed curve containing inlineform26 ( see figref3 ) .the inverse scattering problems considered in this paper are :the weak formulation for the stekloff eigenvalue problem ( eqref2 ) is to find inlineform27such that displayform0where inlineform28 .when inlineform29 is real , all stekloff eigenvalues are real and they form an infinite discrete set   .we call inlineform30 a modified dirichlet eigenvalue of inlineform31if there exists a nontrivial inlineform32such that displayform0remark 2.1 note that a standard dirichlet eigenvalue problem is such that inlineform33 in ( eqref7 ) .for simplicity , in the rest of the paper , we call inlineform34 in ( eqref7 )a dirichlet eigenvalue .it is shown in   that stekloff eigenvalues accumulate at inlineform35 if inlineform36 is not a dirichlet eigenvalue .next , we prove a property of the largest negative stekloff eigenvalue inlineform37 when inlineform38 is given by displayform0suppose inlineform39 is perturbed by inlineform40where inlineform41 is also a real constant .the perturbation inlineform42 leads to inlineform43 and inlineform44 of the eigenpair .from ( eqref6 ) , inlineform45 and inlineform46 satisfies inlineform47using the fact that inlineform48 is a real eigenpair , we have that inlineform49letting inlineform50 and noting that inlineform51 is real , we have that inlineform52which implies that displayform0where inlineform53 .if inlineform54 is small enough , one has that displayform0from ( eqref10 ) and ( eqref11 ) , we have displayform0this implies that inlineform55 is monotonically increasing with respect to inlineform56 .this breaksuntil inlineform57 becomes a ( modified ) neumann eigenvalue , i.e. , there exists a non - trivial inlineform58such that displayform0note that a standard eigenvalues is inlineform59 satisfying ( eqref13 ) for inlineform60 .again , we inlineform61 a neumann eigenvalue for simplicity .excluding this case , we actually proved the following theorem .theorem 2.2let the index of refraction be defined in ( eqref9 ) and inlineform62 be an interval that inlineform63 is not a neumann eigenvalue of ( eqref13 ) for any inlineform64 .then the largest negative stekloff eigenvalue inlineform65 is monotonically increasing on inlineform66 .assume that the largest negative stekloff eigenvalue inlineform67 is obtained .if the shape of inlineform68 is known , inlineform69 uniquely determines inlineform70 on some suitable interval inlineform71 by secref14 .however , it is not true on inlineform72 as it is known that different inlineform73 's can give the same inlineform74 .\n\nreconstruction of stekloff eigenvalues: now we consider ip1 to reconstruct stekloff eigenvalues from cauchy data .the main ingredient is the reciprocity gap method using cauchy data   .assume that inlineform75 and inlineform76 are known on inlineform77 for each point source incident wave inlineform78( see figref3 ) .the following auxiliary scattering problem will be useful in the subsequent analysis .find inlineform79such that displayform0where inlineform80 is the unit outward normal to inlineform81 and inlineform82 is a constant such that inlineform83 .it is shown in   that ( eqref15 ) has a unique solution .denote by inlineform84 and inlineform85 the sets of solutions inlineform86 to ( eqref1 ) and inlineform87 to ( eqref15 ) for the incident wave inlineform88 , respectively .define the reciprocity gap functional by displayform0where inlineform89 and inlineform90 are solutions of the helmholtz equation .let inlineform91 and consider the integral equation of finding inlineform92 to displayform0where inlineform93 for some inlineform94 and inlineform95 is the herglotz wave function defined by inlineform96lemma 3.1 if inlineform97 for all inlineform98 , then inlineform99 on inlineform100 .\n\nreconstruction of the index of refraction: the algorithm in the previous section can reconstruct stekloff eigenvalues using cauchy data .given these reconstructed eigenvalues , in this section , we turn to ip2 to estimate the index of refraction .the relation between the index of refraction and stekloff eigenvalues is complicated and , to a large extend , unknown .even when inlineform101 is constant , a single stekloff eigenvalue can not uniquely determine it .note that secref14 only holds on an appropriate interval .to this end , we resort to the bayesian approach , which has been popular for solving inverse problems in recent years   .firstly , the inverse problem is reformulated as a statistical inference for the index of refraction using a bayes formula .then the metropolis - hastings algorithm is employed to explore the posterior distribution of inlineform102 .\n\nbayesian formulation: denote by inlineform103 the normal distribution and inlineform104 the uniform distribution .ip2 can be written as the statistical inference of inlineform105such that displayform0where inlineform106 is a vector of ( reconstructed ) stekloff eigenvalues , inlineform107 is a random function , inlineform108 is the operator mapping inlineform109 to inlineform110 based on ( eqref2 ) , and inlineform111 is the random noise .the noise inlineform112 , which is modeled as additive and mutually independent of inlineform113 .in the bayesian framework , the prior information can be coded into the prior densityinlineform114 .for example , if inlineform115 is known to be a real constant inlineform116 such that inlineform117 , one may take the prior as the continuous uniform distribution , i.e. , inlineform118 .given stekloff eigenvalues inlineform119 , the goal of the bayesian inverse problem is to seek statistical information of inlineform120 by exploring the conditional probability distribution inlineform121 , called the posterior distribution of inlineform122 .an important quantity is the conditional mean ( cm ) of inlineform123 defined as displayform0which is an constant estimation of inlineform124 .if inlineform125 , by the bayes formula , the posterior distribution satisfies displayform0i.e. , displayform0where inlineform126 is the density function for inlineform127 .\n\nmarkov chain monte carlo method: to explore inlineform128 given in ( eqref52 ) , we employ the popular mcmc ( markov chain monte carlo ) .mcmc to estimate cm is as follows : design a markov chain inlineform129 from required distribution and take the mean of the chain to approximate expectation .in particular , one could estimate inlineform130 by a sample mean using monte carlo integration : displayform0where inlineform131 , inlineform132 , are samples drawing from inlineform133 .two popular methods are metropolis - hasting ( m - h )algorithm   ,   and gibbs sampler   .in this paper , we choose a delayed rejection adaptive metropolis - hasting algorithm   .the mh algorithmchoose initial value inlineform134 and set inlineform135 ;draw a sample inlineform136 from a proposal distribution inlineform137and compute inlineform138draw inlineform139 ;if inlineform140 , setinlineform141 , else inlineform142 .when inlineform143 , the maximum sample size , stop ; else , inlineform144 and go to 2 .\n\nspectral indicator method: in the above algorithm , for each sample inlineform145 , one needs to compute stekloff eigenvalues , which are done by the finite element method for ( eqref2 )   .note that the stekloff eigenvalues are complex if inlineform146 is a complex function .in addition , the reconstructed eigenvalues only approximate the exact ones and the multiplicities are not known in general .thus the numerical methods need to compute stekloff eigenvalues of ( eqref2 ) in a region on inlineform147 close to the origin .a recently developed spectral indicator method ( sim ) is a good fit for this case   .given reconstructed eigenvalues inlineform148 , a rectangular region containing these eigenvalues is chosen .then sim is used to compute all eigenvalues inside the region .\n\nnumerical examples: in this section , we present some numerical examples to use the rg method to reconstruct stekloff eigenvalues from the cauchy data and estimate the index of refraction using the bayesian approach .three scatterers are considered : a disc with radius 1 centered at the origin , a square with vertices given by displayform0and an l - shaped domain given by displayform0three different indices of refraction are chosen : i ) inlineform149 , ii )inlineform150 , and iii )inlineform151 .the synthetic scattering data is simulated by a finite element method with a perfectly matched layer ( pml ) for ( eqref1 )   .the wavenumber is inlineform152 .there are 100 source points uniformly distributed on the curveinlineform153 , a circle with radius 3 .we compute the cauchy data at 100 points uniformly distributed on inlineform154 ( a circle with radius 2 ) and add inlineform155 noise .\n\nestimation of the index of refraction: given the reconstructed stekloff eigenvalues , we present some numerical examples for the estimation of the index of refraction by the bayesian approach .the examples are rather naive .nonetheless , the results show the potential of statistical approaches for inverse scattering problems .since the main goal is to show the effectiveness of the bayesian approach , we assume that the shape of the scatterer is known in the following examples .example 4 real constant index of refractioninlineform180 .assume one stekloff eigenvalue is reconstructed from cauchy data :inlineform181 for the disk , inlineform182 for the square , and inlineform183 for the l - shaped domain .since inlineform184 is a real constant , we take a uniform prior inlineform185 .the posterior density is given by displayform0we generate 3000 samples for each domain .the initial sample is chosen to be inlineform186 .the rest samples inlineform187 are drawn from the symmetric proposal distribution inlineform188where inlineform189 .  tabref82 shows inlineform190 for three domains .the markov chains are shown in figref83 .the samples concentrate around inlineform191 for the unit circle domain and square domain .however , for the l - shaped domain , the samples are accumulated around two values , 5 and 7 .in fact , this implies that one stekloff eigenvalue can not uniquely determine the constant index of refraction .if two stekloff eigenvalues , inlineform192 and inlineform193 , are used ( see figref90 ) , we obtain inlineform194 , which is a good approximation of 5 .example 5 real function index of refraction inlineform195 .assume that two stekloff eigenvalues are given and inlineform196 is of the forminlineform197 .we first obtain a constant approximation inlineform198 for inlineform199 as the above example .this provides some ideas of how to choose priors for inlineform200 and inlineform201 .for the second step , two stekloff eigenvalues are used .the posterior distribution is given by displayform0two reconstructed stekloff eigenvalues from tabref70 are used for each domain :inlineform202 for disc , inlineform203 for the square , and inlineform204 for the l - shaped domain .tabref94 shows the reconstruction results .example 6 complex index of refraction inlineform205 .assume that inlineform206 and inlineform207 .the same proposal distribution inlineform208 are used to sample both inlineform209 and inlineform210 .we use stekloff eigenvalues , inlineform211 for the circle , inlineform212 for the square , and inlineform213 for the l - shaped domain .tabref95 shows the reconstruction results and figref96 shows the markov chains .\n\nconclusions: in this paper , we show that the cauchy data of a medium scattering problem can be used to reconstruct the related stekloff eigenvalues , which is demonstrated by numerical examples .a bayesian approach is proposed to estimate the index of refraction using the reconstructed stkeloff eigenvalues .the method is particularly meaningful when there is a lack of understanding of the relation between the known data and the unknown quantities .in future , the authors plan to extend the preliminary study here to more challenging inverse scattering problems .\n\nacknowledgement: the research was supported in part by guangdong natural science foundation of china ( 2016a030313074 ) and nnsf of china under grants 11801218 and 11771068 .",
      "target_text": "this paper studies the reconstruction of stekloff eigenvalues and the index of refraction of an inhomogeneous medium from cauchy data .the inverse spectrum problem to reconstruct stekloff eigenvalues is investigated using a new integral equation for the reciprocity gap method .given reconstructed eigenvalues , a bayesian approach is proposed to estimate the index of refraction .moreover , since it is impossible to know the multiplicities of the reconstructed eigenvalues and since the eigenvalues can be complex , we employ the recently developed spectral indicator method to compute stekloff eigenvalues .numerical experiments validate the effectiveness of the proposed methods .",
      "output_text": "thea: and to of"
    },
    {
      "input_text": "Title: Geodesic equations in the static and rotating dilaton black holes: Analytical solutions and applications\nDomains: Physics\n\nintroduction: the well - known exact solution of the vacuum einstein equations described by schwarzschild in 1916   as a spherically symmetric black hole in a four dimensional spacetime .addition of an electric charge , change the schwarzschild solution to a charged black hole .this solution was discovered by reissner ( 1916 ) , weyl ( 1917 ) and nordstr\u00f6m ( 1918 ) , independentlyand now it is known as reissner - nordstr\u00f6m metric   .also , another solution of charged black hole in four dimensions was obtained by gibbons and maeda   , independently , by garfinkle , horowitz and strominger   using a scalar field in range of low - energy of heterotic string theory , which is called gmghs solution   .the gmghs black hole can be explained in string or einstein frame , which are connected to each other by conformal transformation despite of differences of the physical properties in each frames   .study of motion of massive and massless particles give a set of comprehensive information about the gravitational field around a black hole .analysis of geodesic equation of motion predict some observational phenomena such as perihelion shift , gravitational time - delay and light deflection .the first analytic solution for schwarzschild spacetime using weierstrassian elliptic functions and their derivatives presented by hagihara in 1931   .the theoretical and mathematical properties of weierstrassian elliptic functions demonstrated by jacobi   , abel   , riemann weierstrass   and baker   .analytical solutions of geodesic equations were investigated for different spacetimes such as reissner - nordstr\u00f6m , schwarzschild-(anti)de sitter and reissner - nordstr\u00f6m\u2013(anti)\u2013de sitter spactime in four dimensions and in higherdimensions   .also the motion of test particles around rotating black holes   ,   and in the spacetime of a black hole which is combined by cosmic string was studied extensively   .recently , geodesic equations were solved analytically in the spacetime of black hole in f(r )gravity   .analysis of geodesics , include null , timelike circular null andtimelike geodesics were studied in the spacetime of gmghs black hole in the special cases .the aim of this paper is to determine the complete set of analytic solutions of the geodesic equations in the spacetime of the static ( gmghs , magnetically charged gmghs , electrically charged gmghs ) and rotating ( ker - sen dilaton - axion )dilaton black holes .we discussed the motion of test particles and light rays in the spacetime of these black holes and present the analytic solutions of the geodesic equations in terms of the elliptic weierstrass functions .then we determined the type of the orbits for test particles and light rays in the vicinity of these static and rotating dilaton black holes .in the first part the static dilaton black holes is studied and in the second part the rotating dilaton black hole is analysed .our paper is organized as follows : in sec .( secref2 ) , we introduce the metrics and their histories for static dilaton black holes .then we derive the geodesic equations from lagrangian corresponding to the metric and discuss the effective potentials .we solve geodesic equations and classify the solutions of timelike and null geodesic equations .using analytical solutions and effective potentials , we plot some possible orbits for test particles around each black hole in acceptable regions .at the end of this section , we study astrophysical applications .in sec .( secref3 ) , we introduce the metric for a rotating dilaton black hole .then we derive the geodesic equations and effective potential .we solve the geodesic equations analytically and plot the possible orbits .our conclusions are drawn in sec .( secref4 ) .\n\nstatic dilaton black holes: in this section , we will discuss the geodesics in the static dilaton black holes and present analytical solutions of the equations of motion .\n\nmetrics: in this section , we review all the spacetimes which are used as static dilaton black holes .in the einstein frame , the gmghs action is   displayform0where inlineform0 is a dilaton , inlineform1 is the scalar curvature , and inlineform2 is the maxwell field .the spherically symmetric static charged solutions to equations of motion of the action ( eqref2 )is displayform0where inlineform3 and inlineform4 are mass and charge , respectively .this solution was obtained by gibbons and maeda , and independently garfinkle , horowitz and strominger with use a transformation to the schwarzschild solution   .the gmghs action in the string frame , is s = d4x - ge-2(r+4()2-ff ) , where inlineform5 is a dilaton , r is the scalar curvature , and inlineform6 is the maxwell 's field strength .string frame is related to the einstein frame action by the conformal transformation of inlineform7   .by going from an electrically to a magnetically charged black hole , the string metric does change with the change in sign of dilaton inlineform8 , but the einstein metric does not change .thus , the magnetically charged gmghs black hole metric in the string frame is given by   ,   : dsmag2=-(1 - 2mr)(1-q2mr )dt2+dr2(1 - 2mr)(1-q2mr)+r2(d2+sin2d2 ) , and the electrically charged gmghs solution in the string frame is given by : dsel2=-(1+q2 - 2m2mr)(1+q2mr)2dt2+dr2(1+q2 - 2m2mr)+r2(d2+sin2d2 ) .\n\nthe geodesic equations: the geodesic equations can be derived by compute the lagrangian for each metric as   displayform0where inlineform9 for null and timelike geodesics respectively .thus the lagrangian for the metric ( eqref3 ) is : 2lgm=-(1 - 2mr )t2+(1 - 2mr)-1r2+r(r - q2 m ) 2+r(r - q2m)sin22 , for the metric ( secref1 ) is : 2lmag=-(1 - 2mr)(1-q2mr ) t2 +1(1 - 2mr)(1-q2mr)r2+r2 ( 2+sin2 2 ) , andfor the metric ( secref1 ) is : 2lel=-(1+q2 - 2m2mr)(1+q2mr)2t2 + 1(1+q2 - 2m2mr ) r2+r2 ( 2+sin22 ) .the killing vectors respect to the spacetime from the euler - lagrange for time and latitude are inlineform10 and inlineform11 .the energy inlineform12 and the angular momentum inlineform13 are the constants of motion which are given by the generalized momenta inlineform14and inlineform15 displayform0from the euler - lagrange equation for inlineform16 we get to the energy conservationsdisplayform0displayform1and for inlineform17 we obtained the angular momentum conservationsdisplayform0 displayform1we consider the motion is took place in a equatorial plane because of the existence of spherically symmetry and choose inlineform18 and inlineform19 as the initial conditions .therefore with substitute inlineform20 and inlineform21 from eqs .( eqref7 ) -( eqref12 ) , in eqs .( secref4 ) -( secref4 ) , we get displayform0displayform1we obtain the corresponding equation for inlineform22 as a function of inlineform23 and as a function of inlineform24 , with energy and angular momentum conservation ( drd)2gm=(e2l2 -l2)r4+(-2e2q2ml2 + 2q2ml2 + 2ml2)r3+ ( e2q4m2l2--q4m2l2 - 4q2l2 - 1)r2+(2q4ml2+q2m+2m)r-2 q2 = rgm(r ) ,( drd)2mag=(e2l2 -l2)r4+(-2e2q2ml2+q2ml2+ 2ml2)r3++ ( e2q4m2l2 - 2q2l2 -1)r2+(q2m+2m)r-2 q2 =rmag(r ) , ( drd)2el=(e2l2 -l2)r4+(2e2q2ml2-q2ml2+ 2ml2)r3++ ( e2q4m2l2 - 1)r2+(-q2m+2m)r= rel(r ) , and ( drdt)gm2=1e2(1 - 2mr)2[e2 -(1 - 2mr)(l2r(r - q2m)+ ) ] , ( drdt)mag2=1e2(1 - 2mr)2 ( 1-q2mr)2[(1-q2mr)2 e2-(1 - 2mr)(1-q2mr)(l2r2 + ) ] , ( drdt)el2=1e2 ( 1+q2 - 2m2mr)2(1+q2mr)2[ ( 1+q2mr)2e2-(1+q2 - 2m2mr)(l2r2 + ) ] .eqs .( eqref13 ) -( secref4 ) gives a complete definition of the dynamics .in these set of equations , the values of inlineform25 and inlineform26 in the right hand side refer to indices of the left hand side of them , that we ignore indices for inlineform27 and inlineform28 for simplicity .we get the effective potential by comparing eqs .( eqref13 )-( eqref15 ) with inlineform29 , displayform0 displayform1which depends on radial coordinate inlineform30 , charge inlineform31 and mass inlineform32 of the black hole , the type of the geodesics inlineform33 and the angular momentum inlineform34 of the particles .in these set of equations , again the value of inlineform35 in the right hand side refer to indices of the left hand side of them , that we ignore indices for inlineform36 and inlineform37 for simplicity .we introduce dimensionless quantities for rescale the parameters displayform0and rewrite eqs .( secref4 )-( secref4 )as ( d rd)2gm=(e2-)lr4+(2 q2 + 2 - 2 e2 q2 )l r3+( e2q4 l-q4l-4 q2 l -1 ) r2 +( 2q4l + q2+2 )r -2 q2 = rgm(r ), ( d rd)2mag=(e2 - ) l r4 +( q2 -2 q2 e2+2)l r3+( q4 e2 l - 2q2l-1)r2+(q2 +2)r -2q2=rmag(r ) , ( d rd)2el=(e2 - ) l r4 +( 2 q2 e2-q2+2)l r3+ ( q4 e2l -1)r2+(-q2+2)r= rel(r ) .also , in these set of equations , the values of inlineform38 and inlineform39 in the right hand side refer to indices of the left hand side of them , that we ignore indices for inlineform40 and inlineform41 for simplicity .\n\nanalytical solution of geodesic equations: in this section , we present the solution of the equations of motion analytically .in the eqs .( secref4 ) -( secref4 ) for the test particle ( inlineform42 ) and light ray ( inlineform43 ) , we have polynomials of degree four in the form inlineform44 , with only simple zeros , which for solving them in this way , we can apply up to two substitutions .the first substitution is inlineform45 , where inlineform46 is a zero of inlineform47 , transforms the problem to displayform0with a polynomial inlineform48 of degree 3 .where displayform0in which inlineform49 is an arbitrary constant for each metric which is related to the parameter of the relevant metric .a second substitution inlineform50 , changes inlineform51 , into the weierstrass form displayform0where displayform0are the weierstrass invariants .the differential equation ( eqref23 ) is of elliptic type and we used the weierstrass inlineform52 function to solve it   ,   displayform0where inlineform53 with inlineform54 depends only on the initial values inlineform55 and inlineform56 .therefore , the solution of eqs.()- ( secref4 ) takes the form displayform0this is the analytic solution of the equation of motion of a test particle and light ray in a gmghs , magnetically charged gmghs and electrically charged gmghs spacetimes .this solution is valid in all regions of this spacetimes .\n\norbits: in a special spacetime with electric charge , the shape of an orbit depends on three paremeters , in which the angular momentum , inlineform57 and the energy , inlineform58 are the specifications of test particle or light ray and the electric charge , inlineform59 comes from the related spacetime ( the mass can be absorbed through a rescaling of the radial coordinate ) .the polynomial inlineform60 defined in eqs .( secref4 ) -( secref4 ) are included all these quantities .since inlineform61 should be real and positive , the physically admissible regions are given by those inlineform62 for which inlineform63 which is presented on the left hand side of eqs .( eqref13 )-( eqref15 ) .therefore , the form of the resulting orbits are characterized uniquely by the number of positive real zeros of inlineform64 .in the following , we introduce different types of orbit .let inlineform65 be the outer event horizon and inlineform66be the inner horizon .escape orbit ( eo ) with rangeinlineform67 with inlineform68 , or with range inlineform69with inlineform70 .two - world escape orbit ( teo ) with range inlineform71where inlineform72 .bound orbit ( bo ) with range inlineform73 with  inlineform74 , or  inlineform75 .many - world bound orbit ( mbo ) with range inlineform76where inlineform77 and inlineform78 .terminating orbit ( to ) with ranges either inlineform79 or inlineform80 with  inlineform81 , or  inlineform82 .other types of orbits are exceptional and treated separately .they are connected with the appearance of multiple zeros in inlineform83 or with parameter values which reduce the degree of inlineform84 .in both cases the differential eqs .( secref4 ) -( secref4 ) have much simplified structure .these orbits are radial geodesics with inlineform85 , circular orbits with constant inlineform86 and orbits asymptotically approaching circular orbits .defining the borders of inlineform87 or , equivalently , inlineform88 is done by the four regular types of geodesic motion correspond to various arrangements of the real and positive zeros of inlineform89 .if inlineform90 has no real and positive zeros , a terminating orbit is possible if inlineform91 for all inlineform92 , but else no geodesic motion is allowed .if inlineform93 has at least one real and positive zero then an escape orbit is possible , or a terminating orbit if inlineform94 for inlineform95 , where , inlineform96 is the smallest positive zero .if inlineform97 has at least two real zeros , inlineform98 with inlineform99 for inlineform100 a bound orbit is permitted .if inlineform101 is such that , multiple types of orbits are possible , then the actual orbit depends on the initial position of the test particle or light ray .in the following , we will analyse possible types of orbits .the major point in this analysis is that eqs .( secref4 ) -( secref4 ) impliesinlineform102 , as a necessary condition for the existence of a geodesic .thus , the zeros of inlineform103 , are extremal values of inlineform104 and determine ( together with the sign of inlineform105 between two zeros )the type of geodesic .the polynomial inlineform106 is in our metrics of degree 4 and , therefore , has 4 ( complex ) zeros of which the positive real zeros are of interest for the type of orbit .for a given set of parameters inlineform107 , and inlineform108 the polynomial inlineform109 has a certain number of positive real zeros .if inlineform110 and inlineform111 are varied , this number can change only if two zeros of inlineform112 merge to one .solving inlineform113 , inlineform114 for inlineform115 and inlineform116 , for inlineform117 yields egm2 = q2r2 - 4q2r-2r3 + 4q2 + 8r2 - 8r(q2r-4q2 - 2r2 + 6r)r , lgm=-12q2r-4q2 - 2r2 + 6rr(q4 - 2q2r+r2 ) ,emag2=-2(r2 - 4r+4)q2r-4q2 - 2r2 +6r , lmag = q2r-4q2 - 2r2+ 6rr3(q2 - 2 ) , eel2=2(q4 + 2q2r-4q2+r2 - 4r+4)r(q2+r)(q4 + 3q2r-2q2 + 2r2 - 6r ) , lel= q4 + 3q2r-2q2+ 2r2 - 6rr2(q4+q2r-2q2 + 2r ) .in fig .figref37( a ) , the result of this analysis is shown for test particles ( inlineform118 ) .for light rays inlineform119 , the analysis is the same as in the inlineform120 case and the result of this analysis is shown in fig .figref37( b ) .four different regions can be identified in fig .figref37 , for timelike geodesic but for null geodesic , in fig .figref37( b ) , we have only two regions .it should be noted that , for electrically charged gmghs metric , the number of zeros is one more less than zeros of gmghs and magnetically charged gmghs metrics in each region .summary of possible orbit types can be found in tabels tabref40 and tabref41 .in the following , we give a list of existence regions ( for fig .figref37and tables tabref40 and tabref41 ) .for each region , examples of effective potentials and possible orbit types are demonstrated in figs .figref46 \u2013 figref56 .in region i , for gmghs and magnetically charged gmghs black holes , inlineform121 has 4 positive real zerosinlineform122 with inlineform123 for inlineform124 and inlineform125 .therefore the possible orbit types are bound and mani - world bound orbits ,respectively [ see figs .figref46( a ) , figref51 ( a ) , and figref51 ( d ) ] .but in this region for electrically charged gmghs black hole , inlineform126 has 3 positive real zeros inlineform127 with inlineform128 for inlineform129 and inlineform130 .therefore the possible orbit types are terminating and bound orbits , respectively [ see figs .figref46( c ) ,figref56( a ) , and figref56 ( b ) ] .in region ii , for gmghs and magnetically charged gmghs black holes , inlineform131 has 3 positive real zeros inlineform132 with inlineform133 for inlineform134 and inlineform135 .therefore the possible orbit types are mani - world bound and escape orbits , respectively [ see figs .figref46( b ) , figref51 ( d ) , and figref51 ( c ) ] .but in this region for electrically charged gmghs black hole , inlineform136 has 2 positive real zeros inlineform137 with inlineform138 for inlineform139 and inlineform140 .therefore the possible orbit types are terminating and escape orbits , respectively [ see figs .figref46( d ) , figref56( a ) and figref56( c ) ] .in region iii , for gmghs and magnetically charged gmghs black holes , inlineform141 has 1 positive real zero inlineform142 with inlineform143 for inlineform144 .therefore the possible orbit type is the two - world escape orbit [ see figs .figref46( b ) and figref51 ( b ) ] .but in this region for electrically charged gmghs black hole , inlineform145 has 0 positive real zeros and inlineform146 for inlineform147 .therefore the possible orbit type is terminating orbit [ see figs .figref46( d ) and figref56( a ) ] .in region iv , for gmghs and magnetically charged gmghs black holes , inlineform148 has 2 positive real zeros inlineform149 ,inlineform150 with inlineform151 for inlineform152 .therefore the possible orbit type is mani - world bound orbit [ see figs .figref46( b ) and figref51 ( d ) ] .but in this region for electrically charged gmghs black hole , for electrically charged gmghs black hole , inlineform153 has 1 positive real zero inlineform154 with inlineform155 for positive inlineform156 .therefore the possible orbit type is terminating orbit [ see figs .figref46( d ) and figref56( a ) ] .\n\nastrophysical applications: the deflection of light in a schwarzschild de sitter spacetime was discussed by rindler and ishak   .though the equation of motion is the same as in schwarzschild spacetime for identical periapsisinlineform157 , they showed that the measuring process for angles reintroduces the effect of the cosmological constant .also , this method was applied in a more general solution of weyl conformal gravity in refs .   .according to their scheme , they used the invariant formula for the cosine of the angle between two coordinate directions inlineform158 and inlineform159 , as ( ) =gij di j(gijdidj)12(gij ij)12 .for our purpose the relevant inlineform160 is ( grr)gm=( 1 - 2mr)-1 , ( g)gm = r(r - q2 m ) ,( grr)mag= ( ( 1 - 2mr)(1-q2mr ) ) -1 , ( g)mag = r2 ,( grr)el= ( 1+q2 - 2m2mr )-1 ,( g)mag = r2 .then , if we call the direction of the orbit inlineform161 and that of the coordinate lineinlineform162 , we have displayform0substituted into ( uid61 ) , these values yield ( ) =( drd)(drd)2 + ( ggrr ) , or more conveniently , the exact angle between the radial direction and the spatial direction of the light ray is now given by ( )= ggrrdrd .thus , according to eq .( uid61 ) , we have ( ) gm =( 1 - 2mr ( ) )rp(rp - q2 m )( 1 - 2mrp ) r()(r ( ) - q2 m ) -( 1 - 2mr ( ) )rp(rp - q2 m ) , for gmghs spacetime ,( ) mag =rp4(1 - q2mrp)2rp2 ( 1 - 2mrp )( 1 - q2mrp ) r()4 ( 1 - q2mr())2 (r()2(1 - 2mr())(1-q2mr ( ) ) )-rp4(1 - q2mrp)2 , for magnetically charged gmghs spacetime , and ( )el= r()2 ( 1+q2 - 2m2mr ( ) ) rp2(rp+q2m)2rp2 ( 1+q2 - 2m2mrp)-r()2 ( 1+q2 - 2m2mr( ) ) rp2(rp+q2m)2 , for electrically charged gmghs spacetime .where , inlineform163 is the solutions of eqs .( secref4 ) -( secref4 ) which solved in eq .( eqref26 ) for inlineform164 .these now are valid for all light rays , not only for those rays showing a small deflection as discussed in refs .   .in the case that inlineform165 ( eqs .( secref4 ) -( secref4 ) )has at least two positive zeros, we may have a bound orbit for some initial values .the periastron advance inlineform166 for such a bound orbit is given by the difference of the inlineform167 -periodicity of the angle inlineform168 and the periodicity of the solutioninlineform169 displayform0the equations of motion for the static dilaton ( gmghs , magnetically gmghs and electrically gmghs )black hole and the rotating dilaton ( the ker - sen dilaton - axion ) black hole , which is described in sec .secref69 , are polynomials of degree four and can be solved in terms of weierstrass elliptic functions similarly .so , for example , we consider the electrically gmghs black hole and calculate physical datai.e. the aphelinlineform170 , the perihel inlineform171 , and the perihelion advance .for the calculation of the perihelion precession and the orbital characteristics of mercury we use the following values for the physical constants : displayform0as free parameters we may use inlineform172 and inlineform173 given in refs .displayform0the two half - periods inlineform174 and inlineform175 are given by the following abelian integrals displayform0the results are shown in table tabref68 .here , we used the rotation period 87.97 days of mercury and 100 si - years per century to determine the unit inlineform176 inlineform177 .it can be seen from table tabref68 , that , by increasing the value of charge , the values of inlineform178 and inlineform179 , decrease , but the value of inlineform180 , increases .moreover , the results all and particular the case of inlineform181 , compare well to the results in ref .  ,   and also to observations   .\n\nrotating dilaton black hole: in this section , we will discuss the geodesics in the rotating ( the ker - sen dilaton - axion )dilaton black hole and present analytical solutions of the equations of motion .\n\nmetric: in 1992 , sen   was able to find a charged , stationary , axially - symmetric solution   of the field equations by using target space duality , applied to the classical ker solution .the line element of this solution can be written , in generalized boyer \u2013 linquist coordinates , as ds2=-(1 - 2mr2)dt2+2dr2+ 2d2- 4mra22dtd+ ( r(r+r)+a2 + 2mra222)2d2 , where r = r(r+r)-2mr+a2 , 2 = r(r+r)+a22 .here inlineform182 is the mass of the black hole , inlineform183 is the angular momentum per unit mass of the black hole inlineform184 and inlineform185 , where q is the charge of the black hole .for inlineform186 , the kerr - sen black hole reduces to the gibbons - maeda - garfinkle - horowitz - strominger ( gmghs )black hole and for inlineform187, we get kerr black hole .further if both inlineform188and inlineform189 then it reduces to schwarzschild black hole .\n\nconclusions: in this paper , we considered the motion of test particles and light rays in the spacetime of the static ( gmghs , magnetically charged gmghs and electrically charged gmghs ) andthe rotating ( ker - sen dilaton - axion ) dilaton black holes .we have derived geodesic equations of motion and classified them according to their energy inlineform346 and angular momentuminlineform347 .the geodesic equations of motion can be solved in terms of the elliptic weierstrass inlineform348 , inlineform349 and inlineform350 functions .possible types of orbits were derived using analytical solutions , effective potential techniques and parametric diagrams .for electrically charged gmghs black hole , eo , to and bo are possible , while for gmghs and magnetically charged gmghs black holes , eo , teo , bo and mbo are possible and any type of terminating orbit are not possible for these metrics .also , for rotating ( ker - sen dilaton - axion ) dilaton black hole , tro , eo , teo , cteo , to , bo and mbo are possible .some observational phenomena such as the periastron shift of bound orbits and the deflection angle of light , are some results of these solutions .at the end of sec .secref2 , we calculated such astrophysical applications .moreover , it would be interesting to use the results of this paper to study the shadow of dilaton black holes .we would like to thank anonymous referee for useful comments .also we would like to thank bahareh hoseini for helpful discussions and her guidance .",
      "target_text": "in this paper , we consider the timelike and null geodesics around the static [ gmghs ( gibbons , maeda , garfinkle , horowitz and strominger ) , magnetically charged gmghs , electrically charged gmghs ] andthe rotating ( kerr - sen dilaton - axion ) dilaton black holes .the geodesic equations are solved in terms of weierstrass elliptic functions .to classify the trajectories around the black holes , we use the analytical solution and effective potential techniques and then characterize the different types of the resulting orbits in terms of the conserved energy and angular momentum .also , using the obtained results we study astrophysical applications .",
      "output_text": "thea: and to of"
    },
    {
      "input_text": "Title: f(T) modified teleparallel gravity models as an alternative for holographic and new agegraphic dark energy models\nDomains: Physics\n\nintroduction: recent observational data coming from type ia supernovae ( sneia ) surveys , large scale structure ( lss ) , and the cosmic microwave background ( cmb ) anisotropy spectrum points toward the picture of a spatially flat universe undergoing an accelerated expansion driven by a dominant negative pressure fluid , typically referred to as dark energy ( de )   .it is shown that de takes up about two - thirds of the total energy density from cosmic observations .although the nature and cosmological origin of de is still enigmatic at the present , a great variety of models have been proposed to describe the de ( for reviewsee   ,   ) .two promising candidates are the holographic de ( hde )   and the agegraphic de ( ade )   models which are originated from some considerations of the features of the quantum theory of gravity .the hde model is motivated from the holographic principle   .following guberina et al .  , the hde density can be derived from the entropy bound .in the thermodynamics of the black hole   , there is a maximum entropy in a box of size inlineform0 , namely , the bekenstein - hawking entropy bound inlineform1 , which scales as the area of the box inlineform2 , rather than the volume inlineform3 .here inlineform4 is the reduced planck mass inlineform5 .also for a macroscopic system in which self - gravitation effects can be disregarded , the bekenstein entropy bound inlineform6 is given by the product of the energy inlineform7 and the length scale ( ir cut - off ) inlineform8 of the system .here inlineform9 is the quantum zero point energy density caused by the uv cut - off inlineform10 .requiring inlineform11 , namely inlineform12 , one has inlineform13 .if the largest cut - off inlineform14 is taken for saturating this inequality , we get the energy density of the hde as displayform0where inlineform15 is a numerical constant .recent observational data , which have been used to constrain the hde model , show that for the non - flat universe inlineform16   , and for the flat case inlineform17   .li   showed that the cosmic coincidence problem can be resolved by inflation in the hde model , providing the minimal number of e - foldings .the hde models have been studied widely in the literature   .indeed , the definition and derivation of the hde density depends on the entropy - area relationship inlineform18 , where inlineform19 is the area of horizon .however , this definition can be modified by the inclusion of quantum effects , motivated from the loop quantum gravity ( lqg ) .these quantum corrections provided to the entropy - area relationship lead to the curvature correction in the einstein - hilbert action and vice versa   .the corrected entropy takes the form  displayform0where inlineform20 and inlineform21 are dimensionless constants of order unity .determination of the exact values of these constants is still an open issue in quantum gravity .these corrections arise in the black hole entropy in the lqg due to thermal equilibrium fluctuations and quantum fluctuations   .taking the corrected entropy - area relation ( eqref4 ) into account , and following the derivation of the hde ( especially the one shown in   ) , the energy density of the hde will be modified .on this basis , wei   proposed the energy density of the so - called \u201c entropy - corrected hde \u201d ( echde ) in the form displayform0where inlineform22 and inlineform23 are dimensionless constants of order unity .in the special case inlineform24 , the above equation yields the well - known hde density ( eqref3 ) .since the last two terms in eq .( eqref5 ) can be comparable to the first term only when inlineform25 is very small , the corrections make sense only at the early stage of the universe .when the universe becomes large , the echde reduces to the ordinary hde model .the echde models have arisen a lot of enthusiasm recently and have examined in ample detail by   .the ade model is originated from the uncertainty relation of quantum mechanics together with the gravitational effect in general relativity ( gr ) .the ade model assumes that the observed de comes from the spacetime and matter field fluctuations in the universe .following the line of quantum fluctuations of spacetime , karolyhazy et al .  discussed that the distance inlineform26 in minkowski spacetime can not be known to a better accuracy than inlineform27 , where inlineform28 is the reduced planck time .based on the karolyhazy relation , maziashvili   discussed that the energy density of the metric fluctuations of the minkowski spacetime is given by inlineform29 .based on the karolyhazy relation   and maziashvili arguments   , cai proposed the original ade model to explain the accelerated expansion of the universe   .the original ade has the energy density inlineform30 , where inlineform31 is the age of the universe   .also the numerical factor 3 inlineform32 is introduced to parameterize some uncertainties , such as the species of quantum fields in the universe , the effect of curved spacetime ( since the energy density is derived for minkowski spacetime ) , and so on .however , the original ade model had some difficulties .for example it suffers from the difficulty to describe the matter - dominated epoch .therefore , a new model of the ade was proposed by wei and cai   , while the time scale is chosen to be the conformal time instead of the age of the universe .the energy density of the new ade ( nade ) is given by   displayform0where inlineform33 is the conformal time of the frw universe .the joint analysis of the astronomical data for the nade model in flat universe gives the best - fit value ( with 1 inlineform34 uncertainty ) inlineform35   .it was found that the coincidence problem could be solved naturally in the nade model   .the ade models have been examined and studied in ample detail by   .more recently , very similar to the echde model , the energy density of the entropy - corrected nade ( ecnade ) was proposed by wei   as displayform0in the special case inlineform36 , eq .( eqref7 )yields the nade density ( eqref6 ) .the motivation for taking the energy density of the modified nade in the form ( eqref7 ) comes from the fact that both the nade and hde models have the same origin .indeed , it was argued that the nade models are the hde model with different ir lengthscales   .the ecnade has been investigated in ample detail in   .one of among other interesting alternative proposals for de is modified gravity .it can explain naturally the unification of earlier and later cosmological epochs ( for review see   ) .moreover , modified gravity may serve as dark matter   .there are some classes of modified gravities containing inlineform37 , inlineform38 and inlineform39 which are considered as gravitational alternatives for de   .here the lagrangian density of modified gravity theories inlineform40 is an arbitrary function of inlineform41 , inlineform42 or both inlineform43 and inlineform44 .the field equations of these modified gravity theories are the 4th order that making it difficult obtain both exact and numerical solutions .recently , a new modified gravity model was proposed by bengochea and ferraro   to describe the present accelerating expansion of the universe without resorting to de .instead of using the curvature defined via the levi - civita connection in gr , the weitzenb\u00f6ck connection is used in teleparallel gravity   .as a result , the spacetime has no curvature but contains torsion .similar to gr where the action is a curvature scalar inlineform45 , the action of teleparallel gravity is a torsion scalar inlineform46 .following this line and in analogy to the inlineform47 theory , bengochea and ferraro   suggested a new model , named inlineform48 theory , by generalizing the action of teleparallel gravity as a function of the torsion scalar inlineform49 , and found that it can explain the observed acceleration of the universe .indeed , there are some terms in the modified friedmann equation in inlineform50 -gravity that can be identified as the effective de to produce the accelerated expansion of the late - time universe   .models based on modified teleparallel gravity may also provide an alternative to inflation   .another advantage of inlineform51 theory is that its field equations are the second order which are remarkably simpler than the fourth order equations of inlineform52 theory   .recently , inlineform53 -gravity has been extensively studied in the literature   .viewing the inlineform54 modified gravity model as an effective description of the underlying theory of de , and considering the original and entropy - corrected version of the hde and nade scenarios as pointing in the direction of the underlying theory of de , it is interesting to study how the inlineform55 -gravity can describe the hde , echde , nade and ecnade densities as effective theories of de models .this motivated us to establish different models of inlineform56 -gravityaccording to the original and entropy - corrected version of the hde and nade scenarios .this paper is organized as follows .in section 2 , we review the theory of inlineform57 -gravity in the metric formalism .in sections 3 , 4 , 5 and 6 , we reconstruct different inlineform58-gravity models corresponding to the hde , echde , nade and ecnade models , respectively .section 7 is devoted to our conclusions .\n\nf(t)f(t) modified teleparallel gravity: in the framework of inlineform59 theory , the action of modified teleparallel gravity is given by   displayform0where inlineform60and inlineform61 .also inlineform62and inlineform63 are the torsion scalar and the lagrangian density of the matter inside the universe , respectively .note that inlineform64 is the vierbein field which is used as a dynamical object in teleparallel gravity and has the following orthonormal property  displayform0where inlineform65 .each vector inlineform66 can be described by its components inlineform67 , where inlineform68 refers to the tangent space of the manifold and inlineform69 labels coordinates on the manifold .the metric tensor is obtained from the dual vierbein as displayform0the torsion scalar inlineform70 is defined as   displayform0where the non - null torsion tensor inlineform71 is displayform0and displayform0also inlineform72 is the contorsion tensor defined as displayform0taking the variation of the action ( eqref8 ) with respect to the vierbein , one can obtain the field equations in inlineform73 modified teleparallel gravity as   displayform0where subscript inlineform74 denotes a derivative with respect to inlineform75 , inlineform76and inlineform77 is the matter energy - momentum tensor .the set of field equations ( eqref15 ) are the 2nd order which makes them simpler than the corresponding field equations in the other modified gravity theories like inlineform78 , inlineform79 and inlineform80   .now if we consider the spatially - flat frw metric for the universe as displayform0where inlineform81 is the scale factor , then from eq .( eqref10 )one can obtain displayform0substituting the vierbein ( eqref17 ) into ( eqref11 ) yields   displayform0where inlineform82 is the hubble parameter .taking inlineform83 for the matter energy - momentum tensor in the prefect fluid form and using the vierbein ( eqref17 ) , the set of field equations ( eqref15 ) for inlineform84 reduces to   displayform0and for inlineform85 yields displayform0here inlineform86and inlineform87 are the total energy density and pressure of the matter inside the universe , respectively , and satisfy the conservation equation displayform0note that eqs .( eqref19 ) and ( eqref20 ) are the modified friedmann equations in the framework of inlineform88 -gravity in the spatially - flat frw universe .one can rewrite eqs .( eqref19 ) and ( eqref20 ) as   displayform0 displayform1where displayform0 displayform1are the torsion contribution to the energy density and pressure which satisfy the energy conservation law displayform0in the case of inlineform89 , from eqs .( eqref24 )and ( eqref25 )we have inlineform90 and inlineform91 .therefore , eqs .( eqref22 ) and ( eqref23 ) are transformed to the usual friedmann equations in gr .the equation of state ( eos ) parameter due tothe torsion contribution is defined as displayform0note that for the de sitter universe , i.e. inlineform92 , we have inlineform93 which behaves like the cosmological constant .also for a inlineform94 -dominated universe , eq .( eqref22 ) yields displayform0taking time derivative of the above relation and using the continuity equation ( eqref26 ) , one can get the eos parameter as displayform0which shows that for the phantom - dominated , inlineform95 , and quintessence - dominated , inlineform96 , universe , we need to have inlineform97 and inlineform98 , respectively .for a given inlineform99 , with the help of eqs .( eqref24 )and ( eqref25 )one can reconstruct the inlineform100 -gravityaccording to any de model given by the eos inlineform101 or inlineform102 .there are two classes of scale factors which usually people consider for describing the accelerating universe in inlineform103 , inlineform104 and inlineform105 modified gravities   .the first class of scale factors is given by   ,   displayform0using eqs .( eqref18 )and ( eqref30 ) one can obtain displayform0in which the last relation inlineform106 shows that the model ( eqref30 ) is corresponding to a phantom - dominated universe .this is why in the literature the model ( eqref30 ) is usually so - called the phantom scale factor .for the second class of scale factors defined as   displayform0one can obtain displayform0in which the last relation inlineform107 reveals that the model ( eqref32 ) describes a quintessence - dominated universe .due to this fact , the model ( eqref32 ) is so - called the quintessence scale factor in the literature .in the next sections , using the two classes of scale factors ( eqref30 ) and ( eqref32 ) , we reconstruct different inlineform108-gravitiesaccording to the hde , echde , nade and ecnade models .\n\nholographic f(t)f(t)-gravity model: here we reconstruct the inlineform109 -gravity from the hde model .note that in eq .( eqref3 ) , taking inlineform110 as the size of the current universe , for instance , the hubble scale , the resulting energy density is comparable to the present day de .however , as hsu found in   , in this case , the evolution of the de is the same as that of dark matter ( dust matter ) , and therefore it can not drive the universe to accelerated expansion .the same appears if one chooses the particle horizon of the universe as the length scale inlineform111   .to obtain an accelerating universe , li   proposed that for a flat universe , inlineform112 should be the future event horizon inlineform113 .following li  the hde density with the ir cut - off inlineform114 is given by displayform0where the future event horizon inlineform115 is defined as displayform0for the first class of scale factors ( eqref30 ) and using eq .( eqref31 ) , the future event horizoninlineform116 yields displayform0inserting eq .( eqref36 ) into ( eqref34 ) one can get displayform0where displayform0equating ( eqref24 ) with ( eqref37 ) , i.e. inlineform117, we obtain the following differential equation displayform0solving eq .( eqref39 ) yields the holographic inlineform118 -gravity model as displayform0where inlineform119 is an integration constant .substituting eq .( eqref40 ) into ( eqref27 ) one can obtain the eos parameter of the torsion contribution as displayform0which is always smaller than inlineform120 and corresponds to a phantom accelerating universe .recent observational data indicates that the eos parameter inlineform121 at the present lies in a narrow strip around inlineform122 and is quite consistent with being below this value   .for the second class of scale factors ( eqref32 ) and using eq .( eqref33 ) , the future event horizoninlineform123 reduces to displayform0where the condition inlineform124 is obtained due to having a finite future event horizon .if we repeat the above calculations then we can obtain the both inlineform125 and inlineform126corresponding to the hde for the second class of scale factors ( eqref32 ) .the result for inlineform127 is the same as ( eqref40 )where displayform0also the eos parameter is obtained as displayform0which describes an accelerating universe with the quintessence eos parameter , i.e. inlineform128 .it should be mentioned that for inlineform129 , the eos parameter ( eqref44 ) also takes place in the range of inlineform130 .\n\nentropy-corrected holographic f(t)f(t)-gravity model: the echde density ( eqref5 ) with the ir cut - off inlineform131 yields displayform0for the first class of scale factors ( eqref30 ) , substituting eq .( eqref36 ) into ( eqref45 )one can get displayform0where displayform0equating ( eqref24 ) with ( eqref46 ) one can get displayform0solving the differential equation ( eqref48 ) yields the entropy - corrected holographicinlineform132-gravity model as displayform0where inlineform133 is an integration constant .substituting eq .( eqref49 ) into ( eqref27 )one can get displayform0if we set inlineform134and inlineform135 then eqs .( eqref49 ) and( eqref50 ) reduce to ( eqref40 ) and ( eqref41 ) , respectively .note that the time - dependent eos parameter ( eqref50 ) in contrast with constant eos parameter ( eqref41 ) can justify the transition from the quintessence state , inlineform136 , to the phantom regime , inlineform137 , as indicated by recent observations   .to illustrate this transition in ample detail , the eos parameter of the entropy - corrected holographic inlineform138 -gravity model , eq .( eqref50 ) , versus redshift inlineform139 for the first class of scale factors , eq .( eqref30 ) , is plotted in fig .figref73 .note that the torsion scalar inlineform140 can be expressed in terms of redshiftinlineform141 .for the first class of scale factors ( 28 ) one can obtain inlineform142figure figref73 presents that for a set of free parametersinlineform143   , inlineform144 , inlineform145 and inlineform146 , inlineform147 crosses the inlineform148 line twice .at the transition redshift inlineform149 , we have a direct transition from inlineform150 ( quintessence phase ) to inlineform151 ( phantom phase ) .whereas at inlineform152 , the crossing direction is opposite ,i.e. inlineform153 .crossing the inlineform154 line twice in the direct and opposite transitions is in agreement with that obtained recently for some inlineform155 -gravity models   .considering eqs .( eqref50 ) and ( eqref46 )it seems that at inlineform156 , a singularity in inlineform157 and a change of sign in inlineform158 appear .regarding inlineform159 , fig .figref73 shows that the eos parameter of the entropy - corrected holographic inlineform160 -gravity model , eq .( eqref50 ) , does not show any singularity .to check the change of sign in inlineform161 given by eq .( eqref46 ) , we plot it in fig .figref74 .figure figref74 clears that for the first class of scale factors although a future big rip singularity in the echde density ( inlineform162 ) occurs at inlineform163 ( or inlineform164 ) , the sign of inlineform165 does not change .also the eos parameter remains finite at the future big rip singularity when inlineform166 ( see again fig .figref73 ) .it is also interesting to note that fig .figref74presents that the local minimum and maximum points of inlineform167 occur at the transition redshifts when inlineform168 ( see fig .figref73 ) .this can also be shown analytically .from eq .( eqref24 ) , inlineform169 yields inlineform170inserting the above relation into eq .( eqref27 ) gives inlineform171 .for the second class of scale factors ( eqref32 ) , the resulting inlineform172 is the same as eq .( eqref49 )where displayform0also the eos parameter is obtained as displayform0here also in order to make inlineform173 be finite , the parameter inlineform174 should be in the range of inlineform175 .one notes that the dynamical eos parameter ( eqref52 ) in contrast with the constant eos parameter ( eqref44 ) can accommodate the transition from inlineform176 to inlineform177 at recent stage .figure figref75 displays the evolution of the eos parameter of the entropy - corrected holographic inlineform178 -gravity model , eq .( eqref52 ) , versus redshif inlineform179 for the second class of scale factors , eq .( eqref32 ) .in this case , the torsion scalar inlineform180 can be expressed in terms of redshift inlineform181 as inlineform182figure figref75 like fig .figref73 shows that the inlineform183 line is crossed twice for another values set of the free parameters , inlineform184   , inlineform185 , inlineform186 and inlineform187 .at inlineform188 we have a direct transition ( i.e. inlineform189 ) .also an opposite transition occurs in the future at inlineform190 .furthermore , fig .figref75 clears that there is no any singularity in the dynamical eos parameter ( eqref52 ) .note that also the sign of the echde density ( eqref45 ) for the second class of scale factors ( eqref32 ) remains unchanged ( see fig . figref76 ) .\n\nnew agegraphic f(t)f(t)-gravity model: for the nade density  displayform0the conformal time inlineform191 of the frw universe is defined as displayform0for the first class of scale factors ( eqref30 ) , the conformal time inlineform192 by the help of eq .( eqref31 )yields displayform0substituting eq .( eqref55 ) into ( eqref53 )gives displayform0where displayform0equating ( eqref24 ) with ( eqref56 ) yields displayform0solving eq .( eqref58 ) results in the new agegraphicinlineform193-gravity model as displayform0where inlineform194 is an integration constant .inserting eq .( eqref59 ) into ( eqref27 ) gives displayform0which is always smaller than inlineform195 like the eos parameter of the holographic inlineform196 -gravity model ( eqref41 ) , and it behaves as a phantom type de .for the second class of scale factors ( eqref32 ) and using ( eqref33 ) , the conformal time inlineform197 is obtained as displayform0where the condition inlineform198 is necessary due to having a finite conformal time .the resulting inlineform199 is displayform0where displayform0also the eos parameter of the new agegraphicinlineform200 -gravity model is obtained as displayform0which shows a quintessence - like eos parameter inlineform201 .here in order to have inlineform202 , the parameter inlineform203 should be in the range of inlineform204 .\n\nentropy-corrected new agegraphic f(t)f(t)-gravity model: here , we reconstruct the inlineform205 -gravity model corresponding to the ecnade density  displayform0which closely mimics to that of the echde density ( eqref45 ) and inlineform206 is replaced with the conformal time inlineform207 .for the first class of scale factors ( eqref30 ) , substituting eq .( eqref55 ) into ( eqref65 )one can get displayform0where displayform0equating ( eqref24 ) with ( eqref66 ) gives displayform0solving the differential equation ( eqref68 ) one can obtain the entropy - corrected new agegraphic inlineform208 -gravity model as displayform0where inlineform209 is an integration constant .inserting eq .( eqref69 ) into ( eqref27 ) yields displayform0if we set inlineform210and inlineform211 then eqs .( eqref69 ) and ( eqref70 ) reduce to ( eqref59 ) and ( eqref60 ) , respectively .note that the time - dependent eos parameter ( eqref70 ) in contrast with constant eos parameter ( eqref60 ) can justify the transition from inlineform212 to inlineform213 .figure figref77 illustrates the eos parameter of the entropy - corrected new agegraphic inlineform214 -gravity model , eq .( eqref70 ) , for the first class of scale factors , eq .( eqref30 ) .here for a values set of free parametersinlineform215   , inlineform216 , inlineform217 and inlineform218 , the direct and opposite transitions occur at inlineform219 and inlineform220 , respectively .besides , fig .figref77 reveals that there is no any singularity in the dynamical eos parameter ( eqref70 ) .note that here also the sign of the ecnade density ( eqref66 ) for the first class of scale factors ( eqref30 ) does not change ( see fig . figref78 ) .for the second class of scale factors ( eqref32 ) , the resulting inlineform221 is the same as eq .( eqref69 )where displayform0also the eos parameter can be obtained as displayform0here also in order to have a finite conformal timeinlineform222 , the parameter inlineform223 should be in the range of inlineform224 .contrary to the constant eos parameter ( eqref64 ) , the dynamical eos parameter ( eqref72 ) can accommodate the transition from inlineform225 to inlineform226 at recent stage .figure figref79 presents the evolution of the eos parameter of the entropy - corrected new agegraphic inlineform227 -gravity model , eq .( eqref72 ) , for the second class of scale factors , eq .( eqref32 ) .here also like fig .figref77 , for another values set of the free parameters , inlineform228   , inlineform229 , inlineform230 and inlineform231, inlineform232 crosses the inlineform233 line twice at inlineform234and inlineform235 corresponding to the direct and opposite transitions , respectively .besides , fig .figref79presents that there is no any singularity in the dynamical eos parameter ( eqref72 ) .note that here also the sign of the ecnade density ( eqref65 ) for the second class of scale factors ( eqref32 ) does not change ( see fig .figref80 ) .\n\nconclusions: here , we considered the original and entropy - corrected version of the hde and nade models .among various candidates explaining cosmic accelerated expansion , only the hde and nade models are based on the entropy - area relation .however , this definition can be modified by the inclusion of quantum effects , motivated from the lqg .hence the echde and ecnade were introduced by addition of correction terms to the energy densities of the hde and nade , respectively   .we investigated the hde , echde , nade and ecnade in the framework ofinlineform236-gravity .among other approaches related with a variety of de models , a very promising approach to de is related with the modified teleparallel gravity known as inlineform237 -gravity , in which de emerges from the modification of torsion .the class of inlineform238-gravitytheories is an intriguing generalization of einstein 's new gr , taking a curvature - free approach and using a connection with torsion .it is analogous to the inlineform239 extension of the einstein - hilbert action of standard gr , but has the advantage of the second order field equations   .we reconstructed different theories of modified gravity based on the inlineform240 action in the spatially - flat frw universe for two classes of scale factors containing i ) inlineform241 and ii )inlineform242 and according to the original and entropy - corrected version of the hde and nade scenarios .furthermore , we obtained the eos parameters of the corresponding inlineform243 -gravity models .our calculations show that for the first class of scale factors , the eos parameters of the holographic and new agegraphicinlineform244 -gravity models always behave as that of phantom de .whereas for the second class , the eos parameters of the above - mentioned models behave like quintessence eos parameter .the eos parameters of the entropy - corrected holographic and new agegraphicinlineform245 -gravity models can cross the phantom - divide line twice .for the first class of scale factors inlineform246 , the eos parameters of both entropy - corrected holographic and new agegraphic inlineform247-gravity models have an opposite transition ( inlineform248 ) in the far past and a direct transition ( inlineform249 ) in the near past .for the second class of scale factors inlineform250 , the eos parameters of both entropy - corrected holographic and new agegraphic inlineform251 -gravity models have a direct transition in the near past and an opposite transition in the future .it is interesting to note that the direct transition from the non - phantom phase to the phantom one in the near past is consistent with the recent cosmological observational data   .acknowledgementsthe authors thank dr . arash sorouri and dr .mubasher jamil for helping in good english presentation of the paper .the work of k. karami has been supported financially by research institute for astronomy inlineform252 astrophysics of maragha ( riaam ) , maragha , iran .",
      "target_text": "in the present work , we reconstruct different f(t)-gravity models corresponding to the original and entropy - corrected version of the holographic and new agegraphic dark energy models .we also obtain the equation of state parameters of the corresponding f(t)-gravity models .we conclude that the holographic and new agegraphic f(t)-gravity models behave like phantom or quintessence model .whereas in the entropy - corrected models , the equation of state parameter can justify the transition from the quintessence state to the phantom regime as indicated by the recent observations .",
      "output_text": "thea: and to of"
    },
    {
      "input_text": "Title: Piecewise convexity of artificial neural networks\nDomains: Mathematics\n\nintroduction: artificial neural networks are currently considered the state of the art in applications ranging from image classification , to speech recognition and even machine translation .however , little is understood about the process by which they are trained for supervised learning tasks .the problem of optimizing their parameters is an active area both practical and theoretical research .despite considerable sensitivity to initialization and choice of hyperparameters , neural networks often achieve compelling results after optimization by gradient descent methods .due to the nonconvexity and massive parameter space of these functions , it is poorly understood how these sub - optimal methods have proven so successful .indeed , training a certain kind of neural network is known to be np - complete , making it difficult to provide any worst - case training guarantees   .much recent work has attempted to reconcile these differences between theory and practice   .this article attempts a modest step towards understanding the dynamics of the training procedure .we establish three main convexity results for a certain class of neural network , which is the current the state of the art .first , that the objective is piecewise convex as a function of the input data , with parameters fixed , which corresponds to the behavior at test time .second , that the objective is again piecewise convex as a function of the parameters of a single layer , with the input data and all other parameters held constant .third , that the training objective function , for which all parameters are variable but the input data is fixed , is piecewise multi - convex .that is , it is a continuous function which can be represented by a finite number of multi - convex functions , each active on a multi - convex parameter set .this generalizes the notion of biconvexity found in the optimization literature to piecewise functions and arbitrary index sets   .to prove these results , we require two main restrictions on the definition of a neural network : that its layers are piecewise affine functions , and that its objective function is convex and continuously differentiable .our definition includes many contemporary use cases , such as least squares or logistic regression on a convolutional neural network with rectified linear unit ( relu ) activation functions and either max- or mean - pooling .in recent years these networks have mostly supplanted the classic sigmoid type , except in the case of recurrent networks   .we make no assumptions about the training data , so our results apply to the current state of the art in many practical scenarios .piecewise multi - convexity allows us to characterize the extrema of the training objective .as in the case of biconvex functions , stationary points and local minima are guaranteed optimality on larger sets than we would have for general smooth functions .specifically , these points are partial minima when restricted to the relevant piece .that is , they are points for which no decrease can be made in the training objective without simultaneously varying the parameters across multiple layers , or crossing the boundary into a different piece of the function .unlike global minima , we show that partial minima are reliably found by the optimization algorithms used in current practice .finally , we provide some guarantees for solving general multi - convex optimization problems by various algorithms .first we analyze gradient descent , proving necessary convergence conditions .we show that every point to which gradient descent converges is a piecewise partial minimum , excepting some boundary conditions .to prove stronger results , we define a different optimization procedure breaking each parameter update into a number of convex sub - problems .for this procedure , we show both necessary and sufficient conditions for convergence to a piecewise partial minimum .interestingly , adding regularization to the training objective is all that is needed to prove necessary conditions .similar results have been independently established for many kinds of optimization problems , including bilinear and biconvex optimization , and in machine learning the special case of linear autoencoders   .our analysis extends existing results on alternating convex optimization to the case of arbitrary index sets , and general multi - convex point sets , which is needed for neural networks .we admit biconvex problems , and therefore linear autoencoders , as a special case .despite these results , we find that it is difficult to pass from partial to global optimality results .unlike the encouraging case of linear autoencoders , we show that a single rectifier neuron , under the squared error objective , admits arbitrarily poor local minima .this suggests that much work remains to be done in understanding how sub - optimal methods can succeed with neural networks .still , piecewise multi - convex functions are in some senses easier to minimize than the general class of smooth functions , for which none of our previous guarantees can be made .we hope that our characterization of neural networks could contribute to a better understanding of these important machine learning systems .\n\npreliminary material: we begin with some preliminary definitions and basic results concerning continuous piecewise functions .definition 2.1let inlineform0 be continuous functions from inlineform1 .a continuous piecewise function inlineform2 has a finite number of closed , connected sets inlineform3 covering inlineform4such that for each inlineform5 we have inlineform6 for all inlineform7 .the set inlineform8 is called a piece of inlineform9 , and the function inlineform10 is called active on inlineform11 .more specific definitions follow by restricting the functions inlineform12 .a continuous piecewise affine function has inlineform13 where inlineform14and inlineform15 .a continuous piecewise convex function has inlineform16 convex , with inlineform17 convex as well .note that this definition of piecewise convexity differs from that found in the convex optimization literature , which focuses on convex piecewise convex functions , i.e. maxima of convex functions   .note also that we do not claim a unique representation in terms of active functions inlineform18 and pieces inlineform19 , only that there exists at least one such representation .before proceeding , we shall extend definition secref1 to functions of multidimensional codomain for the affine case .definition 2.2a function inlineform20 , and let inlineform21 denote the inlineform22 component of inlineform23 .then inlineform24 is continuous piecewise affine if each inlineform25 is .choose some piece inlineform26 from each inlineform27 and let inlineform28 , with inlineform29 .then inlineform30 is a piece of inlineform31 , on which we have inlineform32 for some inlineform33 and inlineform34 .first , we prove an intuitive statement about the geometry of the pieces of continuous piecewise affine functions .theorem 2.3let inlineform35 be continuous piecewise affine .then inlineform36 admits a representation in which every piece is a convex polytope .let inlineform37 denote the inlineform38 component of inlineform39 .now , inlineform40 can be written in closed form as a max - min polynomial   .that is , inlineform41 is the maximum of minima of its active functions .now , for the minimum of two affine functions we have displayform0this function has two pieces divided by the hyperplane inlineform42 .the same can be said of inlineform43 .thus the pieces of inlineform44 are intersections of half - spaces , which are just convex polytopes .since the pieces of inlineform45 are intersections of the pieces of inlineform46 , they are convex polytopes as well .see figurefigref56in section secref8 for an example of this result on a specific neural network .our next result concerns the composition of piecewise functions , which is essential for the later sections .theorem 2.4let inlineform47 and inlineform48 be continuous piecewise affine .then so is inlineform49 .to establish continuity , note that the composition of continuous functions is continuous .let inlineform50 be a piece of inlineform51 and inlineform52 a piece of inlineform53such that inlineform54 , where inlineform55 denotes the inverse image of inlineform56 .by theorem secref3 , we can choose inlineform57 and inlineform58 to be convex polytopes .since inlineform59 is affine , inlineform60 is closed and convex   .thus inlineform61 is a closed , convex set on which we can write displayform0  thus displayform0  which is an affine function .now , consider the finite set of all such pieces inlineform62 .the union of inlineform63 over all pieces inlineform64 is just inlineform65 , as is the union of all pieces inlineform66 .thus we have inlineform67  thus inlineform68 is piecewise affine on inlineform69 .we now turn to continuous piecewise convex functions , of which continuous piecewise affine functions are a subset .theorem 2.5let inlineform70 be a continuous piecewise affine function , and inlineform71 a convex function .then inlineform72 is continuous piecewise convex .on each piece inlineform73 of inlineform74 we can write displayform0this function is convex , as it is the composition of a convex and an affine function   .furthermore , inlineform75 is convex by theorem secref3 .this establishes piecewise convexity by the proof of theorem secref5 .our final theorem concerns the arithmetic mean of continuous piecewise convex functions , which is essential for the analysis of neural networks .theorem 2.6let inlineform76 be continuous piecewise convex functions .then so is their arithmetic mean inlineform77 .the proof takes the form of two lemmas .lemma 2.7let inlineform78 and inlineform79 be a pair of continuous piecewise convex functions on inlineform80 .then so is inlineform81 .let inlineform82 be a piece of inlineform83 , and inlineform84 a piece of inlineform85 , with inlineform86 .note that the sum of convex functions is convex   .thus inlineform87 is convex on inlineform88 .furthermore , inlineform89 is convex because it is an intersection of convex sets   .since this holds for all pieces of inlineform90 and inlineform91 , we have that inlineform92is continuous piecewise convex on inlineform93 .lemma 2.8let inlineform94 , and let inlineform95 be a continuous piecewise convex function .then so is inlineform96 .the continuous function inlineform97 is convex on every piece of inlineform98 .having established that continuous piecewise convexity is closed under addition and positive scalar multiplication , we can see that it is closed under the arithmetic mean , which is just the composition of these two operations .\n\nneural networks: in this work , we define a neural network to be a composition of functions of two kinds : a convex continuously differentiable objective ( or loss ) function inlineform99 , and continuous piecewise affine functionsinlineform100 , constituting the inlineform101 layers .furthermore , the outermost function must be inlineform102 , so that we have displayform0where inlineform103 denotes the entire network .this definition is not as restrictive as it may seem upon first glance .for example , it is easily verified that the rectified linear unit ( relu ) neuron is continuous piecewise affine , as we have displayform0where the maximum is taken pointwise .it can be shown that maxima and minima of affine functions are piecewise affine   .this includes the convolutional variant , in which inlineform104 is a toeplitz matrix .similarly , max pooling is continuous piecewise linear , while mean pooling is simply linear .furthermore , many of the objective functions commonly seen in machine learning are convex and continuously differentiable , as in least squares and logistic regression .thus this seemingly restrictive class of neural networks actually encompasses the current state of the art .by theorem secref5 , the composition of all layers inlineform105 is continuous piecewise affine .therefore , a neural network is ultimately the composition of a continuous convex function with a single continuous piecewise affine function .thus by theorem secref8 the network is continuous piecewise convex .figure figref17 provides a visualization of this result for the example network displayform0where inlineform106 .for clarity , this is just the two - layer relu network displayform0with the squared error objective and a single data point inlineform107 , setting inlineform108 and inlineform109 , with all other parameters set to 1 .before proceeding further , we must define a special kind of differentiability for piecewise continuous functions , and show that this holds for neural networks .definition 3.1let inlineform110 be piecewise continuous .we say that inlineform111 is piecewise continuously differentiable if each active function inlineform112 is continuously differentiable .to see that neural networks are piecewise continuously differentiable , note that the objective inlineform113 is continuously differentiable , as are the affine active functions of the layers .thus their composition is continuously differentiable .it follows that non - differentiable points are found only on the boundaries between pieces .\n\nnetwork parameters of a single layer: in the previous section we have defined neural networks as functions of labeled data .these are the functions relevant during testing , where parameters are constant and data is variable .in this section , we extend these results to the case where data is constant and parameters are variable , which is the function to optimized during training .for example , consider the familiar equation displayform0with parameters inlineform114 and data inlineform115 ) .during testing , we hold inlineform116 constant , and consider inlineform117 as a function of the data inlineform118 .during training , we hold inlineform119 constant and consider inlineform120 as a function of the parameters inlineform121 .this is what we mean when we say that a network is being \u201c considered as a function of its parameters . \u201dthis leads us to an additional stipulation on our definition of a neural network .that is , each layer must be piecewise affine as a function of its parameters as well .this is easily verified for all of the layer types previously mentioned .for example , with the relu neuron we have displayform0so for inlineform122 we have that the inlineform123 component of inlineform124 is linear in inlineform125 , while for inlineform126 it is constant .to see this , we can re - arrange the elements of inlineform127 into a column vector inlineform128 , in row - major order , so that we have displayform0in section secref3 we have said that a neural network , considered as a function of its input data , is convex and continuously differentiable on each piece .now , a neural network need not be piecewise convex as a function of the entirety of its parameters .however , we can regain piecewise convexity by considering it only as a function of the parameters in a single layer , all others held constant .theorem 4.1a neural network inlineform129 is continuous piecewise convex and piecewise continuously differentiable as a function of the parameters in a single layer .for the time being , assume the input data consists of a single pointinlineform130 .by definitioninlineform131 is the composition of a convex objective inlineform132 and layers inlineform133 , with inlineform134 a function of inlineform135 .let inlineform136 denote the network inlineform137 considered as a function of the parameters of layer inlineform138 , all others held constant .now , the layers inlineform139 are constant with respect to the parameters of inlineform140 , so we can write inlineform141 .thus on each piece of inlineform142 we have displayform0by definitioninlineform143 is a continuous piecewise affine function of its parameters .since inlineform144 is constant , we have that inlineform145 is a continuous piecewise affine function of the parameters of inlineform146 .now , by theorem secref5 we have that inlineform147 is a continuous piecewise affine function of the parameters of inlineform148 .thus by theorem secref8 , inlineform149 is continuous piecewise convex .to establish piecewise continuous differentiability , recall that affine functions are continuously differentiable , as is inlineform150 .having established the theorem for the case of a single data point , consider the case where we have multiple data points , denoted inlineform151 .now , by theorem secref10the arithmetic meaninlineform152 is continuous piecewise convex .furthermore , the arithmetic meanpreserves piecewise continuous differentiability .thus these results hold for the mean value of the network over the dataset .we conclude this section with a simple remark which will be useful in later sections .let inlineform153 be a neural network , considered as a function of the parameters of the inlineform154 layer , and let inlineform155 be a piece of inlineform156 .then the optimization problem displayform0  is convex .\n\nnetwork parameters of multiple layers: in the previous section we analyzed the convexity properties of neural networks when optimizing the parameters of a single layer , all others held constant .now we are ready to extend these results to the ultimate goal of simultaneously optimizing all network parameters .although not convex , the problem has a special convex substructure that we can exploit in proving future results .we begin by defining this substructure for point sets and functions .definition 5.1let inlineform157 , let inlineform158 , and let inlineform159 .the set displayform0is the cross - section of inlineform160 intersecting inlineform161 with respect to inlineform162 .in other words , inlineform163 is the subset of inlineform164 for which every point is equal to inlineform165 in the components not indexed by inlineform166 .note that this differs from the typical definition , which is the intersection of a set with a hyperplane .for example , inlineform167 is the inlineform168 -axis , whereas inlineform169 is the inlineform170-plane . note also that cross - sections are not unique , for exampleinlineform171 .in this case the first two components of the cross section are irrelevant , but we will maintain them for notational convenience .we can now apply this concept to functions on inlineform172 .definition 5.2let inlineform173 , let inlineform174 and let inlineform175 be a collection of sets covering inlineform176 .we say that inlineform177 is multi - convex with respect to inlineform178 if inlineform179 is convex when restricted to the cross section inlineform180 , for all inlineform181 and inlineform182 .this formalizes the notion of restricting a non - convex function to a variable subset on which it is convex , as in section secref4 when a neural network was restricted to the parameters of a single layer .for example , let inlineform183 , and let inlineform184 , and inlineform185 .then inlineform186 is a convex function of inlineform187 withinlineform188 fixed at inlineform189 .similarly , inlineform190 is a convex function of inlineform191 with inlineform192 fixed at inlineform193 .thus inlineform194 is multi - convex with respect to inlineform195 .to fully define a multi - convex optimization problem , we introduce a similar concept for point sets .definition 5.3let inlineform196 and let inlineform197 be a collection of sets covering inlineform198 .we say that inlineform199 is multi - convex with respect to inlineform200 if the cross - section inlineform201 is convex for all inlineform202 and inlineform203 .this generalizes the notion of biconvexity found in the optimization literature   .from here , we can extend definition secref1 to multi - convex functions .however , we will drop the topological restrictions on the pieces of our function , since multi - convex sets need not be connected .definition 5.4let inlineform204 be a continuous function .we say that inlineform205 is continuous piecewise multi - convex if each there exists a collection of multi - convex functions inlineform206 and multi - convex setsinlineform207 covering inlineform208such that for each inlineform209 we have inlineform210 for all inlineform211 .next , let inlineform212 .then , inlineform213 is continuous piecewise multi - convex so long as each component is , as in definition secref2 .from this definition , it is easily verified that a continuous piecewise multi - convex function inlineform214 admits a representation where all pieces are multi - convex , as in the proof of theorem secref3 .before we can extend the results of section secref4 to multiple layers , we must add one final constraint on the definition of a neural network .that is , each of the layers must be continuous piecewise multi - convex , considered as functions of both the parameters and the input .again , this is easily verified for the all of the layer types previously mentioned .we have already shown they are piecewise convex on each cross - section , taking our index sets to separate the parameters from the input data .it only remains to show that the number of pieces is finite .the only layer which merits consideration is the relu , which we can see from equation eqref21 consists of two pieces for each component : the \u201c dead \u201d or constant region , with inlineform215 , and its compliment .with inlineform216 components we have at most inlineform217 pieces , corresponding to binary assignments of \u201c dead \u201d or \u201c alive \u201d for each component .having said that each layer is continuous piecewise multi - convex , we can extend these results to the whole network .theorem 5.5let inlineform218 be a neural network , and let inlineform219 be a collection of index sets , one for the parameters of each layer of inlineform220 .then inlineform221 is continuous piecewise multi - convex with respect to inlineform222 .we begin the proof with a lemma for more general multi - convex functions .lemma 5.6let inlineform223 , inlineform224 , and letinlineform225 andinlineform226 be continuous piecewise multi - convex , inlineform227 with respect to a collection of index sets inlineform228 , and inlineform229 with respect to inlineform230 , where inlineform231 indexes the variables in inlineform232 , and inlineform233 the variables in inlineform234 .then inlineform235 is continuous piecewise multi - convex with respect to inlineform236 .let inlineform237 be a piece of inlineform238 , let inlineform239 be a piece of inlineform240 and let inlineform241 , with inlineform242 chosen so thatinlineform243 .clearly inlineform244 is multi - convex on inlineform245 with respect to inlineform246 .it remains to show that inlineform247 is a multi - convex set .now , let inlineform248and we shall show that the cross - sections are convex .first , for any inlineform249 we have inlineform250 .similarly , we have inlineform251 .these sets are convex , as they are the cartesian products of convex sets   .finally , as in the proof of theorem secref5 , we can cover inlineform252 with the finite collection of all such pieces inlineform253 , taken over all inlineform254 and inlineform255 .our next lemma extends theorem secref10 to multi - convex functions .lemma 5.7let inlineform256 be a collection of sets covering inlineform257 , and let inlineform258 and inlineform259 be continuous piecewise multi - convex with respect to inlineform260 .then so is inlineform261 .let inlineform262 be a piece of inlineform263and inlineform264 be a piece of inlineform265 with inlineform266 .then for all inlineform267 , inlineform268 , a convex set on which inlineform269 is convex .thus inlineform270 is continuous piecewise multi - convex , where the pieces of inlineform271 are the intersections of pieces of inlineform272 and inlineform273 .we can now prove the theorem .for the moment , assume we have only a single data point .now , let inlineform274 and inlineform275 denote layers of inlineform276 , with parametersinlineform277 .since inlineform278 and inlineform279 are continuous piecewise multi - convex functions of their parameters and input , we can write the two - layer sub - network as inlineform280 .by repeatedly applying lemma secref33 , the whole network is multi - convex on a finite number of sets covering the input and parameter space .now we extend the theorem to the whole dataset , where each data point defines a continuous piecewise multi - convex function inlineform281 .by lemma secref34 , the arithmetic meaninlineform282 is continuous piecewise multi - convex .in the coming sections , we shall see that multi - convexity allows us to give certain guarantees about the convergence of various optimization algorithms .but first , we shall prove some basic results independent of the optimization procedure .these results were summarized by gorksi et al .for the case of biconvex differentiable functions   .here we extend them to piecewise functions and arbitrary index sets .first we define a special type of minimum relevant for multi - convex functions .definition 5.8let inlineform283 and let inlineform284 be a collection of sets covering inlineform285 .we say that inlineform286 is a partial minimum of inlineform287 with respect to inlineform288if inlineform289 for all inlineform290 .in other words , inlineform291 is a partial minimum of inlineform292 with respect to inlineform293 if it minimizesinlineform294 on every cross - section of inlineform295 intersecting inlineform296 , as shown in figure figref36 .by convexity , these points are intimately related to the stationary points of inlineform297 .theorem 5.9let inlineform298 be a collection of sets covering inlineform299 , let inlineform300 be continuous piecewise multi - convex with respect to inlineform301 , and let inlineform302 .then inlineform303 is a partial minimum of inlineform304 on every piece containing inlineform305 .let inlineform306 be a piece of inlineform307 containing inlineform308 , let inlineform309 , and let inlineform310 denote the relevant cross - section of inlineform311 .we know inlineform312 is convex on inlineform313 , and since inlineform314 , we have that inlineform315 minimizesinlineform316 on this convex set .since this holds for all inlineform317 , inlineform318 is a partial minimum of inlineform319 on inlineform320 .it is clear that multi - convexity provides a wealth of results concerning partial minima , while piecewise multi - convexity restricts those results to a subset of the domain .less obvious is that partial minima of smooth multi - convex functions need not be local minima .an example was pointed out by a reviewer of this work , that the biconvex functioninlineform321 has a partial minimum at the origin which is not a local minimum .however , the converse is easily verified , even in the absence of differentiability .theorem 5.10let inlineform322 be a collection of sets covering inlineform323 , let inlineform324 be continuous piecewise multi - convex with respect to inlineform325 , and let inlineform326 be a local minimum on some piece inlineform327 of inlineform328 .then inlineform329 is a partial minimum on inlineform330 .the proof is essentially the same as that of theorem secref37 .we have seen that for multi - convex functions there is a close relationship between stationary points , local minima and partial minima .for these functions , infinitesimal results concerning derivatives and local minima can be extended to larger sets .however , we make no guarantees about global minima .the good news is that , unlike global minima , we shall see that we can easily solve for partial minima .\n\ngradient descent: in the realm of non - convex optimization , also called global optimization , methods can be divided into two groups : those which can certifiably find a global minimum , and those which can not .in the former group we sacrifice speed , in the latter correctness .this work focuses on algorithms of the latter kind , called local or sub - optimal methods , as only this type is used in practice for deep neural networks .in particular , the most common methods are variants of gradient descent , where the gradient of the network with respect its parameters is computed by a procedure called backpropagation .since its explanation is often obscured by jargon , we shall provide a simple summary here .backpropagation is nothing but the chain rule applied to the layers of a network .splitting the network into two functions inlineform331 ,where inlineform332 , and inlineform333 , we have displayform0where inlineform334 denotes the jacobian operator .note that here the parameters of inlineform335 are considered fixed , whereas the parameters of inlineform336 are variable and the input data is fixed .thus inlineform337 is the gradient of inlineform338 with respect to the parameters of inlineform339 , if it exists .the special observation is that we can proceed from the top layer of the neural network inlineform340 to the bottom inlineform341 , with inlineform342 , and inlineform343 , each time computing the gradient of inlineform344 with respect to the parameters of inlineform345 .in this way , we need only store the vectorinlineform346and the matrix inlineform347 can be forgotten at each step .this is known as the \u201c backward pass , \u201d which allows for efficient computation of the gradient of a neural network with respect to its parameters .a similar algorithm computes the value of inlineform348 as a function of the input data , which is often needed to evaluate inlineform349 .first we compute and store inlineform350 as a function of the input data , theninlineform351 , and so on until we have inlineform352 .this is known as the \u201c forward pass . \u201dafter one forward and one backward pass , we have computed inlineform353 with respect to all the network parameters .having computed inlineform354 , we can update the parameters by gradient descent , defined as follows .definition 6.1let inlineform355 , and inlineform356 be partial differentiable , with inlineform357 .then gradient descent on inlineform358 is the sequence inlineform359 defined by displayform0where inlineform360 is called the step size or \u201c learning rate . \u201din this work we shall make the additional assumption that inlineform361 .variants of this basic procedure are preferred in practice because their computational cost scales well with the number of network parameters .there are many different ways to choose the step size , but our assumption that inlineform362 covers what is usually done with deep neural networks .note that we have not defined what happens if inlineform363 .since we are ultimately interested in neural networks on inlineform364 , we can ignore this case and say that the sequence diverges .gradient descent is not guaranteed to converge to a global minimum for all differentiable functions .however , it is natural to ask to which points it can converge .this brings us to a basic but important result .theorem 6.2let inlineform365 , and let inlineform366 result from gradient descent on inlineform367 with inlineform368 , and inlineform369 continuously differentiable at inlineform370 .then inlineform371 .first , we have displayform0assume for the sake of contradiction that for the inlineform372 partial derivative we have inlineform373 .now , pick some inlineform374such that inlineform375 , and by continuous differentiability , there is someinlineform376such that for all inlineform377 , inlineform378 implies inlineform379 .now , there must be some inlineform380such that for all inlineform381 we have inlineform382 , so that inlineform383 does not change sign .then we can write inlineform384  but this contradicts the fact that inlineform385 converges .thus inlineform386 .in the convex optimization literature , this simple result is sometimes stated in connection with zangwill 's much more general convergence theorem   .note , however , that unlike zangwill we state necessary , rather than sufficient conditions for convergence .while many similar results are known , it is difficult to strictly weaken the conditions of theorem secref42 .for example , if we relax the condition that inlineform387 is not summable , and take inlineform388 , then inlineform389 will always converge to a non - stationary point .similarly , if we relax the constraint that inlineform390 is continuously differentiable , taking inlineform391 and inlineform392 decreasing monotonically to zero , we will always converge to the origin , which is not differentiable .furthermore , if we have inlineform393 with inlineform394 constant , then inlineform395 will not converge for almost all inlineform396 .it is possible to prove much stronger necessary and sufficient conditions for gradient descent , but these results require additional assumptions about the step size policy as well as the function to be minimized , and possibly even the initializationinlineform397   .it is worth discussing inlineform398 in greater detail , since this is a piecewise affine function and thus of interest in our investigation of neural networks .while we have said its only convergence point is not differentiable , it remains subdifferentiable , and convergence results are known for subgradient descent   .in this work we shall not make use of subgradients , instead considering descent on a piecewise continuously differentiable function , where the pieces are inlineform399 and inlineform400 .although theorem secref42 does not apply to this function , the relevant results hold anyways .that is , inlineform401 is minimal on some piece of inlineform402 , a result which extends to any continuous piecewise convex function , as any saddle point is guaranteed to minimize some piece .here we should note one way in which this analysis fails in practice .so far we have assumed the gradientinlineform403 is precisely known .in practice , it is often prohibitively expensive to compute the average gradient over large datasets .instead we take random subsamples , in a procedure known as stochastic gradient descent .we will not analyze its properties here , as current results on the topic impose additional restrictions on the objective function and step size , or require different definitions of convergence   .restricting ourselves to the true gradientinlineform404 allows us to provide simple proofs applying to an extensive class of neural networks .we are now ready to generalize these results to neural networks .there is a slight ambiguity in that the boundary points between pieces need not be differentiable , nor even sub - differentiable .since we are interested only in necessary conditions , we will say that gradient descent diverges when inlineform405 does not exist .however , our next theorem can at least handle non - differentiable limit points .theorem 6.3let inlineform406 be a collection of sets covering inlineform407 , let inlineform408 be continuous piecewise multi - convex with respect to inlineform409 , and piecewise continuously differentiable .then , let inlineform410 result from gradient descent on inlineform411 , with inlineform412 , such that eitherthen inlineform413 is a partial minimum of inlineform414 on every piece containing inlineform415 .if the first condition holds , the result follows directly from theorems secref42 and secref37 .if the second condition holds , then inlineform416 is a convergent gradient descent sequence on inlineform417 , the active function of inlineform418 on inlineform419 .since inlineform420 is continuously differentiable on inlineform421 , the first condition holds for inlineform422 .since inlineform423 , inlineform424 is a partial minimum of inlineform425 as well .the first condition of theorem secref44 holds for every point in the interior of a piece , and some boundary points .the second condition extends these results to non - differentiable boundary points so long as gradient descent is eventually confined to a single piece of the function .for example , consider the continuous piecewise convex function inlineform426 as shown in figure figref47 .when we converge to inlineform427 from the pieceinlineform428 , it is as if we were converging on the smooth functioninlineform429 .this example also illustrates an important caveat regarding boundary points : although inlineform430 is an extremum of inlineform431 on inlineform432 , it is not an extremum on inlineform433 .\n\niterated convex optimization: although the previous section contained some powerful results , theorem secref44 suffers from two main weaknesses , that it is a necessary condition and that it requires extra care at non - differentiable points .it is difficult to overcome these limitations with gradient descent .instead , we shall define a different optimization technique , from which necessary and sufficient convergence results follow , regardless of differentiability .iterated convex optimization splits a non - convex optimization problem into a number of convex sub - problems , solving the sub - problems in each iteration .for a neural network , we have shown that the problem of optimizing the parameters of a single layer , all others held constant , is piecewise convex .thus , restricting ourselves to a given piece yields a convex optimization problem .in this section , we show that these convex sub - problems can be solved repeatedly , converging to a piecewise partial optimum .definition 7.1let inlineform434 be a collection of sets covering inlineform435 , and let inlineform436 and inlineform437 be multi - convex with respect to inlineform438 .then iterated convex optimization is any sequence where inlineform439 is a solution to the optimization problem displayform0  with inlineform440 .we call this iterated convex optimization because problem eqref49 can be divided into convex sub - problems displayform0  for each inlineform441 .in this work , we assume the convex sub - problems are solvable , without delving into specific solution techniques .methods for alternating between solvable sub - problems have been studied by many authors , for many different types of sub - problems   .in the context of machine learning , the same results have been developed for the special case of linear autoencoders   .still , extra care must be taken in extending these results to arbitrary index sets .the key is that inlineform442 is not updated until all sub - problems have been solved , so that each iteration consists of solving inlineform443 convex sub - problems .this is equivalent to the usual alternating convex optimization for biconvex functions , where inlineform444 consists of two sets , but not for general multi - convex functions .some basic convergence results follow immediately from the solvability of problem eqref49 .first , note that inlineform445 is a feasible point , so we have inlineform446 .this implies that inlineform447 exists , so long as inlineform448 is bounded below .however , this does not imply the existence of inlineform449 .see gorskiet al .for an example of a biconvex function on which inlineform450 diverges   .to prove stronger convergence results , we introduce regularization to the objective .theorem 7.2let inlineform451 be a collection of sets covering inlineform452 , and let inlineform453 and inlineform454 be multi - convex with respect to inlineform455 .next , let inlineform456 , and let inlineform457 , where inlineform458and inlineform459 is a convex norm .finally , let inlineform460 result from iterated convex optimization of inlineform461 .then inlineform462 has at least one convergent subsequence , in the topology induced by the metric inlineform463 .from lemma secref11 , inlineform464 is multi - convex , so we are allowed iterated convex optimization .now , if inlineform465 we have that inlineform466 .thus inlineform467whenever inlineform468 .since inlineform469 is a non - increasing sequence , we have that inlineform470 .equivalently , inlineform471 lies in the set inlineform472 .since inlineform473 is continuous , inlineform474 is closed and bounded , and thus it is compact .then , by the bolzano - weierstrauss theorem , inlineform475 has at least one convergent subsequence   .in theorem secref51 , the function inlineform476 is called the regularized version of inlineform477 .in practice , regularization often makes a non - convex optimization problem easier to solve , and can reduce over - fitting .the theorem shows that iterated convex optimization on a regularized function always has at least one convergent subsequence .next , we shall establish some rather strong properties of the limits of these subsequences .theorem 7.3let inlineform478 be a collection of sets covering inlineform479 , and let inlineform480 and inlineform481 be multi - convex with respect to inlineform482 .next , let inlineform483 result from iterated convex optimization of inlineform484 .then the limit of every convergent subsequence is a partial minimum on inlineform485 with respect to inlineform486 , in the topology induced by the metric inlineform487 for some norm inlineform488 .furthermore , if inlineform489 and inlineform490 are convergent subsequences , then inlineform491 .let inlineform492 denote a subsequence of inlineform493 with inlineform494 .now , assume for the sake of contradiction that inlineform495 is not a partial minimum on inlineform496 with respect to inlineform497 .then there is some inlineform498 and some inlineform499 with inlineform500such thatinlineform501 .now , inlineform502 is continuous at inlineform503 , so there must be some inlineform504such that for all inlineform505 , inlineform506 implies inlineform507 .furthermore , since inlineform508 is an interior point , there must be some open ball inlineform509 of radiusinlineform510 centered at inlineform511 , as shown in figure figref53 .now , there must besome inlineform512 suchthat inlineform513 .then , let inlineform514 , and since inlineform515 , we know that inlineform516 , and thus inlineform517 .finally , inlineform518 , so we have inlineform519 , which contradicts the fact that inlineform520 minimizes inlineform521 over a set containing inlineform522 .thus inlineform523 is a partial minimum on inlineform524 with respect to inlineform525 .finally , let inlineform526 and inlineform527 be two convergent subsequences of inlineform528 , with inlineform529 and inlineform530 , and assume for the sake of contradiction thatinlineform531 .then by continuity , there is some inlineform532 such thatinlineform533 .but this contradicts the fact that inlineform534 is non - increasing .thus inlineform535 .the previous theorem is an extension of results reviewed in gorski et al .to arbitrary index sets   .while gorskietal .explicitly constrain the domain to a compact biconvex set , we show that regularization guaranteesinlineform536 can not escape a certain compact set , establishing the necessary condition for convergence .furthermore , our results hold for general multi - convex sets , while the earlier result is restricted to cartesian products of compact sets .these results for iterated convex optimization are considerably stronger than what we have shown for gradient descent .while any bounded sequence in inlineform537 has a convergent subsequence , and we can guarantee boundedness for some variants of gradient descent , we can not normally say much about the limits of subsequences .for iterated convex optimization , we have shown that the limit of any subsequence is a partial minimum , and all limits of subsequences are equal in objective value .for all practical purposes , this is just as good as saying that the original sequence converges to partial minimum .\n\nglobal optimization: although we have provided necessary and sufficient conditions for convergence of various optimization algorithms on neural networks , the points of convergence need only minimize cross - sections of pieces of the domain .of course we would prefer results relating the points of convergence to global minima of the training objective .in this section we illustrate the difficulty of establishing such results , even for the simplest of neural networks .in recent years much work has been devoted to providing theoretical explanations for the empirical success of deep neural networks , a full accounting of which is beyond the scope of this article .in order to simplify the problem , many authors have studied linear neural networks , in which the layers have the forminlineform538 , where inlineform539 is the parameter matrix .with multiple layers this is clearly a linear function of the output , but not of the parameters .as a special case of piecewise affine functions , our previous results suffice to show that these networks are multi - convex as functions of their parameters .this was proven for the special case of linear autoencoders by baldi and lu   .many authors have claimed that linear neural networks contain no \u201c bad \u201d local minima , i.e. every local minimum is a global minimum   .this is especially evident in the study of linear autoencoders , which were shown to admit many points of inflection , but only a single strict minimum   .while powerful , this claim does not apply to the networks seen in practice .to see this , consider the dataset inlineform540 consisting of three inlineform541 pairs , parameterized by inlineform542 .note that the dataset haszero mean and unit variance in the inlineform543 variable , which is common practice in machine learning .however , we do not take zero mean in the inlineform544 variable , as the model we shall adopt is non - negative .next , consider the simple neural network displayform0  this is the squared error of a single relu neuron , parameterized byinlineform545 .we have chosen this simplest of all networks because we can solve for the local minima in closed form , and show they are indeed very bad .first , note that inlineform546 is a continuous piecewise convex function of six pieces , realized by dividing the plane along the line inlineform547 for each inlineform548 , as shown in figure figref56 .now , for all but one of the pieces , the relu is \u201c dead \u201d for at least one of the three data points , i.e. inlineform549 .on these pieces , at least one of the three terms of equation eqref54 is constant .the remaining terms are minimized when inlineform550 , represented by the three dashed lines in figure figref56 .there are exactly three points where two of these lines intersect , and we can easily show that two of them are strict local minima .specifically , the point inlineform551 minimizes the first two terms of equation eqref54 , while inlineform552 minimizes the first and last term .in each case , the remaining term is constant over the piece containing the point of intersection .thus these points are strict global minima on their respective pieces , and strict local minima on inlineform553 .furthermore , we can compute inlineform554 and inlineform555 .this gives inlineform556  and displayform0now , it might be objected that we are not permitted to take inlineform557 if we require that the inlineform558 variable has unit variance .however , these same limits can be achieved with variance tending to unity by addinginlineform559 instances of the pointinlineform560 to our dataset .thus even under fairly stringent requirements we can construct a dataset yielding arbitrarily bad local minima , both in the parameter space and the objective value .this provides some weak justification for the empirical observation that success in deep learning depends greatly on the data at hand .we have shown that the results concerning local minima in linear networks do not extend to the nonlinear case .ultimately this should not be a surprise , as with linear networks the problem can be relaxed to linear regression on a convex objective .that is , the composition of all linear layers inlineform561 is equivalent to the functioninlineform562 for some matrix inlineform563 , and under our previous assumptions the problem of finding the optimal inlineform564 is convex .furthermore , it is easily shown that the number of parameters in the relaxed problem is polynomial in the number of original parameters .since the relaxed problem fits the data at least as well as the original , it is not surprising that the original problem is computationally tractable .this simple example was merely meant to illustrate the difficulty of establishing results for every local minimum of every neural network .since training a certain kind of network is known to be np - complete , it is difficult to give any guarantees about worst - case global behavior   .we have made no claims , however , about probabilistic behavior on the average practical dataset , nor have we ruled out the effects of more specialized networks , such as very deep ones .\n\nconclusion: we showed that a common class of neural networks is piecewise convex in each layer , with all other parameters fixed .we extended this to a theory of a piecewise multi - convex functions , showing that the training objective function can be represented by a finite number of multi - convex functions , each active on a multi - convex parameter set .from here we derived various results concerning the extrema and stationary points of piecewise multi - convex functions .we established convergence conditions for both gradient descent and iterated convex optimization on this class of functions , showing they converge to piecewise partial minima .similar results are likely to hold for a variety of other optimization algorithms , especially those guaranteed to converge at stationary points or local minima .we have witnessed the utility of multi - convexity in proving convergence results for various optimization algorithms .however , this property may be of practical use as well .better understanding of the training objective could lead to the development of faster or more reliable optimization methods , heuristic or otherwise .these results may provide some insight into the practical success of sub - optimal algorithms on neural networks .however , we have also seen that local optimality results do not extend to global optimality as they do for linear autoencoders .clearly there is much left to discover about how , or even if we can optimize deep , nonlinear neural networks .\n\nacknowledgments: the author would like to thank mihir mongia for his helpful comments in preparing this manuscript .\n\nfunding: this research did not receive any specific grant from funding agencies in the public , commercial , or not - for - profit sectors .",
      "target_text": "although artificial neural networks have shown great promise in applications including computer vision and speech recognition , there remains considerable practical and theoretical difficulty in optimizing their parameters .the seemingly unreasonable success of gradient descent methods in minimizing these non - convex functions remains poorly understood .in this work we offer some theoretical guarantees for networks with piecewise affine activation functions , which have in recent years become the norm .we prove three main results .firstly , that the network is piecewise convex as a function of the input data .secondly , that the network , considered as a function of the parameters in a single layer , all others held constant , is again piecewise convex .finally , that the network as a function of all its parameters is piecewise multi - convex , a generalization of biconvexity .from here we characterize the local minima and stationary points of the training objective , showing that they minimize certain subsets of the parameter space .we then analyze the performance of two optimization algorithms on multi - convex problems : gradient descent , and a method which repeatedly solves a number of convex sub - problems .we prove necessary convergence conditions for the first algorithm and both necessary and sufficient conditions for the second , after introducing regularization to the objective .finally , we remark on the remaining difficulty of the global optimization problem .under the squared error objective , we show that by varying the training data , a single rectifier neuron admits local minima arbitrarily far apart , both in objective value and parameter space .",
      "output_text": "thea: and to of"
    },
    {
      "input_text": "Title: Robust Neural Machine Translation with Joint Textual and Phonetic Embedding\nDomains: Computer science\n\nintroduction: recently we witnessed tremendous progresses in the field of neural machine translation ( nmt ) especially the birth of transformer network   .despite tremendous success , nmt models are very sensitive to the noises in input sentences   .the causes of such vulnerability are multifold , and some of them are : 1 )neural networks are inherently sensitive to noises , such as adversarial examples 2 )every input word can affect every output word generated by the decoder due to the global effects of attention , and 3 ) all nmt models have an input embedding layer , which is sensitive to noises in the input sentences .in this paper , we focus on homophone noise , where words are replaced by other words with similar pronunciations , which is common in real - world systems .one example is speech translation where an asr system may output correct or almost correct phoneme sequences , but transcribe some words into their homophones .another example is pronunciation - based input systems for non - phonetic writing systems such as pinyin for chinese or katakana / hiragana for japanese .it is very common for a user to accidentally choose a homophone instead of the correct word .existing nmt systems are very sensitive to homophone noises , and table tabref2 illustrates such an example .the transformer model can correctly translate the clean input sentence ; however , when one mandarin character , ` \u6709'(y\u01d2u ) , is replaced by one of its homophones , ` \u53c8'(y\u00f2u ) , the transformer generates a strange and irrelevant translation .the method proposed in this paper can generate correct results under such kind of noises , since it uses both textual and phonetic information .since words are discrete signals , to feed them into a neural network , a common practice is to encode them into real - valued vectors through embedding .however , the output of the embedding layer is very sensitive to noises in the input sentences .this is because when a word inlineform0 is replaced by another word inlineform1 with different meanings , the embedding vector of inlineform2 may be very different from the embedding vector of inlineform3 , thus results in dramatic changes .to make things worse , the input embedding layer is usually the first layer of the network , and errors from this layer will propagate and be amplified in the following layers , leading to more severe errors .for homophone noises , since correct phonetic information exists , we can make use of it to make the output of the embedding layer much more robust .in this paper , we propose to improve the robustness of nmt models to homophone noises by jointly embedding both textual and phonetic information .in our approach , both words and their corresponding pronunciations are embedded and then combined to feed into a neural network .this approach has the following advantages :to further improve the robustness of nmt models to homophone noises , we use data augmentation to expand the training datasets , by randomly adding homophone noises .the experimental results clearly show that data augmentation improves the robustness of nmt models .\n\njoint embedding: for a word inlineform4 in the source language , suppose its pronunciation can be expressed by a sequence of pronunciation units , such as phonemes or syllables , denoted by inlineform5 .note that we use the term \u201c word \" loosely here , and in fact inlineform6 may be a word or a subword , or even a character .we embed both pronunciation units and words , and both of them are learnt from scratch .for a pronunciation unit inlineform7 , its embedding is denoted by inlineform8 , and for a word inlineform9 , its embedding is denoted by inlineform10 .for a pair of a word inlineform11 and its pronunciation sequence inlineform12 , we have inlineform13 embedding vectors , that is , inlineform14 .to get a fixed length vector representation , we first merge inlineform15 into a single vector by averaging , denoted by inlineform16 , then combine the word embedding and inlineform17 as follows : displayform0where inlineform18 is a parameter .when inlineform19 , only textual embedding is used ; while when inlineform20 , only phonetical embedding is used .the best balance , as demonstrated by our experiments , is a very large inlineform21 close to but not 1 .\n\nmodels: in our experiments , we use transformer as baseline .specifically , we use the pytorch version ( pytorch 0.4.0 ) of opennmt .all models are trained with 8 gpus , and the values of important parameters are : 6 layers , 8 heads attention , 2048 neurons in feed - forward layer , and 512 neurons in other layers , dropout rate is inlineform22 , label smoothing rate is inlineform23 , adam optimizer , learning rate is 2 with noam decay .\n\ntranslation tasks: we evaluate our method on the translation task of mandarin to english , and reported the 4-gram bleu score   as calculated by multi-bleu.perl .pinyin is used as pronunciation units and there are 404 types of pinyin syllables in total .a large mandarin lexicon is used .for words or subwords not in the lexicon , if all of their characters have pinyins , the concatenation of these characters 's pinyins are used as the pinyin of the whole words or subwords .note that when there are multiple pronunciations , we just randomly pick one in both training and testing .for symbols or entries without pronunciation , we use a special pronunciation unit , inlineform24 , to represent them .\n\ntranslation results: for the dataset , we use an extended nist corpus which consists of 2 m sentence pairs with about 51 m mandarin words and 62 m english words , respectively .we apply byte - pair encodings ( bpe )   on both mandarin and english sides to reduce the vocabulary size down to 18k and 10k , respectively .sentences longer than 256 subwords or words are excluded .in figure figref13 , we compare the performances , measured by bleu scores to multiple references , of the baseline model and our models with inlineform25 , respectively .we report the results every 10000 iterations from iteration 10000 to iteration 90000 .note that our model is almost exactly the same as baseline model , with only different source embeddings .in theory , when inlineform26 , our model is identical to baseline model .however , in practice , there is a slight difference : when inlineform27 , the embedding parameters are still there , which will affect the optimization procedure even no gradients flow back to these parameters .when inlineform28 , only phonetic information is used .there are some interesting observations from figure figref13 .first , combing textual and phonetic information improves the performance of translation .compared with baseline , when inlineform29 , the bleu scores improves inlineform30 points , and when inlineform31 , the bleu scores improves inlineform32 points .second , the phonetic information plays a very important role in translation .even when inlineform33 , that is , the weight of phonetic embedding is inlineform34and the weight of word embedding is only inlineform35 , the performance is still very good .in fact , our best bleu score ( inlineform36 ) , is achieved when inlineform37 .however , word embedding is still important .in fact , when we use only phonetic information ( when inlineform38 ) , the performance become worse , almost the same as baseline ( only using textual information ) .our human only needs phonetic information to communicate with each other , this is probably because we have better ability to understand context than machines , thus do not need the help of textual information .table tabref14 reports results on the baseline model and our models under different inlineform39s. nist 06 is used as dev set to select the best models , and nist 2002 , 2003 , 2004 , 2005 and 2008 datasets are used as test sets .there are some interesting observations .first , combing textual and phonetic information improves the performance of translation .this seems to be surprising since no additional information is provided .although the real reason is unknown , we suspect that it is because some kind of regularization effects from phonetic embeddings .second , the phonetic information plays a very important role in translation .even when inlineform40 , that is , most weights are put on phonetic embedding , the performance is still very good .in fact , our best bleu score ( inlineform41 ) , is achieved when inlineform42 .however , word embedding is still important .in fact , when we use only phonetic information ( inlineform43 ) , the performance degrades , almost the same as baseline ( only using textual information ) .to understand why phonetic information helps the translation , it is helpful to visualize the embedding of pronunciation units .we projects the whole pinyin embedding space into a 2-dimensional space using t - sne technique   , and illustrate a small region of it in figure figref15 .an intriguing property of the embedding is that pinyins with similar pronunciations are close to each other , such as zhen and zheng , ji and qi , mu and hu .this is very helpful since in mandarin , two characters with similar pronunciations will either inlineform44 be represented by the same pinyin or inlineform45 be represented by two pinyins with similar pronunciations .homophones are very common in mandarin .in our training dataset , about inlineform46 mandarin words have homophones .to test the robustness of nmt models to homophone noises , we created two noisy test sets , namely , noisyset1 , and noisyset2 , based on nist06 mandarin - english test set .the creation procedure is as follows : for each source sentence in nist06 , we scan it from left to right , and if a word has homophones , it will be replaced by one of its homophones by a certain probability ( inlineform47 for noisyset1 and inlineform48 for noisyset2 ) .in figure figref16 , we compare the performance of the baseline model and our models with inlineform49 , respectively , on nist06 test set and the two created noisy sets .the models are chosen based on their performance ( bleu scores ) on nist06 test set .as figure figref16 shows , as inlineform50 grows , which means that more weights are put on phonetic information , the performances on both noisy test sets almost steadily improve .when inlineform51 , as expected , homophone noises will not affect the results since the model is trained solely based on phonetic information .however , this is not our best choice since the performance on the clean test set gets much worse .in fact , from the perspective of robustness to homophone noises , the best choice of inlineform52 is still a value smaller but close to 1 , which mainly focuses on phonetic information but still utilizes some textual information .table tabref17 demonstrate the effects of homophone noises on two sentences .the baseline model can translate both sentences correctly ; however , when only one word ( preposition ) is replaced by one of its homophones , the baseline model generates incorrect , redundant and strange translations .this shows the vulnerability of the baseline model .note that since the replaced words are prepositions , the meaning of the noisy source sentences are still very clear , and it does not affect our human 's understanding at all .for our method , we use the model with inlineform53 , and it generates reasonable translations .to further improve the robustness of nmt models , we augment the training dataset by randomly picking training pairs from training datasets , and revising the source sentences by randomly replacing some words with their homophones .we add inlineform54 noisy sentence pairs on the original 2 m sentence pairs in the training set , resulting in a training dataset withabout inlineform55 m sentence pairs .in table tabref18 , we report the performance of baseline model and our model with inlineform56 , with and without data augmentation .not surprisingly , data augmentation significantly improves the robustness of nmt models to homophone noises .however , the noises in training data seem to hurt the performance of the baseline model ( from inlineform57 to inlineform58 ) , and its effect on our model seems to be much smaller , probably because our model mainly uses the phonetic information .\n\nrelated work:    proposed to use a character - level translator to deal with misspelled words in the input sentences , but in general their method can not deal with homophone noises effectively .  proposed to use adversarial stability training to improve the robustness of nmt systems , but their method does not specifically target homophone noises and do not use phonetic information .the effects of asr errors on machine translation have been extensively analyzed   .in a parallel work ,   also proposed to utilize both textual and phonetic information to improve the robustness of nmt systems , but their method is different with ours in how textual and phonetic information are combined .\n\nconclusion: in this paper , we propose to use both textual and phonetic information in nmt by combining them in the input embedding layer of neural networks .such combination not only makes nmt models much more robust to homophone noises , but also improves their performance on clean datasets .our experimental results clearly show that both textual and phonetical information are important , and the best choice is to rely mostly on phonetic information .we also augment the training dataset by adding homophone noises , and our experiments demonstrate that this is very useful in improving the robustness of nmt models to homophone noises .",
      "target_text": "neural machine translation ( nmt ) is notoriously sensitive to noises , but noises are almost inevitable in practice .one special kind of noise is the homophone noise , where words are replaced by other words with similar pronunciations .we propose to improve the robustness of nmt to homophone noises by 1 ) jointly embedding both textual and phonetic information of source sentences , and 2 ) augmenting the training dataset with homophone noises .interestingly , to achieve better translation quality and more robustness , we found that most ( though not all ) weights should be put on the phonetic rather than textual information .experiments show that our method not only significantly improves the robustness of nmt to homophone noises , but also surprisingly improves the translation quality on some clean test sets .",
      "output_text": "thea: and to of"
    },
    {
      "input_text": "Title: PT-symmetric deformations of Calogero models\nDomains: Mathematics\n\nintroduction: the study of pseudo - hermitian hamiltonian systems has attracted a considerable amount of attention in the last few years .for recent reviews and special issues devoted to this topic see   .one of the main reasons for the popularity of these types of hamiltonians is the fact that they possess real eigenvalue spectra , despite of being non - hermitian , and therefore constitute interesting candidates for a new sort of stable physical systems overlooked up to now .alternatively to using the concept of pseudo - hermiticity   or quasi - hermiticity   ,   one may equivalently explain the reality of the spectrum of some non - hermitian hamiltonians when one encounters unbroken inlineform0 -symmetry , which in the recent context was first pointed out in   .unbroken specifies that both the hamiltonian and the wavefunction remain invariant under a simultaneous parity transformation inlineform1 and time reversal inlineform2 .when acting on complex valued functionsthe anti - linear operator inlineform3 is understood to act as complex conjugation .these observations can be exploited in the construction of new interesting models with real eigenvalue spectra when taking previously studied hermitian examples as starting points .the above statements imply that one has two possibilities at hand .one could either employ pseudo - hermiticity , which involves the usually technically difficult task to construct a meaningful metric , e.g. or in contrast use inlineform4 -symmetry as a very transparent and simple principle , at least on the level of the hamiltonian itself .starting with a inlineform5-symmetrichamiltonian or less restrictive with a parity invariant potential systemone may extend such type of models by adding inlineform6 -symmetric terms to it or by deforming existing terms in a inlineform7 -symmetric manner .the latter construction principle has been applied to a huge number of models , notably the harmonic oscillator in   , which constitutes the starting point of the current activities in this field of research .in the context of calogero models  such type of extensions were first carried out in   ,   by adding inlineform8 -symmetric terms to the inlineform9 and inlineform10 -calogero models .shortly afterwards an alternative procedure was proposed in   , where the inlineform11 -calogero model was genuinely deformed in a inlineform12 -symmetric manner .the analysis in   was extended thereafter in   to calogero models related to all coxeter groups and also generalized to the larger class of calogero - moser - sutherland ( cms ) models   involving more general types of potentials rather than the rational one .other versions of deformations of cms - models have also been proposed for instance in   , albeit a concrete relation to inlineform13 -symmetry had not been established , even though it is easy to verify that the models constructed in   are also inlineform14-symmetric .the purpose of this paper is to provide the general mathematical framework for the deformation carried out in   and generalize the construction to all coxeter groups and more general potentials .thereafter we study some of the physical properties of the newly obtained models .our manuscript is organized as follows : in order to fix our conventions we recall in section 2 some of the basic features of cms - models and indicate the structure we expect to find for the deformed models .in section 3 we demonstrate how coxeter groups may be systematically deformed in a inlineform15 -symmetric manner .we illustrate the general setting with the two explicit examples of the inlineform16 and inlineform17 -coxeter group .we apply these construction in section 4 to cms - models , which are invariant under the extended coxeter group .we show that models for which this invariance is broken in a particular way also possess interesting properties .thereafter we specialize to the calogero models and construct their eigensystems for some specific deformations .the key finding is that some constraints on the parameter space of the model resulting from physical requirements may be relaxed in the deformed model .for some simple extended model we demonstrate that the energy spectrum is real and contains the one of the undeformed case as a subsystem .we state our conclusions in section 5 .\n\nextended symmetries for calogero-moser-sutherland models: let us briefly recall some features of the cms - models , which will be relevant for our analysis .the models describe inlineform18 particles moving on a line , whose coordinates inlineform19 and canonically conjugate momenta inlineform20 may be assembled into vectors inlineform21 .the hamiltonian for the cms - models related to all coxeter groups inlineform22 may be written generically as displayform0the dimensionality of the space in which the roots inlineform23 of the root system inlineform24 are realized is inlineform25 .the sum in the confining term of the potential only extends over the short roots inlineform26 .one may impose further restrictions on the coupling constants inlineform27 in order to guarantee integrability   and invariance of the hamiltonian under the action of inlineform28 .the latter demands that inlineform29 when the roots inlineform30 and inlineform31 have the same length ,i.e. if inlineform32 .when the potential inlineform33 is taken to be inlineform34the hamiltonian ( eqref1 ) constitutes the calogero model , whereas the generalized cms - models are obtained by choosing inlineform35 , inlineform36 or inlineform37 .a key feature of the model ( eqref1 ) for our purposes is that it admits the entire coxeter group inlineform38 as a symmetry, i.e. displayform0where inlineform39 can be any weyl reflection ( eqref5 ) .for the confining term to remain invariantwe need to use that short roots are mapped into short root by the entire coxeter group .this symmetry stipulates that these models are invariant with respect to various parity transformations inlineform40 across the hyperplanes through the origin orthogonal to the root inlineform41 or in other words across the boundaries of all weyl chambers .our aim is here to modify the models such that they remain invariant under the action of the newly defined inlineform42 -symmetrically extended coxeter group , which we denote by inlineform43 .we propose the new hamiltonians to be of the form displayform0where we have replaced the standard roots inlineform44 by deformed roots inlineform45 .formally inlineform46 and inlineform47 are very similar , with the crucial difference that the latter is in general complex and non - hermitian .nonetheless , the inlineform48 -symmetry can be utilised to establish the reality of the spectrum with a minor modification .as we have complexified here each weyl reflection across any hyperplane orthogonal to every root we have as many inlineform49 -operators , i.e. anti - linear operators , as hyperplanes .this means we can employ any of these operators in the standard argument .in turn this also means that we could in principle make our construction less constraining by demanding less symmetry .what is of course not known at this point is whether the wavefunctions of the deformed hamiltonian also respect the extended symmetry .however , as we shall demonstrate below with some concrete examples this will indeed be the case .in order to see that such type of models really exist and how these models can be constructed we need to assemble first some mathematical tools and establish the fact that one can indeed construct a meaningful set of deformed roots inlineform50 .\n\n\ud835\udcab\ud835\udcaf\\mathcal {pt}-symmetric deformations of coxeter groups: we recall , see e.g. that a coxeter group inlineform51 is generated by the weyl reflections inlineform52 associated with a set of simple roots inlineform53 which span the entire root space inlineform54 displayform0the roots may be represented in various different euclidean spaces with dimensionality not necessarily equal to inlineform55 .here our aim is to construct a complex extended root system inlineform56 containing the roots inlineform57 represented in inlineform58 , depending on some deformation parameterinlineform59 .we demand that each deformed root reduces one - to - one to a root in the root space inlineform60 displayform0such that the entire root space reduces as displayform0furthermore , we require that the extended root system inlineform61 remains invariant under the inlineform62 -symmetrically extended coxeter group inlineform63 .note that in principle we may choose any of the hyperplanes through the origin orthogonal to a root inlineform64across which the parity symmetry inlineform65 can be extended to a inlineform66 -symmetry .thus we could expect inlineform67 deformed roots , with inlineform68 denoting the coxeter number and inlineform69 being the total number of roots .however , the deformations to any of the hyperplanes can in fact be made equivalent and the replacement displayform0becomes indeed one - to - one as we shall see below .from these requirements we may now attempt to construct the root system inlineform70 .we start by selecting a particular root inlineform71 , which does not have to be simple , and perform a complex inlineform72 -symmetric extension across the hyperplane through the origin orthogonal to this root .this deformation leads to a new , so far unspecified root inlineform73 .studying now the properties of this root will enable us to determine it .decomposing the complex extended weyl reflection into a product of standard weyl reflections ( eqref5 ) and a complex conjugation ( time - reversal ) as displayform0we compute its action on a root displayform0in view of ( eqref6 )we demanded here that the complex extended weyl reflection inlineform74 maps the deformed root inlineform75 into its negative which should in view of the limit ( eqref6 ) also hold for the real part independently .for the remaining term of the root the minus sign is created by the complex conjugation inlineform76 , such the imaginary part has to be invariant under the weyl reflection , i.e. it has to be a vector lying in the hyperplane across which the reflection is carried out .comparing now ( eqref10 )and ( ) we find as solution for inlineform77 displayform0where inlineform78 and the inlineform79 have to be elements of the weight lattice , i.e. they are orthogonal to the simple roots inlineform80 inlineform81 .the real valued functions inlineform82 and inlineform83 are arbitrary at this stage , with the only condition to satisfy displayform0in order to fulfill the requirement ( eqref6 ) .note that inlineform84 and inlineform85may also be multiplied by any invariant of the extended weyl group inlineform86 .the remaining roots can be constructed by acting with all possible non - equivalent inlineform87 reflections inlineform88 on these roots and hence producing the anticipated number of inlineform89 roots inlineform90 inlineform91 .supposing now we have constructed a new root as inlineform92 , it is then clear that by construction also for that new root the imaginary part is orthogonal to its real part displayform0the property which is , however , not guaranteed is that the decomposition of the undeformed root into a sum over simple roots inlineform93 is preserved by the deformation .nonetheless , we shall verify this feature for the explicit examples below .sometimes we can even find displayform0for which there is also no general justification .when ( eqref14 ) holds we can even impose a stronger constraint and require that inner products of roots and deformed roots are identical displayform0which allows us to fix the functions inlineform94 and inlineform95 .alternatively to the above construction we may also deform each root as displayform0note that in this deformation positive and negative roots in inlineform96 and inlineform97 are no longer related by an overall minus sign as in inlineform98 and inlineform99 , where inlineform100 always has a counter partinlineform101 .however , by construction we still have the property displayform0which is needed to achieve invariance under the extended coxeter groupinlineform102 .now , unlike as in the previous construction , the minus sign for the imaginary part is created by definition and not by the action of inlineform103 .in general , we may encounter the four possibilities displayform0thus any root inlineform104 of the form ( eqref16 ) is guaranteed to be mapped into inlineform105 under the action of inlineform106 , which means the deformed root system remains only invariant up to an overall sign .however , in our application below overall signs are irrelevant so that the deformation ( eqref16 ) will be suitable for the application in mind .let us now verify that the procedure outlined above indeed leads to a closed inlineform107 -symmetrically extended weyl group inlineform108 for some concrete examples .\n\n\ud835\udcab\ud835\udcaf\\mathcal {pt}-symmetric deformations of the a 2 a_{2}-coxeter group: we recall first the action of the weyl reflections on the simple roots by computing ( eqref5 ) with the cartan matrix inlineform109 , whose entries are inlineform110 , inlineform111 .the combinations of weyl reflections achieving a reflection across the hyperplanes through the origin orthogonal to the three positive roots inlineform112 , inlineform113 and inlineform114 of inlineform115 are displayform0as a starting point for the deformation we choose the simple root inlineform116 and extend the parity symmetry across the hyperplane through the origin orthogonal to this root .according to ( eqref11 ) the deformed root should be taken to displayform0where we introduced the fundamental weightinlineform117 .next we compute the action of the complex reflections inlineform118 on this root in order to construct the remaining five deformed roots .by construction we have displayform0having determined inlineform119 we may calculate the deformations of inlineform120 from inlineform121 guided by the undeformed case ( eqref20 ) .we compute displayform0where we obtained the fundamental weight inlineform122 .we may verify that the remaining reflections in ( eqref20 ) indeed yield a consistent system displayform0we may verify that in ( eqref24 ) and ( ) the imaginary part of the deformed rootinlineform123 is indeed orthogonal to the rootinlineform124 as it should be by construction .alternatively we could have started with the expressions ( eqref21 ) and ( eqref23 ) involving the ambiguities of the relative signs in front of the imaginary parts and the unknown functions inlineform125 and inlineform126 .the subsequent action of combinations of inlineform127 and inlineform128 would fix the sign ambiguity and produce the same set of deformed roots .note that we also have the property displayform0if we impose the additional constraint that inner products of root and deformed roots are identical displayform0we may fix the deformation functions to inlineform129 , inlineform130 .the factor of inlineform131 in the function inlineform132 is somewhat natural as it ensures that the roots in the real part of the deformed roots inlineform133 and the weights in the imaginary part have the same length .as intended , we have achieved a simple one - to - one relation between ( eqref20 ) and the corresponding identities for the deformed system simply by replacing inlineform134 and inlineform135 .we depict the roots and the hyperplanes in figure 1 .figure 1 : real and imaginary parts of the inlineform136 deformed roots divided by inlineform137 and inlineform138 , respectively , in the three dimensional standard representation for the simple roots inlineform139 ,inlineform140 .both parts of a particular positive rootinlineform141 are depicted in the same colour ( on - line ) .alternatively we can deform the six roots according to the principle ( eqref17 ) as displayform0as pointed out we no longer have inlineform142 .nonetheless , it is easy to verify that these roots are mapped into each other by inlineform143 as displayform0for these roots the inner product is not preserved and ( eqref25 ) does not hold in this case .\n\n\ud835\udcab\ud835\udcaf\\mathcal {pt}-symmetric deformations of the g 2 g_{2}-coxeter group: since only roots of one length emerge in root systems related to simply laced lie algebras , some features discussed this far are slightly different for non - simply laced cases .let us therefore present one explicitly example in order to exhibit the differences .we recall , see e.g. that the set of roots invariant under the inlineform144 -coxeter group separates into a set of short and long rootsinlineform145 and inlineform146 , respectively, displayform0using the cartan matrix with entries inlineform147 , inlineform148 and inlineform149 we may compute the action of the weyl reflections on the simple roots by evaluating ( eqref5 ) .the combinations of weyl reflections achieving a reflection across the hyperplanes through the origin orthogonal to the six positive roots are presented in the following table :  inlineform150table 1 : simple weyl reflections acting on the six positive roots of inlineform151having assembled the key properties for the undeformed root system , we choose as a starting point for the construction of inlineform152 the deformation of the simple roots inlineform153 or inlineform154 and extend the parity symmetry across the hyperplane through the origin orthogonal to these roots .according to ( eqref11 ) the deformed counterparts can be taken to be displayform0where we used the two fundamental weights inlineform155 and inlineform156 of inlineform157 .acting now with products of the complex reflections inlineform158 first on inlineform159 yields the deformations of the short roots displayform0the action of products of reflections inlineform160 on inlineform161 yields the deformations of the long roots displayform0for a particular representation we depict the constructed roots in figure 2 . figure 2 : real and imaginary parts of the inlineform162 -deformed roots in the two dimensional basis for the simple roots inlineform163 , inlineform164 .both parts of a particular positive rootinlineform165 are depicted in the same line style ( on - line also colour ) .as it should be by construction , we can check for consistence once more that indeed the imaginary part is orthogonal to the real part of each deformed root .again we observe the property displayform0and with the additional requirement displayform0we may fix the deformation functions to inlineform166 , inlineform167 .we have achieved a simple one - to - one relation between ( eqref20 ) and the corresponding identities for the deformed system simply by replacing inlineform168 and inlineform169 .clearly we can also choose the deformation according to ( eqref16 ) as for the inlineform170 -case , but we will not report this here .\n\n\ud835\udcab\ud835\udcaf\\mathcal {pt}-symmetric deformations of calogero-moser-sutherland models: taking the previous remarks into account it is now straightforward to formulate new types of cms - models , which are invariant under the action of inlineform171 -symmetrically extended weyl groups inlineform172 .the hamiltonian will be of the form inlineform173 as specified in ( eqref3 ) .let us study some concrete examples .\n\n\ud835\udcab\ud835\udcaf\\mathcal {pt}-symmetrically deformed a 2 a_{2}-calogero-moser-sutherland models: beyond the two particle problem the inlineform174 -cms model is the simplest classical example , constituting in some representation the three - body problem with a two particle interaction   .for this coxeter group we consider now the hamiltonian inlineform175 in ( eqref3 ) with the two simple roots taken in the standard three dimensional representation inlineform176 and inlineform177 , with inlineform178 being an orthogonal basis in inlineform179 with inlineform180 and the dynamical variables to be inlineform181 .using then the deformed roots as constructed in ( eqref21 ) -( ) , the potential of the inlineform182 -symmetrically extended model acquires the form displayform0where inlineform183 can be of calogero type , i.e. inlineform184 or any of the functionsinlineform185 , inlineform186 , inlineform187 .by construction these potentials are symmetric with regard to inlineform188 , which of course can also be seen explicitly for the dynamical variablesinlineform189inlineform190 , inlineform191 inlineform192and inlineform193inlineform194 .instead of the three dimensional representation we may also represent the roots in a two dimensional space , i.e. inlineform195 , inlineform196 and express the dynamical variables in terms of jacobi relative coordinates inlineform197 . comparison between the two representations then leads to the well known relations between the different sets of variables inlineform198 and inlineform199 .the third coordinate is usually taken to be the center - of - mass coordinateinlineform200 .moreover , it is convenient to parameterize inlineform201and inlineform202 further in terms of polar coordinatesinlineform203 inlineform204 , inlineform205 .in this formulation the relations for the potential simplify even more with the special choice inlineform206 and inlineform207 as already mentioned after ( eqref26 ) .with these choices the potential ( eqref37 ) is transformed into displayform0taking the special case inlineform208 the version ( eqref38 ) of the inlineform209 -symmetrically extended inlineform210 -calogero model is essentially the potential suggested in   , where it was obtained by deforming directly the calogero model in the form ( eqref38 ) for inlineform211 across the symmetryinlineform212 via the recipeinlineform213 .we have demonstrated here how to obtain it as a special case from a more general and systematic setting .the virtue of the version ( eqref38 ) in the new coordinate system is that it leads to a separable schr\u00f6dinger equation .in section 4.3 we make use of this fact and investigate some properties of the model , notably to construct its eigenfunctions and eigenvalues .clearly we may also choose the deformations according to the alternative deformation ( eqref16 ) , in which case the inlineform214 -symmetrically extended model is of the form displayform0when choosing the roots to be in the standard representation .note that , whereas in the undeformed case the contributions form any negative roots equals the one resulting from its positive counterpart , now these roots give different contributions .expressing ( eqref39 ) in terms of jacobian relative coordinates and making in addition the choiceinlineform215and inlineform216 the potential simply becomes displayform0note that in the choice for inlineform217 we made use fact that we can multiply this quantity by any invariant of inlineform218 .clearly inlineform219 is such an invariant .thus when we restrict the sum in ( eqref39 ) , ( eqref40 ) to the positive or negative roots only the deformation is simply achieved by inlineform220 or inlineform221 , respectively .this corresponds to the deformation of the symmetry inlineform222 .one should note that the restriction to just half of the number of roots will break the invariance under the action of inlineform223 .\n\n\ud835\udcab\ud835\udcaf\\mathcal {pt}-symmetrically deformed g 2 g_{2}-calogero-moser-sutherland models: the inlineform224 -cms - model , constitutes a further standard example , since it can be viewed as the classical three - body problem with a two and a three - body interaction term   .as in the previous subsection we may now realize the roots in various different ways .either we can take the so - called standard three dimensional representation for the simple rootsinlineform225 , inlineform226 as concrete realization for the simple roots of inlineform227 in inlineform228 and the dynamical variables to be inlineform229 or alternatively we may also represent them in a two dimensional space as inlineform230 , inlineform231 and express the dynamical variables in terms of jacobi relative coordinatesinlineform232 .once again the comparison between the two representations yields to the same relations for the jacobi relative coordinatesinlineform233 and inlineform234 .explicitly the inner products in all coordinate systems are computed to displayform0the expressions for the short roots ( eqref42 ) , ( ) and ( ) just yield the expressions for the inlineform235 -roots inlineform236 , inlineform237 and inlineform238 in the standard representation .using the expressions ( eqref42 ) - ( ) in the hamiltonian inlineform239 in ( eqref3 ) , the inlineform240 -symmetrically deformed inlineform241 -cms potential becomes displayform0as a result of the aforementioned relation between the inlineform242 and inlineform243 -rootsthe corresponding potentials reduce as inlineform244 , when we switch off the three particle interaction inlineform245 and scale the deformation function .when specifying further inlineform246and inlineform247 we obtain displayform0once again we may also choose a different type of deformations according to ( eqref16 ) , in which the inlineform248 -symmetrically extended model can be brought into the form displayform0when choosing the roots to be in the standard representation .we may also express this in terms of jacobian relative coordinates with the choice inlineform249 and inlineform250as in the inlineform251 -case , such that the potential becomes displayform0let us now study some physical properties of these models .\n\neigensystems: let us now specialize the potential to the one of the calogero model , i.e. we take it to be inlineform252 , and determine the eigensystems of the deformed models .in general this is a difficult task as even for the undeformed cms - models the eigenfunctions are combinations of vandermode determinants and jack polynomials , e.g.   .however , in the cases under consideration we can follow a different route and be very explicit for some very particular choices of the deformation functions .as illustrated in the last subsection we may just consider the inlineform253 -calogero model and treat the inlineform254 -calogero model as a special case of the former by switching off the three particle interaction .the inlineform255 -model was already solved by calogero   almost fourty years ago and the inlineform256 -casethereafter by wolfes   .relying on these solutions , the construction of eigensystems for some specific deformed system is fairly simple , as they may be obtained by implementing a shift as was done in the inlineform257 -case   .for other choices of the functions inlineform258 and inlineform259 the solutions can not be constructed in direct analogy to the undeformed case .however , as was observed in   ,   even the simpler scenario is instructive as there are a few differences in the argumentation leading to various constraints on the parameters resulting from the implementation of physical requirements .the main consequence of the deformation is that some irregular solutions , which had to be discarded in the undeformed case become perfectly viable regularized solutions after the deformation .as a result the energy spectra of the deformed systems differ from those of the undeformed ones .let us briefly recall the argumentation of   ,   and treat thereafter the deformed case .the above mentioned variable transformations inlineform260 have the virtue that they convert the differential equation into a form allowing for completely separability   .the laplace operator transforms simply as displayform0the confining potential transforms as displayform0and the calogero potential as displayform0assembling these expressions into a hamiltonian it is then easy to see that in the inlineform261 -systemthe eigenfunctions can be factorized into inlineform262 , which leads , after separating off the center of mass motion , to the two separate eigenvalue equations displayform0these equations may be solved generically for any real values of the parameters inlineform263 including even the eigenvalues inlineform264 and inlineform265by displayform0here we abbreviated the constants inlineform266, inlineform267 denotes the kummer confluent hypergeometric function and inlineform268 the gauss hypergeometric function .implementing now various different physical requirements leads to the quantization condition for the eigenvalues and several restrictions on the parameters displayform0we briefly recall and extend the argumentations in order to illustrate how they need to be modified in the deformed scenario .the quantization condition inlineform269 originates from the physical requirement that the wavefunction should vanish for inlineform270 .using the asymptotic expansion for kummer 's confluent hypergeometric function , see e.g.   , displayform0with inlineform271 , one observes that for the arguments of the solution inlineform272 in ( eqref53 ) the function will usually diverge exponentially , unless this divergence is compensated by a diverging gamma function , either from the corresponding inlineform273 in ( eqref56 ) or inlineform274 in ( ) .as this is the case when the first argument in inlineform275 becomes a negative integer , i.e. when the hypergeometric series terminates , the wavefunctioninlineform276 vanishes at infinity with the condition inlineform277 .for these values the kummer confluent hypergeometric function reduces to a generalized laguerre polynomial inlineform278 by means of the identity displayform0and one obtains , up to normalization , the expression for inlineform279 already found by calogero   .note that this argumentation does not change even if we continue inlineform280 into the complex plane and inlineform281 remains also valid in that case .the constraint inlineform282 arises from the condition that a proper physical wavefunction should be finite on its domain .in the undeformed case the divergence of inlineform283 at inlineform284 can be cured by the constraint inlineform285 .clearly this constraint can be removed if inlineform286 acquires a nonvanishing imaginary part , since the factor inlineform287 no longer diverges for inlineform288 .the constraint inlineform289 results from same requirement as inlineform290 , but demanding finiteness in the entire domain also for its derivative .for inlineform291 and inlineform292 the prefactors in ( ) would diverge for inlineform293 and inlineform294 , respectively .clearly when inlineform295 there is no longer any justification for this constraint and it can be removed , thus allowing the values inlineform296 .the quantization conditioninlineform297 stems from the divergence of inlineform298 at for instance inlineform299 .this is seen from the fact that for generic arguments the function inlineform300 is absolutely convergent when inlineform301 ,which for the values in ( ) translates into inlineform302 .having already excluded inlineform303 by conditioninlineform304 this inequality can never be satisfied .however , when inlineform305 becomes a negative integer the hypergeometric series terminates and reduces to a jacobi polynomial inlineform306 by means of the identity displayform0since inlineform307 with inlineform308 the divergence is removed by condition inlineform309 .alternatively we could also equate the second argument in ( ) to an integer and deduce inlineform310 , which is however excluded by condition inlineform311 .notice that when inlineform312 , we will even leave the unit circle inlineform313 , in which convergence can be achieved unless we restrict the real part of inlineform314 depending on its imaginary part , which seems very artificial .thus in this case terminating the series by means of property ( eqref61 ) appears even more natural than in the undeformed case .in summary , when the physical constraints inlineform315 hold , the corresponding wave functions are displayform0with energy spectrum displayform0let us now see in a concrete case how the deformation weakens the constraints and how it influences the physics of the models .we may now consider various types of deformations ( eqref11 ) or ( eqref16 ) depending in addition on the possible selections for the deformation functions inlineform316 and inlineform317 .we consider the deformed inlineform318 -calogero model , with the deformation ( eqref11 ) and the simplest choice for the deformation functions inlineform319 and inlineform320 .this leads to the differential equations ( eqref52 ) and ( ) with a shifted inlineform321 , i.e. the wavefunctions are simply obtained from ( eqref62 ) , ( ) by inlineform322 with inlineform323 .however , there is a small change in the physical interpretation .from the discussion of the previous subsection follows that inlineform324 and inlineform325 still have to be implemented on physical grounds , but to demand inlineform326 lacks any justification , since the wavefunctions are regularized and no longer diverge .therefore inlineform327 can be relaxed .consequently we end up with the modified energy spectrum displayform0such that besides the energies inlineform328 we may now also encounter the energiesinlineform329 .note that we have a degeneracyinlineform330whenever displayform0a similar observation was made for inlineform331 -calogero model in   .alternatively we may investigate the deformed inlineform332 -calogero model ( eqref46 ) based on the deformed roots ( eqref16 ) with deformation inlineform333 and inlineform334 .the wavefunctions are easy to construct in this case when we break the invariance under the extended coxeter group inlineform335 by restricting the sum in the potential to the positive or negative roots only and scaling the coupling constants inlineform336 by a factor of 2 .then the corresponding wavefunctions result from ( eqref62 ) , ( ) as inlineform337 with inlineform338 .for each of the models the constraints inlineform339 and inlineform340 still hold on physical grounds , but as the divergence at inlineform341 for inlineform342 has vanished we no longer have to demand inlineform343 .this means for both models , that is either extending the roots just over the positive or just over the negative roots , we have the identical energy spectradisplayform0thus allowing in addition to inlineform344 also inlineform345 .we encounter the degeneracyinlineform346inlineform347 when inlineform348 .due to the identity displayform0we find in that situation the wavefunction are related as displayform0in general , we have the symmetry inlineform349 , such that we can related the wavefunctions of the positive root model inlineform350 and the negative root model inlineform351 by an anyonic statistic as inlineform352 .\n\nconclusions: we have demonstrated that the coxeter group represented in inlineform353 can be deformed in a systematic way to the inlineform354 -symmetrically extended coxeter groupinlineform355 represented in inlineform356 .as we have shown there are various ways to achieve this .we may deform the roots across the hyperplanes through the origin orthogonal to each root either by taking the imaginary part to be a vector in this hyperplane ( eqref11 ) or a vector orthogonal to it ( eqref17 ) .as a natural application one may seek for models for which this group constitutes a symmetry group .cms - models are well known to be invariant under the action of inlineform357and we have demonstrated how they may be deformed such that they remain invariant under the action of inlineform358 .in fact one simply needs to replace the rootsinlineform359 by their deformed counterparts inlineform360 .we have worked out the inlineform361 and inlineform362 -casesin some detail by constructing explicitly the deformed root systems and applying them thereafter to the corresponding cms - models .when specializing the deformation functions inlineform363 and inlineform364 in a certain way some easy cases resemble the undeformed case with some simple shifts when transformed to jacobian relative coordinates , which allowed to determine their corresponding eigensystems .we discussed that as a consequence of the deformation the physical reason leading to some constraints vanishes , such that various restrictions on the parameter space of the model may be relaxed .various open challenges remain , as for instance to establish whether the deformations studied here preserve integrability , analogously to what has been established in   for the different types of deformation , to investigate models for different choices for the functions inlineform365 and inlineform366 and to study in detail coxeter groups of higher rank , together with their applications , such as the cms - models  . models with different choices for the deformation functions will certainly also lead to non - hermitian hamiltonians with real spectra , which may be explained by the built in inlineform367-symmetry   ,   or pseudo - hermiticity   .acknowledgments : af would like to thank paulo gon\u00e7alves de assis for discussions .the participation of mz supported by the m\u0161mt\u201c doppler institute\u201dproject nr .lc06002 , by ga cr grant nr .101/1307 and by the institutional research plan av0z10480505 .",
      "target_text": "we demonstrate that coxeter groups allow for complex pt - symmetric deformations across the boundaries of all weyl chambers .we compute the explicit deformations for the a_2 and g_2-coxeter group and apply these constructions to calogero - moser - sutherland models invariant under the extended coxeter groups .the eigenspecta for the deformed models are real and contain the spectra of the undeformed case as subsystem .",
      "output_text": "thea: and to of"
    },
    {
      "input_text": "Title: Evolutionary Trigger Set Generation for DNN Black-Box Watermarking\nDomains: Mathematics\n\nintroduction: since the success of large scale neural networks in the early 2010s , we have witnessed an explosive growth in the field .the popularity grew not only in academia but in the industry as well .deep neural networks ( dnns ) have become the de facto solution to many complex computer vision , speech recognition , and natural language processing problems .popular as deep learning may be , building ddns to solve real - world problemsremains an arduous task .it requires a vast amount of high - quality labeled data and heavy use of computational resources and human expertise .it goes without saying that dnns are invaluable technological assets that potentially has huge commercial impacts .over the past few years , myriads of companies have joined the ai arms race .just among ai startups , investment from the venture capital market reached a record high $ 9.3 billion in 2018   .among the companies , many provide range from commercial libraries for embedded systems , cloud machine learning apis to building private corporate clouds for ai , spanning across industries like transportation , manufacturing , healthcare , finance , and consumer electronics .while the dnns are fueling commercial successes in the ai market , a void in ip protection for dnn models may hinder the progress .when a model owner sells a service to a customer , she should have a reliable way to prevent the customer from illegally distributing or reselling it .to achieve that goal , the owner not only needs to identify her own model when it is distributed , but also prove the ownership to a trusted arbitrator .recently , several researchers proposed watermarking as a viable solution to the ip protection problem in deep learning        .digital watermarking originally refers to the process of covertly embedding information in multimedia content .the concept has since been extended to cover software   , circuits   as well as dnns .white - box watermarking embeds the owner 's information in the weights of a dnn .black - box watermarking , on the other hand , embeds the watermark in the input - output behavior of the model .the set of input used to trigger that behavior is called trigger set .for the popular task of image classification , a common approach is to assign a random label to trigger images and train the model to classify accordingly .the non - triviality of ownership of a watermarked model is constructed on the extremely small probability for any other model to exhibit the same behavior   .through detecting the watermark in a dnn model , the owner will be able to both identify and prove her ownership .based on the characteristics of their trigger sets , existing black - box watermarking methods can be split into two categories .the first category of methods curates a finite set of special trigger images .the special images can be completely random   , samples derived from unused hidden space   , or adversarial examples   .another category of methods maintain trigger patterns and add them to natural input images to create trigger sets .the trigger patterns are usually meaningful patterns that can serve as a proof of the owner 's identity , such as the logos   and color - coded keys   .figure figref1 describes the workflow of the method .some sample trigger images are shown in figure figref5 .the motivation for designing the trigger sets are different between those two categories of methods .the first is focused on the functionality of the model .they aim to create trigger sets such that watermarking is as orthogonal to the normal functionality of the model as possible .the second , on the other hand , is more focused on the watermarking extraction procedure .associating the owner 's identity with the trigger set makes detection and proof of ownership much more straightforward .the evident drawback of the first category is the difficulty to establish a connection between the trigger set and the owner .to solve that problem , researchers went as fars as to use complex cryptographic tools   .further , the limited size of the trigger set weakens the proof of ownership .the drawback of the second category lies in an inevitable trade - off between the robustness of the watermark and potential of false positive watermark detection .if a trigger pattern is too prominent , then it risks triggering false positives in other neural networks .on the other , if a pattern is too inconspicuous , it may be easily removed during fine - tune attacks .in this paper , we aim to bridge the gap between the different trigger set generation methods .we propose a differential evolution - based framework to determine how any given trigger pattern should be added to the image such that false positive detections are reduced while the robustness of the watermark is maintained .with our framework , trigger pattern - based watermarking adds the model functionality to its equation , while still keeping ownership proofs simple .the contribution of our paper are as follows :the rest of the paper is organized as follows : section secref2 describes the watermarking problem in more details and defines the problem .section secref3 presents our algorithm .section secref4 evaluates the performance of the proposed algorithm .\n\npreliminaries: in this section , we will first introduce the background of watermarking .then we will define our problem .similar to most of the security - related literature , we use the alice / bob narrative to describe the scenario .alice will be the model owner .bob will be the customer who buys the model from alice , and also the malicious attacker who tries to infringe on alice 's ip rights .\n\ndnn watermarking: we define dnn watermarking as the process of covertly embedding information in the dnn in order to verify and prove an owner 's ownership .we focus on black - box watermarking for image classification , which achieves the aforementioned goal by embedding special input - output patterns in dnns .to embed the watermark , alice will train a dnn with both the regular dataset and the trigger set with specific output labels .to detect the watermark , alice will use a subset of the trigger set as the input to the dnn and observe the output .there will be a positive detection if certain probability requirements are met .a successful watermarking method has to meet several criteria regarding its effectiveness , fidelity , false positive rate , and robustness .a detailed description of the criterion is presented in table tabref10 .first , the effectiveness criterion states that a watermark has to ensure successful and consistent detection .the fidelity criterion states that watermarking can not have a significant negative impact on the regular functionality of the model .false positive rate and robustness will be discussed separately in the following subsections .\n\nproof of ownership: a watermarking method 's ability to prove ownership mainly relies on its low false positive rate .suppose that alice decides that a watermark detection is positive if there are at most inlineform0 misclassifications among inlineform1 trigger images .then the probability of the detection can be calculated as follows    , assuming independence between the classification of each trigger image .inlineform2 represents the accuracy of the model on the trigger set .displayform0the ownership is established based on the fact that inlineform3 is disproportionally small for a non - watermarked neural network .if a watermarking method incurs a high false positive rate ( a high inlineform4 for non - watermarked models ) , then inlineform5 is no longer small and that the proof of ownership will be inconclusive at best .\n\nthreat model: in this subsection , we introduce our notion of robustness by defining our threat model .we assume that bob has white - box access to the model , but does not have access to the training set .instead , bob has access to some proprietary test data ( i.e. a subset of test set ) .we argue that proprietary data is one of the most important competitive advantages of alice , and an ip pirate bob by no means should have access to it .otherwise , with the training data and the model , he might as well train a new model on his own , especially when he has the ability to carry out sophisticated attacks such as fine - tuning .on the other hand , it is a reasonable assumption that the attacker may have white - box access to the model architecture and parameters .in the case of cloud ml service , bob can be a malicious service platform .in the case of software ml libraries , bob can be a hacker .in both cases , bob would have the full white - box access to the model .we also assume that alice only has black - box access to the model .in addition , alice will have direct access to input to the model .there are no preprocessing stages between alice 's input and the input of the model .with some test data and the model , bob may fine - tune the model to produce a slightly different version of it .that is called the fine - tune attack .after the fine - tune attack , alice 's watermark should still exist .some researchers also discussed overwrite attacks , where bob tries to embed his own watermark using the same procedure on alice 's model .it is indeed a very reasonable attack scenario .in our experiments , we found that embedding a new watermark using bob 's limited amount of data would adversely affect the model 's performance , rendering the model much less valuable .thus we rule out the possibility of bob carrying out overwrite attacks .\n\nproblem definition: a dnn for classification is a function inlineform6 .given an input inlineform7 , it is desired that the function classifies it correctly to its label inlineform8 , inlineform9 .a pattern inlineform10 has the same dimensions as inlineform11 , but is much more sparse .in its image form , inlineform12 's non - zero entries can be considered as a set of inlineform13 pixels with explicitly designed values and coordinates inlineform14 .inlineform15 is tightly coupled with the identity of the model owner .and the absolute and relative coordinates of the pixels may or may not contribute to inlineform16 's ability to carry information .the pattern can be embedded on any input inlineform17 from the intended data distribution to convert it into a trigger input through a function inlineform18 .a watermarked dnn will be trained to classify inlineform19 to inlineform20 .the fact that a dnn model classifies the trigger inputs disproportionally correctly can serve as a unique proof of the owner 's identity .we consider two alternative approaches to create trigger patterns .in the method proposed by guo et al .( shown in figure uid8 ) , a color - coded key serves as the trigger pattern   .they embed the pattern by offsetting the pixel values of the input , inlineform21 .since the information is mainly ingrained in the pixel values , we consider the pixel locations inlineform22 to be flexible .we use key throughout the paper to denote this trigger pattern .the second approach we consider is proposed by zhang et al . , and shown in figure uid7   .the information is obviously contained in the geometrical shape of the logo and the pixels have to remain in a relatively fixed to each other .thus its location can be represented by its top left corner inlineform23 .the author did not explicitly say how they embed the logo , but we interpret it as blending with the input , inlineform24 .we denote the second type of trigger pattern as logo .our main goal is to find the inlineform25 such that the probability of a non - watermarked dnninlineform26classifying a trigger input to its original labels is maximized .the main motivation behind the goal is to minimize false positive watermark detection .empirically , given dataset inlineform27 , then the goal can be expressed as follows .displayform0we have found that larger inlineform28 leads to more robust watermarking , although it leads to higher false positives .in the key - related experiments , inlineform29 is given .but we can also integrate inlineform30 into the optimization landscape as follows .the logo - related experiments use this objective function .displayform0 inlineform31\n\nmethod: in this section , we first provide a high level overview of why we chose the de framework and how it works .then we delve deeper to provide some algorithmic details that are crucial to the convergence of de .\n\ndifferential evolution: differential evolution input : dataset inlineform32 , non - watermarked dnn model inlineform33 , populartion inlineform34 , number of generations inlineform35output : best candidate inlineform36after evolution   randomly intialize , inlineform37th candidate inlineform38 generationinlineform39each candidate inlineform40randomly pick inlineform41where inlineform42 inlineform43 evolve ( inlineform44 , inlineform45 , inlineform46 ) fitness ( inlineform47 ) inlineform48 fitness ( inlineform49) inlineform50 inlineform51to find the pattern inlineform52 , the first methods that came up to us were the gradient - based methods commonly used for finding adversarial samples     .a key difference between our problem and theirs is that our pattern inlineform53 is universal .therefore , finding the gradient of individual inputs hardly helps our situation .the family of evolutionary algorithms are among the most prominent non - gradient - based optimization methods .we initially relied on the generic evolutionary algorithm ( ea ) , but we were unable to find a reasonable set of parameters to make the algorithm converge .that is when differential evolution ( de ) presented itself as an alternative .de is a metaheuristic search algorithm that optimizes a given objective by evolving a population of candidates in parallel   .it follows the concept of generic eas where a population of candidates evolves , and the candidates that are fittest will survive in each iteration .however , de is simpler and it is known to facilitate faster convergence to the global optimum .instead of using mutations and crossover between two parents , candidates de evolve over a triplet .a new candidate is created by adding a weighted difference between two candidates to the third .displayform0algorithm secref18 presents the high - level procedure of using de to solve our problem .the main idea is to generate 1 ) new pixel coordinates 2 )pixel values of inlineform54 using the differential variation operation described in equation eqref19 .the fitness function can be either of the two objective functions described in section secref14 .the evolve function , on the other hand , is more complex .we describe more details of the function in the next subsection .\n\noptimizations for de: [ ht ] evolve with closest triplet input :3 candidates , each containing inlineform55 pixel coordinates : inlineform56 , inlineform57 , inlineform58 , differential weight inlineform59output : pixel coordinates of a new candidate , inlineform60  pair inlineform61 ,inlineform62 initialize inlineform63each inlineform64 calculate pairwise distances each inlineform65 push distance ( inlineform66 , inlineform67 )in inlineform68 inlineform69 is not empty pair by distance distance ,inlineform70 , inlineform71 inlineform72 pop from inlineform73neither inlineform74 or inlineform75 is paired pair ( inlineform76 with inlineform77 )pair ( inlineform78 , inlineform79 ) pair ( inlineform80 , inlineform81 )each inlineform82 inlineform83 paired with inlineform84 inlineform85 inlineform86inlineform87we use two different variants of evolve functions for the two existing trigger patterns , key and logo .for logo , the evolve function is straightforward .we use the top left pixel of the logo as the anchor , and each candidate can be represented by a simple triplet inlineform88 .we use de to evolve and select the location of the logo as well as its pixel values .we use a different approach for key .since pixel locations are al flexible , the candidate will be an array of inlineform89 tuples inlineform90 .when we evolve using three candidates , each with inlineform91 pixels , which pixels should pair up and evolve becomes an important question .if pixels are randomly paired up , it is likely that the pixels will engage in a brownian motion - like movement across different generations .consequently , as we empirical show later , the evolution will not converge .if our goal is to evolve the pixels into optimal locations , then it makes sense to induce the evolution in such a way that pixels nearest to an optimal location will move toward that location .to that end , we propose an algorithm to pair closest pixels together to evolve .the most efficient implementation is to store all pairwise distances in heaps and always pair available pixels with the smallest distances .algorithm secref20 describes implementation in more details .the time complexity of the algorithm is inlineform92, where inlineform93 is the number of pixels .figure figref21 shows the fitness of the best candidate over the different generations .the fitness function is simply the accuracy of the subset .the proposed method , closest triplet evolve function , converged much faster than the evolve function where pixels are randomly paired together .in fact in the latter case , the fitness plateau at around 0.89 and it is unclear whether it will converge at all .\n\nevaluation: in this section , we report the performance evaulations of our method .we first describe implementation details of the watermarking procedure and the de algorithm .then we evaluate the effectiveness , fidelity , false positive rate and robustness of the watermark in following subsections .since neither logo nor key is an original idea from this paper , we omit many repetitive experiments for brevity .the key is to demonstrate the ability of our de algorithm to reduce false positive while maintaining the robustness of the watermark .it is worth noting that all of our trigger sets are built from test data that has not been used during training .\n\neffectiveness and fidelity: it has been demonstrated in all previous works that dnns can be trained to successfully recognize the triggers sets .in addition , adi et al .also showed the significance of start training from scratch in creating a robust watermark   .we followed the same procedure .table tabref31 shows the classification accuracy of both non - watermarked and watermarked models on the regular test set and the trigger set .in light of the fidelity criterion , the classification accuracy of the watermarked model on the regular test set is sligtly lower compared to the regular non - watermarked model .it is expected as watermarking makes the classification problem much harder .in light of the effectiveness criterion , the ability of the watermarked model to recognize the trigger set is as good as its ability to classify regular images .\n\nfalse positives: figure figref33 shows the false positive rate of different trigger patterns .the false postive rate here is measured by the probability that a non - watermarked model classifies a trigger image into its re - assigned class inlineform101 .we used four different non - watermarked dnn trained on the regular cifar-10 dataset : resnet-18 , resnet-50 , densenet-121 , vgg-16 .the fitness function in de used to obtain the trigger pattern only involves the resnet-18 model .the results show that the what our de learns from one model generalizes well to other models as well .we tested the generalizability further using the same pattern on 5 newly trained vgg-13s , and obtained a 95 % confidence interval of inlineform102 .we used two baselines for comparison .to compare with the de - based key pattern , we used a key pattern with random inlineform103 but the same inlineform104 .we see drastic improvements with up to 10 inlineform105 reduction in the false positive rate .note that the trigger pattern proposed by guo et al .is also based on random location   .but they explicitly selected inlineform106 such that the pattern is imperceptible , resulting in a lower false positive rate .but as we see later , they achieved that at the cost of robustness .to compare with the de - based logo , we use logo trigger pattern used zhang et al .   .we achieve about 2 inlineform107 improvement in false positive rate .putting it in the perspective of equation eqref12 , even 2 inlineform108 translates to over inlineform109 lower probability ( inlineform110 ) .\n\nrobustness: we measure the robustness of the watermarking methods through their resistance against fine - tune attacks .table tabref36 reports trigger set classification accuracy loss after we fine - tuned a watermarked model .unlike some of the earlier approaches that based their attack on the training set , we used 1000 test images and applied various data augmentation techniques .the model watermarked using the original key method suffered a significant drop in accuracy .the accuracy drop was almost entirely eliminated when we switch to our key method .both of the logo method were resilient against the fine - tune attack .images superimposed with our pattern are sufficiently different from the normal input distribution .because of that , a watermarked model 's ability to recognize those patterns is largely orthogonal to its ability to classify objects and is , therefore , harder to remove during a fine - tune attack .\n\ndiscussions: gradient - free methods do exist in the world of adversarial learning , most notably in the subproblem of black - box attacks , where the attackers do n't have access to the gradient information      .those methods again focus on individual samples and are essentially solving a different problem than ours .it is worth noting that the work from moosavi - dezfooliet al .aims at creating universal adversarial perturbations   .their proposal to reduce the search space to a subset of the input provided invaluable insights .due to the limited scope of this paper , the parameter inlineform111 is n't systematically studied .it is more a heuristic and manually selected in many situations .it would be valuable to study how it systematically affects the robustness of the watermarking methods .\n\nconclusion: black - box dnn watermarking has emerged as a viable solution to ip protection in the context of mlaas .adding owner identity - based trigger patterns to natural input images is a popular method to create effective trigger sets that establish strong ownership proofs .in this paper , we propose a novel differential evolution - based framework to optimize the generation of such trigger patterns .compared to the prior art , our method demonstrates significant improvement in false positive rate and robustness in experiments on popular models and datasets .",
      "target_text": "the commercialization of deep learning creates a compelling need for intellectual property ( ip ) protection .deep neural network ( dnn ) watermarking has been proposed as a promising tool to help model owners prove ownership and fight piracy .a popular approach of watermarking is to train a dnn to recognize images with certain \\textit{trigger } patterns .in this paper , we propose a novel evolutionary algorithm - based method to generate and optimize trigger patterns .our method brings a siginificant reduction in false positive rates , leading to compelling proof of ownership .at the same time , it maintains the robustness of the watermark against attacks .we compare our method with the prior art and demonstrate its effectiveness on popular models and datasets .",
      "output_text": "thea: and to of"
    },
    {
      "input_text": "Title: Introspection Learning\nDomains: Mathematics\n\nintroduction: one notable feature of human learners is that we are able to carry out counter - factual reasoning over unrealized events .that is , we contemplate potential answers to questions of the form , \u201c what would i do in situations inlineform0 , inlineform1 , and inlineform2 ? \u201da related , and perhaps more pertinent , form of question is , \u201c are there situations inlineform3 , inlineform4 , and inlineform5 ,such that in these situations i would select actions inlineform6 , inlineform7 , and inlineform8 ? \u201din this case , the actions inlineform9 , inlineform10 , etc . , might be actions that are likely to result in particularly good or bad outcomes , and answers inlineform11 , inlineform12 , etc . , can be useful , especially when they are of an unexpected nature , since they reveal potential failures of robustness ( in the case of bad examples ) or potential strengths ( in the case of good examples ) .in this paper , we describe a novel approach to answering and utilizing the answers to questions of this form when asked not of a human agent , but of a reinforcement learning agent .our approach is not based solely on the deployment of techniques from the typical machine learning toolbox , as we make crucial use of smt - solving , which is more familiar to researchers in the field of formal methods .in the theoretical development , we capture our use of smt - solving technology via the abstraction of what we are calling introspection oracles : oracles that may give us direct access to sets of ( state , action ) pairs satisfying fixed constraints with respect to the policy network .by querying the oracle during training it is possible to generate ( state ,action)-pairs capturing failures / strengths of the agent with respect to properties of interest .for instance , if there are certain \u201c obviously wrong \u201d actions that the agent should never take ( e.g. , selecting a steering angle that would cause the automobile controlled by the policy network to drive off of the road when there are no obstacles or other dangers present ) , we query the oracle as to whether there exists states in which the agent would select such actions .our algorithm then uses this data to train so as to improve the safety of the agent and without requiring that such potentially dangerous or costly situations be encountered in real life .it is true that such ( state , action )pairs are potentially discoverable in simulation / testing , but when the set of such pairs is known beforehand we save time and improve policy robustness by generating them analytically .in this paper , we introduce a new algorithm for reinforcement learning , which we call the introspection learning algorithm , that exploits introspection oracles to improve the training and robustness of reinforcement learning ( rl ) agents versus baseline training algorithms .this algorithm involves modifying the underlying mdp structure and we derive theoretical results that justify these modifications .finally , we discuss several experimental results that clearly showcase the benefits to both performance and robustness of this approach .in particular , in the case of robustness , we evaluated our results by querying the weights after training to determine numbers of sat ( examples found ) , unsat ( examples mathematically impossible ) and timeout( ran out of time to find or refute existence of examples ) results .the paper is organized as follows .in section secref2 we introduce the mathematical abstraction of introspection oracles and discuss briefly their embodiment as smt - solvers .section secref3 details our introspection learning algorithm .finally , section secref4 captures our empirical results .the appendix ( section secref6 ) includes the proof of a basic result that justifies the modification of mdps made in our algorithms .\n\nrelated work: previously , linear programming , which is itself is a constraint solving technique , has been employed in reinforcement learning to constrain the exploration space for the agent \u2019s policy to improve both the speed of convergence and the quality of the policy converged to   or as a replacement for more traditional dynamic programming methods in q - learning to solve for equilibria policies in zero - sum multi - agent markov game mdps   .previous work has also been done on incorporating quadratic program solvers to restrict agent exploration to \u201c safe \u201d trajectories by constraining the output of a neural network policy   .introspection learning is fundamentally different from these approaches as rather than restricting the action space , or replacing our q function , we are instead shaping our agents in policy space by asking our policy for state batches where it would satisfy stated constraints , without needing the agent to actually experience these states .exciting recent work on verification of neural networks ( e.g. ,   ,   ) is closely related the work described here .in addition to the similarity of the techniques , we are indeed capturing verification results as a robustness measure ( see below ) .one practical distinction is that we are using the dreal solver   , which is able to handle networks with general non - linear activations , but as a trade - off ( not made in other smt - solvers ) admits the possibility of \u201c false - positive \u201d inlineform13 -satisfiable instances .in principle , our algorithm can be used with any compatible combination of smt - solvers and neural network architectures .\n\nintrospection oracles: in order to set the appropriate theoretical stage , we will first introduce some notation and terminology .definition 1 a pre - markov decision process ( pre - mdp ) consists of a set inlineform14 of states , a set inlineform15 of actions , and transition probabilities inlineform16 in inlineform17 for inlineform18 and inlineform19such thatinlineform20 .intuitively , the value inlineform21 is the probability inlineform22 transitioning from state inlineform23 to state inlineform24 on taking action inlineform25 .definition 2 given a pre - markov decision process ( pre - mdp ) inlineform26 , a policy for inlineform27 assigns to each state inlineform28a probability distribution inlineform29 over the set inlineform30 .a pre - mdp is called a mdp inlineform31 r in , e.g ..often we are concerned with cases where inlineform32 is finite and the policies inlineform33 under consideration are deterministic in the sense that , for each state inlineform34 , inlineform35 for all but a single element inlineform36 of inlineform37 .when inlineform38 we write inlineform39 .given a pre - mdp inlineform40 , we denote by inlineform41 the set of all policies for inlineform42 .definition 3 a markov decision process ( mdp ) consists of a pre - mdp inlineform43 together with a reward function inlineform44which is bounded , a subset inlineform45 of terminal ( state , action)-pairs , and a new state inlineform46 not in inlineform47such that :one non - standard feature of definition secref4 is that we consider terminal pairs inlineform48 rather than terminal states .this will be technically useful below .we also follow   in that the provision of terminal pairs modifies the pre - mdp structure in adding a dummy stable state inlineform49 to which all terminal states canonically transition such that subsequent transitions from inlineform50 have no reward .this is a technical convenience which streamlines some of the theory .we denote by inlineform51 the set of all markov decision processes over the pre - mdp inlineform52 and by inlineform53 the set of all policies over inlineform54 .given an mdp inlineform55 in inlineform56 , we denote by inlineform57 the subset of inlineform58 consisting of those policies that are optimal for inlineform59 .in broad strokes , inverse reinforcement learning   is concerned with , given a policy inlineform60 in inlineform61 ( or , more often , a set of its trajectories ) , determining an element inlineform62 of inlineform63such that inlineform64 is in inlineform65 .we are concerned with a closely related problem .one difference between our approach and that of inverse reinforcement learning is that instead of assuming access to a target policy inlineform66 or its trajectories , we assume that we have access to certain properties that target policies ought to have .in the simplest case , such a property is given by a subset of the set inlineform67 of ( state , action ) pairs .we refer to policies with the required properties as good policies .there is considerable flexibility in the notion of goodness here , but in many cases it will be associated with safety and robustness .e.g. , a good policy for driving a car would not make unexpected sharp turns when the road ahead is straight and clear of obstacles .much of our focus is on these kinds of examples , but it is worth emphasizing that goodness could instead be associated with performance rather than safety .in order to make the problem tractable , it is necessary to restrict to sufficiently well - behaved subsets of inlineform68 .for us , the well - behaved subsets are those definable in the first - order theory of real arithmetic with common non - linear function symbols ( e.g. , inlineform69 , inlineform70 , inlineform71 , inlineform72 , etc . ) .denote by inlineform73the set of all such subsets of inlineform74 .with this notation in place , we arrive the definition of introspection oracle .definition 4 given policy inlineform75 in inlineform76 ,an introspection oracle for inlineform77 is a map inlineform78such that if inlineform79 , then inlineform80 is in inlineform81 .an introspection oracle is non - trivial when there exists inlineform82 in inlineform83such that inlineform84 .intuitively , an introspection oracle inlineform85 for inlineform86 attempts to answer questions of the form : \u201c are there inputs that give rise via inlineform87 to a ( state , action ) pair with property inlineform88 ? \u201dhere inlineform89 is an error signal which can be provided with several possible semantics .here it is best understood as indicating that the oracle was unable to find an element of inlineform90 in a reasonable amount of time .before turning to describe our use of introspection oracles in reinforcement learning , we observe that non - trivial introspection oracles do indeed exist :observation 1 for policy functions inlineform91 definable in the language of first - order real arithmetic with non - linear function symbols ( inlineform92 , inlineform93 , inlineform94 , inlineform95 , etc . )there exist non - trivial introspection oracles .the existence of such introspection oracles which are moreover practically useful in the sense of returning outputs inlineform96 in a wide range of feasible cases is guaranteed by the inlineform97 -decision procedure of gao , avigad and clarke   , which is implemented in the dreal non - linear smt - solver .the novelty of dreal is that it overcomes the undecidability of real arithmetic with non - linear function symbols by accepting a compromise : whereas unsatisfiable ( unsat ) results are genuine , satisfiable ( sat ) results may be false - positives .note that , unlike in many of the other applications of smt - solving to verification of neural networks such as dreal is able to handle all common non - linear activations .in terms of our abstraction , spurious sat results , which are easily detected by a forward pass of the network , can be regarded as instances where inlineform98 .\n\nthe introspection learning algorithm: we now describe the introspection learning algorithm in detail , starting with its inputs .first , this algorithm assumes given an off - policy reinforcement learning algorithm ( oprl ) and corresponding policy functioninlineform99 .it is furthermore assumed that inlineform100 is describable in the language of real arithmetic with non - linear function symbols .additionally assume given a family inlineform101 of subsetsinlineform102 , which will be used when we query the oracle inlineform103 .having a sufficiently rich family inlineform104 will provide a mechanism for generating more useful examples and the design of these propertiesis one of the main engineering challenges involved in utilizing the algorithm effectively .pairs inlineform105 obtained from the oracle as inlineform106 are added to the oprl agent 's replay buffer .off - policy rl algorithm oprl , policy functioninlineform107 , family of queries inlineform108 , a scheduleinlineform109 , a reward cutoff inlineform110 initialize oprl policy inlineform111 with random weights inlineform112 and replay bufferinlineform113 episode inlineform114 train oprl as specified moving average reward inlineform115 and inlineform116 for each inlineform117 , query inlineform118 and add examplesinlineform119 to inlineform120as terminal introspection learningfinally , we assume given a schedule determining when during training to perform queries and updates .for simplicity in describing the algorithm we assume that the schedule is controlled by two factors .first , a simple set inlineform121 of training indices .second , a bound inlineform122 on moving average reward such that once moving average reward is greater than or equal to inlineform123 we no longer perform queries or updates on gathered examples .in summary , given the aforementioned inputs , the introspection learning algorithm secref3 proceeds by training inlineform124 as usual according to the oprl except that , when episode indices inlineform125 in inlineform126 are arrived at and the moving average reward remains below inlineform127 , the oracle is queried with the specified family of pairs, examples are gathered ( when possible ) and inserted into the replay buffer as terminal .mathematically , this algorithm effectively produces a modified mdp structure inlineform128 by altering the terminal pairs and the reward structure .in the appendix ( section secref6 ) , we show ( theorem secref33 )that , under reasonable hypotheses , the sets of optimal policies for the original mdp inlineform129 and the modified mdp inlineform130 coincide .there are several parameters and variations of this algorithm possible , of which we now mention several .first , in some cases it may be necessary or useful to post - process the gathered state batches( e.g. , to ensure sufficient balance / symmetry properties ) .here consideration should be paid to the bias introduced by state batches which are in one sense \u201c on policy \u201d ( if the agent were in a state returned by the smt - solver it would have taken the specified action with high probability ) , but are not guaranteed to be \u201c on trajectory \u201d as we have no guarantee the state would be reachable by policy inlineform131 .in practice , we have found such processing to be unnecessary provided that suitable inlineform132 are selected and a reasonable schedule is followed .in addition to varying the schedule , it is also possible to consider a range of options for the behavior of the replay buffer and how to train on the examples contained therein .we have found it to usually be sufficient to train on these as terminal states with high - negative or high - positive reward , however other approaches can also be considered .it should be noted that treating these states as terminal will alter the optimal policy , which may or may not be desired , and alternatively one could query the training environment with the state batches and specified actions to recover the reward signal and next state from the environment in order to reduce the change in the optimal policy .our intention was to take a na\u00efve approach as we are interested in applications where acquiring experience is potentially risky or expensive .\n\nexperimental environments and results: our experiments were conducted with the double deep q network algorithm ddqn   with prioritized experience replay   and the openai gym \u201c lunar lander \u201d environment   , openai gym \u201c cliff walk \u201d environment   and the deepmindai safety gridworld \u201c absent supervisor \u201d environment   .prioritized experience replay augments the selection of experience tuples from the ddqn replay buffer by preferentially selecting experience with high td error and simultaneously correcting for the bias this introduces by scaling the loss in the neural network update proportionally to the size of the td error .in the \u201c lunar lander \u201d environment the objective is to safely land a spacecraft on the surface of the moon by controlling four discrete actions for each of its three engines .the state space is eight dimensional with six continuous variables representing location in two - dimensional cartesian - coordinates , linear velocity , angle and angular velocity , and two boolean variables indicating whether or not contact is being made with the ground by each of the lander 's two legs .the reward signal positively reinforces movement toward the landing pad , as well as bonus for making leg contact with the ground .negative reward is given for moving away from the landing pad or losing contact with the ground .the environment is considered solved when the agent achieves a 100 episode moving average reward of at least 200 .in the \u201c cliff walk \u201d gridworld environment ( figure figref12 )the objective is to reach the goal state while avoiding the row of terminal \u201c cliff \u201d states along the bottom edge by controlling four discrete actions up , down , left , right .the state is encoded as a binary vector .the environment provides the agent a reward of -1 at each step and a reward of -100 for entering the cliff .the goal provides no reward and terminates the episode .in our experimentation the environment was considered solved when the agent achieved a 100 episode moving average reward of at least -30 .in the \u201c absent supervisor \u201d gridworld environment ( figure figref13 )the objective is to reach the goal state by controlling four discrete actions up , down , left , right .the four center squares are impassable .for each episode a supervisor is absent or present with uniform probability .the state is encoded as a binary vector .the environment provides the agent a reward of -1 at each time step and a reward of +50 for entering the goal .when the supervisor is present the orange state , located immediately above the goal state , highlighted in figure figref13provides a large negative reward ( -30 )but no such reward when the supervisor is absent .we would like the agent to never pass through the orange punishment state .the intent of the environment is to demonstrate that when provided the opportunity to cheat by passing through the orange state when the supervisor is absent traditional deep reinforcement learning algorithms will do so .in each case , the policy inlineform133 considered was a neural network with two hidden layers each having 32 nodes and hyperbolic tangent activations .the output activation was linear with one node for each action .ddqn with soft target network updates   , the proportional variant of prioritized experience replay   , and an inlineform134 greedy exploration policy were employed to train the agent with the hyperparameters summarized in table tabref15 .in the \u201c lunar lander \u201d environment , the introspection learning parameters were set as follows .for the query schedule , we determine at what interval batches will be searched for and when searching for batches will cease and training will proceed as normal .we experimented with solving for state batches at a predetermined interval ( every 100 episodes ) and ceasing when the 100 episode moving average reward crossed a predetermined threshold .for training on state batches , states found were treated as terminal states with high negative reward ( -100 ) as determined by the rules of the environment for terminal states .we have generally found that incorporating the state batches into the replay buffer is beneficial early in the learning process when the policy is poor , as it introduces bias ( cf .   ) .thequery constraints in both cases were to look for states whose inlineform135 -coordinates were outside of the landing zone ( inlineform136 or inlineform137 ) , such that the agent favors selecting an action that would result in it moving further away from from the landing zone .this region of the state - space was divided into boxes using a simple quantization scheme that ignored regions of state space where examples satisfying the query constraints would be impossible to find .in general , such quantization schemes should be sufficiently fine - grained to allow generation of many and diverse examples .twenty training runs with a set of twenty random seeds were run with and without our approach for a maximum of 500,000 timesteps .results averaged over the training runs are summarized in figure figref14 .ddqn with introspection learning solved the environment in a mean of 893 episodes while ddqn without introspection learning ( baseline )failed to successfully solve the environment on average within 500,000 timesteps .in addition to observing performance benefits , we also evaluated the agents trained with introspection learning for robustness benefits .in particular , we periodically stored the weights of both the introspection learning agent and the baseline agent during training for each of the twenty runs .we then recorded , for different regions of state space , statistics regarding the sat , unsat and timeout results obtained when querying the smt - solver on these agents across training .to recall , in this case , a sat result indicates that there exists a stateinlineform138 in the specified region inlineform139 of state space such that an undesirable actioninlineform140 ( in this case , moving away from the landing zone ) is selected by the agent .likewise , an unsat result indicates that there is a mathematical proof that there exists no state inlineform141 in inlineform142such that inlineform143 is undesirable .we gathered sat , unsat and timeout data across a number of different selections of inlineform144 .tables tabref18 and tabref19 recordthe percentages of each kind of result across all twenty test runs that were captured at four points during training .the selection of inlineform145 queried here were a subset of the subsets of ( state , action)-space queried during the actual introspection learning training and the results show a clear improvement of robustness over the baseline .timeouts during training were set to five seconds and to ten seconds during evaluation .one interesting point that we noticed in analyzing the robustness evaluation data is that larger numbers of unsat results for the introspection learning agents were obtained at the beginning of training than the end .this is illustrated , for a typical example ( the run with id number 480951 ) in figure figref17 .this is likely due to the schedule employed as part of the introspection learning algorithm and highlights the more general fact that reinforcement learning agents are sometimes subject to \u201c forgetting \u201d important learned behavior at later stages of training .since the agents at the end of training were typically very good at solving the task , the regions of state space in which this forgetfulness would manifest themselves were likely off - trajectory ( i.e. , unreachable by the current policy ) .in order to emphasize that this improvement is very much a function of the specific inlineform146 used during training , and tested at evaluation time , we include for comparison in table tabref20the average percentages for an alternative selection of inlineform147 used at evaluation time .here the improvements are more modest .in the \u201c absent supervisor \u201d environment the introspection learning parameters were set as follows .solving for state batches is unnecessary as in this discrete state environment we are only concerned with the agent choosing to enter the orange punishment state from the state directly above it .for the query schedule solving for this specific behavior is performed at every timestep and during training this transition is treated as terminal with high negative reward ( -100 ) .results for ddqn with and without introspection learning are provided in figures figref21 and figref22 respectively .one interesting point about the \u201c absent supervisor \u201d environment is that , for the evident notion of good policy , one of the hypotheses ( the \u201c strong compatiblity \u201d assumption ) of our theorem secref33 is violated .in the \u201c cliff walk \u201d environment the introspection learning parameters were set as follows .solving for state batches is unnecessary as in this discrete state environment we are only concerned with the agent choosing to enter the cliff states which can only be done from the state directly above each cliff state respectively .for the query schedule solving for these specific behaviors is performed at every timestep and during training this transition is treated as terminal with high negative reward ( -100 ) .it should be noted that in this particular case the environment already treats these transitions as terminal with high negative reward ( -100 ) and thus introspection learning will not alter the optimal policy ( in particular , the hypotheses of theorem secref33 are satisfied ) .in this experiment , five training runs with a set of five random seeds were run with and without our approach until the environment was solved .during training , at each timestep , a running count was kept of the number of states from which the agent would select to enter the cliff states \u201c lemming \u201d .during training the policies were found to lemming on average 112 times with introspection learning and 29,501 times without .it was experimentally found that an agent with introspection learning would rarely learn a policy during training that would enter the cliff after the first training episode while it was routine for an agent without introspection learning .representative policies learned by ddqn with and without introspection learning after 30 training episodes are provided in figures figref23 and figref24 respectively .additionally , agents with introspection learning enjoyed a small performance benefit solving the environment in 208 episodes on average over the five training runs while agents without introspection learning averaged 229 episodes to solve the environment .\n\nconclusions: in this paper we have introduced a novel reinforcement learning algorithm based on ideas coming from formal methods and smt - solving .we have shown that , on suitable problems , these techniques can be employed in order to improve robustness of rl agents and to speed up their training .we have also given examples of how smt - solving can be used to analyze reinforcement learning agent robustness .there are a number of extensions of this preliminary work possible .we mention several prominent directions here .first , the focus here has been on single - step analysis of agent behavior , but a reachability analysis approach focused on trajectories leading to target states would likely generate more relevant data for learning .e.g. , consider a geo - fenced space that we do not want the agent to enter and that is reachable through many different ( state , action ) combinations .once a violation occurs , we would like to examine the trajectory in order to learn what earlier choices led the agent there .second , whereas in our \u201c lunar lander \u201d experiments we utilized an ad hoc quantization of the state space , it should be in many cases possible to learn such regions as part of the algorithm .this is a hard search problemso relying on these parameterizations is necessary and should therefore be automated .in conjunction with the reachability analysis mentioned above , this approach is likely to give more targeted and therefore useful data to include in the replay buffer .finally , while the smt - solving technology being used is sufficient for low - dimensional state - spaces , these techniques face scalability issues on large state - spaces such as those coming from video data .how to handle these higher - dimensional state - spaces in a similar way is one of the exciting challenges in this area .\n\nacknowledgments: we would like to thank ramesh s , doug stuart , huafeng yu , sicun gao , aleksey nogin , and pape sylla for useful conversations on topics related to this paper .we are also grateful to tom bui , bala chidambaram , cem saraydar , roy matic , mike daily and son dao for their support of and guidance regarding this research .finally , we would like to thank alessio lomuscio and clark barrett for their interest in this work and for encouraging us to capture these results in a paper .\n\nappendix: theoretical results: fix a pre - mdp inlineform148 and assume given a ( non - empty ) subset inlineform149 of inlineform150 which we regard as the good policies : those policies inlineform151 whose inlineform152 have the properties of interest .definition 5 mdpsinlineform153 and inlineform154 are equivalentwhenever inlineform155 and inlineform156 .furthermore , throughout this section we assume given a fixed mdp inlineform157 in inlineform158 .additionally , assume given a fixed discount factor inlineform159 .we also adopt throughout this section two further hypotheses , which we now describe .assumption 1 ( bad set )there exists a subset inlineform160such that inlineform161 is in inlineform162if and only if , for all inlineform163 , inlineform164 .our next hypothesis guarantees that the reward structure is already sufficiently compatible with inlineform165 .assumption 2 ( strong compatibility )all optimal policies for inlineform166 are in inlineform167 .i.e. , inlineform168 .we define a new mdp structure inlineform169 in inlineform170 by inlineform171  it is straightforward to prove that inlineform172 is bounded since inlineform173 is .note that we are also modifying the underlying pre - mdp here by now imposing the conditionthat inlineform174 .an immediate proof of the following proposition can be obtained using the notion of bounded corecursive algebra from   , where it is shown that the state - value functions inlineform175 are canonically determined by the generating maps inlineform176 given by inlineform177  where inlineform178 is the probability distribution monad .proposition 1 if inlineform179 is in inlineform180 , then inlineform181 .it suffices to show that inlineform182 , which is trivial for inlineform183 in inlineform184 .corollary 1 if inlineform185 is in inlineform186 , then inlineform187 if and only if inlineform188 .lemma 1 inlineform189 .suppose given an optimal policy inlineform190 for inlineform191 .by bellman optimality , inlineform192 is optimal for inlineform193 if and only if , for all inlineform194 , inlineform195  let inlineform196 and inlineform197 be given .there are two cases depending on whether or not inlineform198 .when inlineform199 ,inlineform200  where the equations are by corollary secref30 and the inequality is by optimality of inlineform201 .when inlineform202 , inlineform203  where the final inequality is by optimality of inlineform204 and the final equality is by corollary secref30 .lemma 2 inlineform205 .let a policy inlineform206 for inlineform207 be given such that , for some inlineform208 , inlineform209 and let inlineform210 be an optimal policy for inlineform211 .then inlineform212  so that such a inlineform213 can not be optimal .theorem 1inlineform214and inlineform215 are equivalent .by lemma secref31it suffices to show that inlineform216 , which is immediate since inlineform217  for any optimal policy inlineform218 for inlineform219 and any optimal policy inlineform220 for inlineform221 .here the first equation is by proposition secref29 and lemma secref32 , the second equation is by optimality of inlineform222 for inlineform223 by lemma secref31 , and the final equation is by proposition secref29 and the strong compatibility hypothesis .",
      "target_text": "traditional reinforcement learning agents learn from experience , past or present , gained through interaction with their environment .our approach synthesizes experience , without requiring an agent to interact with their environment , by asking the policydirectly\"are there situationsx , y , and z , such that in these situations you would select actions a , b , and c?\"in this paper we present introspection learning , an algorithm that allows for the asking of these types of questions of neural network policies .introspection learning is reinforcement learning algorithm agnostic and the states returned may be used as an indicator of the health of the policy or to shape the policy in a myriad of ways .we demonstrate the usefulness of this algorithm both in the context of speeding up training and improving robustness with respect to safety constraints .",
      "output_text": "thea: and to of"
    },
    {
      "input_text": "Title: On the Performance of ConvNet Features for Place Recognition\nDomains: Computer science, Engineering\n\nintroduction: robots that aim at autonomous long - term operations over extended periods of time , such as days , weeks , or months , are faced with environments that can undergo dramatic changes in their visual appearance over time .visual place recognition \u2013 the ability to recognize a known place in the environment using vision as the main sensor modality \u2013 is largely affected by these appearance changes and is therefore an active research field within the robotics community .the recent literature proposes a variety of approaches to address the challenges of this field   .recent progress in the computer vision and machine learning community has shown that the features generated by convolutional networks ( convnets , see fig .figref2for examples ) outperform other methods in a broad variety of visual recognition , classification and detection tasks   .convnets have been demonstrated to be versatile and transferable , i.e. even although they were trained on a very specific target task , they can be successfully deployed for solving different problems and often even outperform traditional hand engineered features   .our paper leverages these astounding properties and introduces the first real - time capable convnet - based place recognition system .we exploit the hierarchical nature of convnet features and use the semantic information encoded in the higher layers for search space partitioning and the mid - level features for place matching under challenging conditions .locality - sensitive hashing of these features allows us to perform robust place recognition against 100,000 known places with 3 hz .we provide a thorough investigation of the utility of the individual layers in the convnet hierarchy under severe appearance and viewpoint variations and furthermore compare three state - of - the - art networks for the task of place recognition .we establish the following main results :in the following we review the related literature before describing the datasets and evaluation protocol used .we analyze the performance of individual convnet feature layers for place recognition on several challenging datasets in section secref4 and introduce important algorithmic performance improvements in section secref5 before concluding the paper .\n\nplace recognition: the focus of research in place recognition has recently moved from recognizing scenes without significant appearance changes   to more demanding , but also more realistic changing environments .methods that address the place recognition problem span from matching sequences of images transforming images to become invariant against common scene changes such as shadows learning how environments change over time and predicting these changes in image space particle filter - based approaches that build up place recognition hypotheses over time or build a map of experiences that cover the different appearances of a place over time   .learning how the appearance of the environment changes generally requires training data with known frame correspondences .  builds a database of observed features over the course of a day and night .  ,   present an approach that learns systematic scene changes in order to improve performance on a seasonal change dataset .  learns salient regions in images of the same place with different environmental conditions .beyond the limitation of requiring training data , the generality of these methods is also currently unknown ; these methods have only been demonstrated to work in the same environment and on the same or very similar types of environmental change to that encountered in the training datasets .\n\nconvolutional networks: a commonality between all the aforementioned approaches is that they rely on a fixed set of hand - crafted traditional features or operate on the raw pixel levels .however , a recent trend in computer vision , and especially in the field of object recognition and detection is to exploit learned features using deep convolutional networks ( convnets ) .it therefore appears very promising to analyze these features and experimentally investigate their feasibility for the task of place recognition .convolutional network is a well - known architecture and was proposed by lecun et al .in 1989   to recognize hand - written digits .several research groups have recently shown that convnets outperform classical approaches for object classification or detection that are based on hand - crafted features   .the availability of pre - trained network models makes it easy to experiment with such approaches for different tasks : the software packages overfeat   and caffe  provide network architectures pre - trained for a variety of recognition tasks .  was the first to consider convnets for place recognition .our investigation is more thorough , since we cleanly separate the performance contribution of the convnet features from the matching strategy , conduct more systematic experiments on various datasets , compare three different convnets , and contribute important algorithmic improvements which enable real - time performance .\n\nfeature extraction using a convolutional neural network: for the experiments described in the following section , we deploy the alexnet convnet   provided by caffe   .this network was pre - trained on the imagenetilsvrc dataset   for object recognition .it consists of five convolutional layers followed by three fully connected layers and a soft - max layer .the output of each individual layer can be extracted from the network and used as a holistic image descriptor .since the third fully connected and the soft - max layer are adopted specifically to the ilsvrc task ( they have 1000 output neurons for the 1000 object classes in ilsvrc ) , we do not use them in the following experiments .table tabref10 lists the used layers and compares their dimensionality ; fig .figref2 displays some exemplar features extracted from different layers .all images are resized to inlineform0 pixels to fit the expected input size of the convnet .\n\nimage matching and performance measures: place recognition is performed by single - image nearest neighbor search based on the cosine distance of the extracted feature vectors .we analyze the performance in terms of precision - recall curves and inlineform1 scores .in contrast to   we explicitly do not apply sequence search techniques or other specialized algorithmic approaches to improve the matching performance in order to observe the baseline performance of the convnet features under investigation .\n\ndatasets used in the evaluation: we used four different datasets in the experiments presented in section secref4 .from the summary of their characteristics in table tabref15 we can see that we carefully selected these datasets to form three groups of varying condition changes : two datasets exhibit severe appearance change but virtually no variation in viewpoints , one shows viewpoint changes but only mild appearance changes , and three others feature both types of variations .the nordland dataset consists of video footage of a 10 hours long train journey recorded from the perspective of the front cart in four different seasons .fig .figref25 gives an impression of the severe appearance changes between the different seasons .the nordland dataset is a perfect experimentation dataset since it exhibits no viewpoint variations whatsoever and therefore allows to test algorithms on pure condition changes .see   for a more elaborate discussion of this dataset .for our experiments we extracted image frames at a rate of 1 fps and exclude all images that are taken inside tunnels or when the train was stopped .we furthermore exclude images between the timestamps 1:00h-1:30h and 2:47h-3:30h since these form a training dataset we use in parallel work .the gardens point dataset has been recorded on the gardens point campus of qut in brisbane .it consists of three traverses of the environment , two during the day and one during the night .the night images have been extremely contrast enhanced and converted to grayscale in the process .one of the day traverses was recorded keeping on the left side of the walkways , while the other day and the night datasets have been recorded from the right side .this way , the dataset exhibits both appearance and viewpoint changes .this dataset was recorded in different areas of our campus ( outdoor , office , corridor , food court ) once by a robot using the kinect rgb camera and once by a human with a gopro camera .while the robot footage was recorded during the day , the human traversed the environment during dawn , resulting in significant appearance changes especially in the outdoor scenes .the gopro images were cropped to contain only the center part of half of the size of the original images .the st . lucia dataset  has been recorded from inside a car moving through a suburb in brisbane at 5 different times during a day , and also on different days over a time of two weeks .it features mild viewpoint variations due to slight changes in the exact route taken by the car .more significant appearance changes due to the different times of day can be observed , as well as some changes due to dynamic objects such as traffic or cars parked on the street .\n\nlayer-by-layer studies: this section provides a thorough investigation of the utility of different layers in the convnet hierarchy for place recognition and evaluates their individual robustness against the two main challenges in visual place recognition : severe appearance changes and viewpoint variations .\n\nappearance change robustness: in a first set of experiments we analyze the robustness of the features from different layers in the convnet architecture against appearance changes .we conduct these experiments on the following datasets :the nordland dataset , using 5 different season pairings that involve the spring or winter seasonthe gardens point dataset , using all 3 subsets , spanning day and nightthe st . lucia dataset , using a total of 9 traversals from varying daytimes , over the course of several weeksthe campus dataset , using footage recorded by a human and a robot at different times of the day , including dawnfigures figref21 - figref25 show the resulting precision - recall curves for all experiments .table tabref30 summarizes the results further and compares the inlineform2 scores with other state - of - the - art methods .the experiments consistently show that the mid - level features from layer conv3 are more robust against appearance changes than features from any other layer .both the lower layers ( pool1 , pool2 ) and the higher layers ( e.g. fc6 and fc7 ) in the feature hierarchy lack robustness and exhibit inferior place recognition performance .we further compared our approach to seqslam   a state - of - the - art method for place recognition under extreme appearance changes and found that single image matching using features extracted by layer conv3 matches or exceeds seqslam 's performance .previous work   ,   established that fab - map   ,   is not capable of handling the severe appearance changes of the nordland andst . lucia datasets .\n\nviewpoint change robustness: viewpoint changes are a second major challenge for visual place recognition systems .while point feature - based methods like fabmap are less affected , holistic methods such as the one proposed here are prone to error in the presence of viewpoint changes .in order to quantify the viewpoint robustness of convnet features , we perform two experiments :we use the nordland spring dataset to create synthetic viewpoint changes by using shifted image crops .we use the gardens point dataset ( day left vs. day right ) to verify the observed effects on a real dataset .we conduct the first experiment on the nordland spring dataset using images cropped to half of the width of the original images .we simulate viewpoint changes between two traverses by shifting the images of the second traverse to the right .this results in overlaps between the images of the simulated first and second traverse of between 98 % and 20 % .for these experiments , the first 5000 images from the spring dataset ( excluding tunnels , stoppages and the training dataset mentioned before ) were used .fig .figref32 illustrates the results of the experiment with f - scores extracted from precision recall statistics .in a second experiment we used the gardens point day - left vs.day - right dataset that exhibits a lateral camera movement of 2 - 3 meters for medium viewpoint changes between two traversals of the environment .only minor variations in the appearance of the scenes can be observed , mostly caused by people walking on the campus .fig .figref33 shows the resulting precision recall curves .both experiments show that features from layers higher in the convnet architecture , especially fc6 , are more robust to viewpoint changes than features from lower layers .the increased viewpoint robustness of the upper layers can be accounted to the pooling layers that are part of the network architecture and perform max - pooling after the first , second , and fifth convolutional layer .the synthetic experiments summarized in fig .figref32 also show that all layers are robust to mild viewpoint changes with more than 90 % overlap between scenes .this effect is due to the resampling ( resizing ) of images before they are passed through the network and the convolution and pooling operations that occur in the first layer .in subsequent work   we addressed the scenario of viewpoint and appearance changes occurring simultaneously using a number of new dataset .\n\nsummary and discussion: table tabref37 summarizes our experiments .it is apparent that place recognition based on single image matching performs well even under severe appearance changes when using the mid - level conv3 layer as a holistic feature extractor .features from the higher layers , especially fc6 , only outperform conv3 in situations with viewpoint changes but none or only mild appearance variations .this intuitively makes sense , since the features from the first convolutional layers resemble very simple shape features  that are not discriminative and generic enough to allow place recognition under severe appearance changes .the layers higher in the hierarchy , and especially the fully connected layers , are more semantically meaningful but therefore lose their ability to discriminate between individual places within the same semantic type of scene .for example in the st . lucia footage , the higher layers would encode the information ' suburban road scene ' equally for all images in the dataset .this enables place categorization but is disadvantageous for place recognition .the mid - level layers \u2013 and especially conv3 \u2013 seem to encode just the right amount of information ; they are more informative and more robust to changes than pure low - level pixel or gradient based features , while remaining discriminate enough to identify individual places .extracting a convnet feature from an image requires approximately 15 ms on a nvidia quadro k4000 gpu .the bottleneck of the place recognition system is the nearest neighbor search that is based on the cosine distance between 64,896 dimensional feature vectors \u2013 a computationally expensive operation .our numpy / scipy implementation requires 3.5 seconds to find a match among 10,000 previously visited places .as we shall see in the next section , two important algorithmic improvements can be introduced that lead to a speed - up of 2 orders of magnitude .\n\nreal-time large-scale place recognition: in contrast to typical computer vision benchmarks where the recognition accuracy is the most important performance metric , robotics applications depend on agile algorithms that can provide a solution under certain soft real - time constraints .the nearest neighbor search is the key limiting factor for large - scale place recognition , as its runtime is proportional to the number of stored previously visited places .in the following we will explore two approaches that will decrease the required search time by two orders of magnitude with only minimal accuracy degradation .\n\nlocality sensitive hashing for runtime improvements: computing the cosine distance between many 64,896 dimensional conv3 feature vectors is an expensive operation and is a bottleneck of the convnet - based place recognition .to speed this process up significantly , we propose to use a specialized variant of binary locality - sensitive hashing that preserves the cosine similarity   .this hashing method leverages the property that the probability of a random hyperplane separating two vectors is directly proportional to the angle between these vectors .  demonstrates how the cosine distance between two high - dimensional vectors can be closely approximated by the hamming distance between the respective hashed bit vectors .the more bits the hash contains , the better the approximation .we implemented this method and compare the place recognition performance achieved with the hashed conv3 feature vectors of different lengths ( inlineform3 bits ) on the nordland and gardens point datasets in fig .figref40.using 8192 bits retains approximately 95 % of the place recognition performance .hashing the original 64,896 dimensional vectors into 8192 bits corresponds to a data compression of 99.6 % .since the hamming distance over bit vectors is a computationally cheap operation ,the best matching image among 10,000 candidates can be found within 13.4 ms on a standard desktop machine .this corresponds to a speed - up factor of 266 compared to using the cosine distance over the original conv3 features which required 3570ms per 10,000 candidates .calculating the hashes requires 180 ms using a non - optimized python implementation .table tabref39 summarizes the required time for the main algorithmic steps .we can see that the hashing enables real - time place recognition using convnet features on large scale maps of 100,000 places or more .\n\nsearch space partitioning using semantic categorization: we propose a novel approach to exploit the semantic information encoded in the high - level convnet features to partition the search space and constrain the nearest neighbor search to areas of similar semantic place categories such as office , corridor , classroom , or restaurant .this can significantly shrink the search space in semantically versatile environments .to discriminate between semantic place categories we train a nonlinear svm classifier using the fc7 layer .the training has to be performed only once on images from the sun-397 database   .when performing place recognition , a separate index for each semantic class inlineform4 is maintained that allows fast access to all stored conv3 features that were recorded at previously visited places with the probability of belonging to inlineform5 above a threshold inlineform6 .the nearest neighbor search can then be limited to places that belong to the same semantic category as the currently observed place .we tested this approach on the campus dataset and trained the classifier to discriminate between 11 semantic classes .partitioning the search space decreased the time spent for the nearest neighbor search by a factor of 4 .fig .figref42shows the place recognition performance decreasing slightly when using this technique .however , there is an adjustable trade - off between recognition performance and runtime requirements .lowering the threshold ( inlineform7 was used here ) allows more candidate matches to be assessed , thus increasing recognition performance at the expense of the runtime .\n\ncomparing different convnets for place recognition: all experiments described so far used the alexnet convnet architecture   as implemented by caffe  that was pre - trained for the task of object recognition on the ilsvrc dataset   .while preparing this paper ,   published the places205 and hybrid networks .these two networks have the same principled architecture as alexnet but have been trained for scene categorization ( places205 ) or both tasks ( hybrid ) .we compare their place recognition performance with that of alexnet , using the hashed conv3 features .table tabref44 and fig .figref43 summarize the results .compared to alexnet , places205 and the hybrid network perform slightly better under severe appearance changes .this could be explained by the fact that alexnet is trained for object recognition while the two other networks are specialized for recognizing scene categories , i.e. they learned to discriminate places .however , in the presence of viewpoint changes ( gardens point left vs. right ) , the results are inconclusive and alexnet has a slight performance advantage .\n\nconclusions: our paper presented a thorough investigation on the utility of convnet features for the important task of visual place recognition in robotics .we presented a novel method to combine the individual strengths of the high - level and mid - level feature layers to partition the search space and recognize places under severe appearance changes .we demonstrated for the first time that large - scale robust place recognition using convnet features is possible when applying a specialized binary hashing method .our comprehensive study on four real world datasets highlighted the individual strengths of mid- and high - level features with respect to the biggest challenges in visual place recognition \u2013 appearance and viewpoint changes .a comparison of three state - of - the - art convnets revealed slight performance advantages for the networks trained for semantic place categorization .in subsequent work   we applied the insights gained in this paper and extended the holistic approach presented here to a landmark - based scheme that addresses the remaining challenge of combined viewpoint and appearance change robustness .in future work we will investigate how training convnets specifically for the task of place recognition under changing conditions can improve their performance .\n\nacknowledgements: this research was conducted by the australian research council centre of excellence for robotic vision ( project number ce140100016 ) .we want to thank arren glover and will maddern for collecting the gardens point and st . lucia datasets .",
      "target_text": "after the incredible success of deep learning in the computer vision domain , there has been much interest in applying convolutional network ( convnet ) features in robotic fields such as visual navigation and slam .unfortunately , there are fundamental differences and challenges involved .computer vision datasets are very different in character to robotic camera data , real - time performance is essential , and performance priorities can be different .this paper comprehensively evaluates and compares the utility of three state - of - the - art convnets on the problems of particular relevance to navigation for robots ; viewpoint - invariance and condition - invariance , and for the first time enables real - time place recognition performance using convnets with large maps by integrating a variety of existing ( locality - sensitive hashing ) and novel ( semantic search space partitioning ) optimization techniques .we present extensive experiments on four real world datasets cultivated to evaluate each of the specific challenges in place recognition .the results demonstrate that speed - ups of two orders of magnitude can be achieved with minimal accuracy degradation , enabling real - time performance .we confirm that networks trained for semantic place categorization also perform better at ( specific ) place recognition when faced with severe appearance changes and provide a reference for which networks and layers are optimal for different aspects of the place recognition problem .",
      "output_text": "thea: and to of"
    }
  ]
}